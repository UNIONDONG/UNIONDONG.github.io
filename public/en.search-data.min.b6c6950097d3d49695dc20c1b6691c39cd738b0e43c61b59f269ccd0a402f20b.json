[{"id":0,"href":"/docs/linux/linux_memory_manage/%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E7%94%B1%E6%9D%A5%E5%8F%8A%E6%80%9D%E6%83%B3/","title":"一、内存管理的由来及思想","section":"Linux 内存管理","content":"Linux内存管理 | 一、内存管理的由来及思想 #  1、前言 #  《中庸》有：“九层之台，起于垒土” 之说，那么对于我们搞技术的人，同样如此！\n对于Linux内存管理，你可以说没有留意过，但是它存在于我们日常开发的方方面面，你所打开的文件，你所创建的变量，你所运行的程序，无不以此为基础，它可以说是操作系统的基石；只是底层被封装的太好了，以至于我们在做开发的过程中，不需要关心的太多，哪有什么岁月静好，只是有人在负重前行罢了。\n 虽然日常开发中涉及的比较少，但是作为一个合格的Linux开发者，搞懂内存管理，又显得至关重要，同时也会对嵌入式开发大有脾益，今天我们就来详细聊聊内存管理的那点事。\n 该方面的文章，网上也有很多写的非常不错，但是100个人有100种理解方式，并且不同的人，基础不同，理解能力也不同，所以我写这系列的文章，也更有了意义。\n 2、内存管理的由来 #   为什么要有这个概念呢？\n  首先，内存管理，管理的是个什么东西？  管理的其实是我们的物理内存，也就是我们的RAM空间，在电脑上，表现为我们安装的内存条，有的人装个4G的、8G的、甚至64G的，这些就是实打实的物理空间大小，也就是我们的实际的硬件资源。\n 为什么要进行管理？  做嵌入式的都知道，像我们刚开始玩的C51单片机、STM32单片机，我们将程序烧录到Flash中后，开机启动后，然后CPU会将Flash程序加载到RAM中，也就是我们的物理内存，随后我们的所有操作都是基于这一个物理内存所进行的。\n那么此时：\n 我们想再次运行一个一模一样的程序怎么办？ 即使运行了，那两个程序同时操作了同一个变量，值被错误修改了怎么办？  这些就是Linux内存管理要做的事情。\n  顺便介绍一下 我的圈子：高级工程师聚集地，期待大家的加入。\n 3、Linux内存管理思想 #  为了解决上面的一些问题，Linux采用虚拟内存管理技术。\n Linux操作系统抽象出来一个虚拟地址空间的概念，供上层用户使用，这么做的目的是为了让多个用户进程，都以为自己独享了内存空间。 而虚拟地址空间与物理地址空间的对应关系，就交给了一个MMU(Memory Managerment Unit)的家伙来管理，其主要负责将虚拟内存空间映射到真实的物理地址空间。  这么做的主要目的在于：\n 让每个进程都拥有相同大小的虚拟地址空间 避免用户直接访问物理内存，导致系统崩溃  这样，我们同时执行多个进程，虽然看起来虚拟地址操作都是相同的，但是通过MMU之后，就被映射到了不同的物理地址空间，这样就解决了以上的问题。\n 4、总结 #  熟悉了内存管理由来以及其思想，我们可以看出，操作系统的内存管理，主要分为以下几个方面：\n 虚拟内存空间管理：我们抽象出来的虚拟地址空间，该怎么使用，该怎么管理？ 物理内存空间管理：虚拟地址映射到物理内存空间后，该如何管理，如何分配？ 如何映射：虚拟内存如何映射到物理内存，是怎么操作的，映射方法有哪些？  下面我们来一一详细探究。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":1,"href":"/docs/linux/linux_api/","title":"Linux API 揭秘","section":"Linux开发","content":"Ubi loqui #  Mentem genus facietque salire tempus bracchia #  Lorem markdownum partu paterno Achillem. Habent amne generosi aderant ad pellem nec erat sustinet merces columque haec et, dixit minus nutrit accipiam subibis subdidit. Temeraria servatum agros qui sed fulva facta. Primum ultima, dedit, suo quisque linguae medentes fixo: tum petis.\nRapit vocant si hunc siste adspice #  Ora precari Patraeque Neptunia, dixit Danae Cithaeron armaque maxima in nati Coniugis templis fluidove. Effugit usus nec ingreditur agmen ac manus conlato. Nullis vagis nequiquam vultibus aliquos altera suum venis teneas fretum. Armos remotis hoc sine ferrea iuncta quam!\nLocus fuit caecis #  Nefas discordemque domino montes numen tum humili nexilibusque exit, Iove. Quae miror esse, scelerisque Melaneus viribus. Miseri laurus. Hoc est proposita me ante aliquid, aura inponere candidioribus quidque accendit bella, sumpta. Intravit quam erat figentem hunc, motus de fontes parvo tempestate.\niscsi_virus = pitch(json_in_on(eupViral), northbridge_services_troubleshooting, personal( firmware_rw.trash_rw_crm.device(interactive_gopher_personal, software, -1), megabit, ergonomicsSoftware(cmyk_usb_panel, mips_whitelist_duplex, cpa))); if (5) { managementNetwork += dma - boolean; kilohertz_token = 2; honeypot_affiliate_ergonomics = fiber; } mouseNorthbridge = byte(nybble_xmp_modem.horse_subnet( analogThroughputService * graphicPoint, drop(daw_bit, dnsIntranet), gateway_ospf), repository.domain_key.mouse(serverData(fileNetwork, trim_duplex_file), cellTapeDirect, token_tooltip_mashup( ripcordingMashup))); module_it = honeypot_driver(client_cold_dvr(593902, ripping_frequency) + coreLog.joystick(componentUdpLink), windows_expansion_touchscreen); bashGigabit.external.reality(2, server_hardware_codec.flops.ebookSampling( ciscNavigationBacklink, table + cleanDriver), indexProtocolIsp);  Placabilis coactis nega ingemuit ignoscat nimia non #  Frontis turba. Oculi gravis est Delphice; inque praedaque sanguine manu non.\nif (ad_api) { zif += usb.tiffAvatarRate(subnet, digital_rt) + exploitDrive; gigaflops(2 - bluetooth, edi_asp_memory.gopher(queryCursor, laptop), panel_point_firmware); spyware_bash.statePopApplet = express_netbios_digital( insertion_troubleshooting.brouter(recordFolderUs), 65); } recursionCoreRay = -5; if (hub == non) { portBoxVirus = soundWeb(recursive_card(rwTechnologyLeopard), font_radcab, guidCmsScalable + reciprocalMatrixPim); left.bug = screenshot; } else { tooltipOpacity = raw_process_permalink(webcamFontUser, -1); executable_router += tape; } if (tft) { bandwidthWeb *= social_page; } else { regular += 611883; thumbnail /= system_lag_keyboard; }  Caesorum illa tu sentit micat vestes papyriferi #  Inde aderam facti; Theseus vis de tauri illa peream. Oculos uberaque non regisque vobis cursuque, opus venit quam vulnera. Et maiora necemque, lege modo; gestanda nitidi, vero? Dum ne pectoraque testantur.\nVenasque repulsa Samos qui, exspectatum eram animosque hinc, aut manes, Assyrii. Cupiens auctoribus pariter rubet, profana magni super nocens. Vos ius sibilat inpar turba visae iusto! Sedes ante dum superest extrema.\n"},{"id":2,"href":"/docs/linux/linux_debug/%E4%B8%80%E6%96%87%E7%A7%92%E6%87%82ftrace%E7%B3%BB%E7%BB%9F%E8%B0%83%E8%AF%95%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E7%BB%88%E6%9E%81%E6%8C%87%E5%8D%97/","title":"【一文秒懂】Ftrace系统调试工具使用终极指南","section":"Linux 调试工具","content":"【一文秒懂】Ftrace系统调试工具使用终极指南 #  1、Ftrace是什么 #  Ftrace是Function Trace的简写，由 Steven Rostedt 开发的，从 2008 年发布的内核 2.6.27 中开始就内置了。\nFtrace是一个系统内部提供的追踪工具，旨在帮助内核设计和开发人员去追踪系统内部的函数调用流程。\n随着Ftrace的不断完善，除了追踪函数调用流程外，还可以用来调试和分析系统的延迟和性能问题，并发展成为一个追踪类调试工具的框架。\n除了Ftrace外，追踪类调试工具还包括：\n2、Ftrace的实现原理 #  为了帮助我们更好的使用Ftrace，我们有必要简单了解Ftrace的实现原理。\n2.1 Ftrace框架图 #  Ftrace的框架图如下：\n由框架图我们可以知道：\n ftrace包括多种类型的tracers，每个tracer完成不同的功能 将这些不同类型的tracers注册进入ftrace framework 各类tracers收集不同的信息，并放入到Ring buffer缓冲区以供调用。   2.2 Ftrace是如何记录信息的 #  Ftrace采用了静态插桩和动态插桩两种方式来实现。\n静态插桩：\n我们在Kernel中打开了CONFIG_FUNCTION_TRACER功能后，会增加一个-pg的一个编译选项，这个编译选项的作用就是为每个函数入口处，都会插入bl mcount跳转指令，使得每个函数运行时都会进入mcount函数。\n Ftrace一旦使能，对kernel中所有的函数插桩，这带来的性能开销是惊人的，有可能导致人们弃用Ftrace功能。\n 为了解决这个问题，开发者推出了Dynamic ftrace，以此来优化整体的性能。\n动态插桩：\n 这里的动态，是指的动态修改函数指令。\n  编译时，记录所有被添加跳转指令的函数，这里表示所有支持追踪的函数。 内核将所有跳转指令替换为nop指令，以实现非调试状态性能零损失。 根据 function tracer 设置，动态将被调试函数的nop指令，替换为跳转指令，以实现追踪。   总而言之，Ftrace记录数据可以总结为以下几个步骤：\n 打开编译选项-pg，为每个函数都增加跳转指令 记录这些可追踪的函数，并为了减少性能消耗，将跳转函数替换为nop指令 通过flag标志位来动态管理，将需要追踪的函数预留的nop指令替换回追踪指令，记录调试信息。   3、如何使用Ftrace #  3.1 配置详解 #  CONFIG_FTRACE=y # 启用了 Ftrace CONFIG_FUNCTION_TRACER=y\t# 启用函数级别的追踪器 CONFIG_HAVE_FUNCTION_GRAPH_TRACER=y\t# 表示内核支持图形显示 CONFIG_FUNCTION_GRAPH_TRACER=y\t# 以图形的方式显示函数追踪过程 CONFIG_STACK_TRACER=y\t# 启用堆栈追踪器，用于跟踪内核函数调用的堆栈信息。 CONFIG_DYNAMIC_FTRACE=y\t# 启用动态 Ftrace，允许在运行时启用和禁用 Ftrace 功能。 CONFIG_HAVE_FTRACE_NMI_ENTER=y\t# 表示内核支持非屏蔽中断（NMI）时进入 Ftrace 的功能 CONFIG_HAVE_FTRACE_MCOUNT_RECORD=y\t# 表示内核支持通过 mcount 记录函数调用关系。 CONFIG_FTRACE_NMI_ENTER=y # 表示内核支持通过 mcount 记录函数调用关系。  CONFIG_FTRACE_SYSCALLS=y\t# 系统调用的追踪 CONFIG_FTRACE_MCOUNT_RECORD=y\t# 启用 mcount 记录函数调用关系。 CONFIG_SCHED_TRACER=y\t# 支持调度追踪 CONFIG_FUNCTION_PROFILER=y\t# 启用函数分析器，主要用于记录函数的执行时间和调用次数 CONFIG_DEBUG_FS=y\t# 启用 Debug 文件系统支持  上面只是介绍了部分配置，更多详细配置可自行了解。\n并且上述配置不一定全部打开，勾选自己需要的即可，通常我们选择CONFIG_FUNCTION_TRACER和CONFIG_HAVE_FUNCTION_GRAPH_TRACER即可，然后编译烧录到开发板。\n  3.2 挂载debugfs文件系统 #  Ftrace是基于debugfs调试文件系统的，所以我们的第一步就是先挂载debugfs。\nmount -t debugfs none /sys/kernel/debug 此时我们能够在/sys/kernel/debug下看到内核支持的所有的调试信息了。\n# cd /sys/kernel/debug/ # ls asoc gpio regmap bdi ieee80211 sched_debug block memblock sched_features clk mmc0 sleep_time device_component mmc1 suspend_stats devices_deferred mtd tracing dma_buf opp ubi extfrag pinctrl ubifs fault_around_bytes pm_qos wakeup_sources  3.3 traceing目录介绍 #  在/sys/kernel/debug目录下，包含的是kernel所有的调试信息，本章只关注与tracing目录，下面挑选一些比较重要的属性文件来分析。\n  万变不离其宗，如此复杂的框架，设计人员已经提供了README文件，里面详解了各个属性文件的含义，我建议抛弃本文，看README吧:)\n 3.3.1 trace #  trace ：包含当前追踪的内容，以人类可读的格式展现，通过echo \u0026gt; trace来清除。\n 3.3.2 trace_pipe #  trace_pipe 和 trace 一样，都是记录当前的追踪内容，但它和 trace 不一样的是：\n 对 trace_pipe 的读操作将会阻塞，直到有新的追踪数据进来为止； 当前从trace_pipe 读取的内容将被消耗掉，再次读 trace_pipe 又会阻塞到新数据进来为止。   简单的来说，cat trace_pipe是堵塞读取，有数据就读，没数据就等待；而cat trace有没有数据都是直接返回的\n  3.3.3 tracing_on #  tracing_on：向 tracing_on 写入 1，启用追踪；向 tracing_on 写入 0，停止追踪。\n 追踪使用 ring buffer 记录追踪数据。修改 tracing_on 不会影响 ring buffer 当前记录的内容。\n  3.3.4 current_tracer #  current_tracer 表示当前启用的 tracer ，默认为 nop ，即不做任何追踪工作：\n# cat current_tracer nop  3.3.5 available_filter_functions #  available_filter_functions：可以被追踪的函数列表，即可以写到 set_ftrace_filter，set_ftrace_notrace，set_graph_function，set_graph_notrace 文件的函数列表。\n 3.3.6 available_tracers #  available_tracers 文件中包含的是当前编译到内核的 tracer 列表，也表示当前内核支持的tracer列表。\n该列表的内容，就是可以写到 current_tracer 的 tracer 名。\n# cat available_tracers function_graph function nop  nop：表示为空，不追踪 function：追踪函数调用 function_graph：以图形形式追踪函数调用   3.3.7 buffer_size_kb #  buffer_size_kb 记录 CPU buffer 的大小，单位为 KB 。\nper_cpu/cpuX/buffer_size_kb 记录 每个CPU buffer 大小，单位为 KB 。可通过写 buffer_size_kb 来改变 CPU buffer 的大小。\n 3.3.8 buffer_total_size_kb #  buffer_total_size_kb 记录所有 CPU buffer 的总大小，即所有 CPU buffer 大小总和。\n 如有 128 个 CPU buffer ，每个大小 7KB，则 buffer_total_size_kb 记录的总大小为 128 * 7KB = 896。\n buffer_total_size_kb 文件是只读的。\n 3.3.9 set_ftrace_filter #  set_ftrace_filter ：过滤函数追踪，仅仅追踪写入该文件的函数名。\n可填入的参数，可以通过available_filter_functions文件查看当前支持的函数名。\n该过滤功能，也有很多其他变体，如追踪某个模块的函数调用等。\n 官方给的示例：\n Format: :mod:\u0026lt;module-name\u0026gt; example: echo :mod:ext3 \u0026gt; set_ftrace_filter\t# 该模块必须是已经加载进去的模块  3.3.10 set_ftrace_notrace #  set_ftrace_notrace：和 set_ftrace_filter 刚好相反，系统禁用对其中列举函数的追踪。\n 3.3.11 set_ftrace_pid #  系统对 set_ftrace_pid 文件中指定的 PID进程进行追踪。\n如果开启了 options/function-fork 选项，fork 的子进程的 PID 也会自动加入文件，同时该选项也会引起系统自动将退出进程的 PID 从文件中移除。\n 3.3.12 set_graph_function #  此文件中列出的函数将导致函数图跟踪器仅跟踪这些函数以及它们调用的函数。\n但是该跟踪的记录，仍然受set_ftrace_filter 和 set_ftrace_notrace 的影响。\n 3.3.12 set_graph_notrace #  与 set_graph_function 类似，但当函数被命中时，将禁用函数图跟踪，直到退出函数。\n 3.4 简单使用示例 #   一般我们挂载上debugfs后，tracing_on是处于打开状态的。\n 3.4.1 函数追踪 #   3.4.2 追踪图形显示 #   3.4.3 动态过滤追踪 #   3.4.4 重置追踪 #  echo 0 \u0026gt; tracing_on\t# 关闭trace echo \u0026gt; trace\t# 清空当前trace记录 cat available_tracers # 查看当前支持的追踪类型 echo function_graph \u0026gt; current_tracer # 设置当前的追踪类型 echo 1 \u0026gt; tracing_on\t# 开启追踪 cat trace\t# 查看追踪结果  4、进阶用法 #  上述章节，只是介绍了Ftrace最基本的命令，下面来看一下Ftrace在具体问题中的用法！\n4.1 追踪任意命令 #   如何追踪我们执行的命令呢？\n Ftrace支持追踪特定进程，通过set_ftrace_pid属性来设置指定进程。然后在该进程中，执行特定的命令。\n首先我们需要设置好我们的追踪器\nmount -t debugfs none /sys/kernel/debug cd /sys/kernel/debug/tracing echo 0 \u0026gt; tracing_on\t# 关闭追踪器 echo function \u0026gt; current_tracer\t# 设置当前追踪类别 在我们设置好追踪器后，使用如下命令，即可追踪我们执行的命令your_command\necho \u0026gt; trace; echo $$ \u0026gt; set_ftrace_pid; echo 1 \u0026gt; tracing_on; your_command; echo 0 \u0026gt; tracing_on  4.2 追踪指定函数的调用流程 #  跟踪函数的时候，设置 echo 1 \u0026gt; options/func_stack_trace 即可在 trace 结果中获取追踪函数的调用栈。\nmount -t debugfs none /sys/kernel/debug cd /sys/kernel/debug/tracing echo 0 \u0026gt; tracing_on\t# 关闭追踪器 cat available_filter_functions | grep \u0026#34;xxxxxx\u0026#34;\t# 搜索函数是否存在 echo xxxxxx \u0026gt; set_ftrace_filter\t# 设定追踪的函数 echo function \u0026gt; current_tracer\t# 设置当前追踪类别 echo 1 \u0026gt; options/func_stack_trace\t# 记录堆栈信息 echo \u0026gt; trace\t# 清空缓存 echo 1 \u0026gt; tracing_on\t# 开始追踪 效果如下：\n# cat trace # tracer: function # # entries-in-buffer/entries-written: 2/2 #P:3 # # _-----=\u0026gt; irqs-off # / _----=\u0026gt; need-resched # | / _---=\u0026gt; hardirq/softirq # || / _--=\u0026gt; preempt-depth # ||| / delay # TASK-PID CPU# |||| TIMESTAMP FUNCTION # | | | |||| | | kworker/1:1-59 [001] .... 168.954199: mmc_rescan \u0026lt;-process_one_work kworker/1:1-59 [001] .... 168.954248: \u0026lt;stack trace\u0026gt; =\u0026gt; mmc_rescan =\u0026gt; process_one_work =\u0026gt; worker_thread =\u0026gt; kthread =\u0026gt; ret_from_fork =\u0026gt; 0  4.3 追踪指定模块的所有函数 #  要想我们的ko文件能够被Ftrace记录到，我们需要在编译模块的时候，加上编译参数-pg，这点很重要，否则你在available_filter_functions列表中，查找不到你想要的函数。\n然后，需要我们设置过滤器，设置方法有以下几种：\n 按模块直接过滤：  # 示例 Format: :mod:\u0026lt;module-name\u0026gt; example: echo :mod:ext3 \u0026gt; set_ftrace_filter  追踪ext3模块内的所有函数\n   按函数直接过滤   如果该模块内的函数，命名都有一定的规则，可以按照正则表达式来过滤\n # 示例 echo \u0026#34;mmc*\u0026#34; \u0026gt; set_ftrace_filter  过滤包含mmc字符的所有函数\n   按照函数差异来过滤  如果函数命名没有规律，又想过滤该模块所有函数，该怎么办？\n按照加载模块前后的函数差异，写入到文件中来过滤\ncat available_filter_functions \u0026gt; /tmp/1.txt cat available_filter_functions \u0026gt; /tmp/2.txt diff /tmp/1.txt /tmp/2.txt \u0026gt; /tmp/3.txt cat /tmp/3.txt | sed \u0026#39;s/^+//\u0026#39; | awk \u0026#39;{print $1}\u0026#39;\t# 如果diff出来格式前带有+-号，需要手动去掉 cat /tmp/3.txt \u0026gt; set_ftrace_filter  5、自动化管理 #  Ftrace功能很强大，在内核层面我们通过echo和cat即可获取我们想要的所有信息，但是通过一次一次敲命令显得有些繁琐，自己也对常用的功能整合了一个自动化脚本，能够通过命令行，直接追踪特定模块、函数、命令，极大提高了调试效率。\n自动化脚本获取路径：common_trace.sh\n# /root/common_trace.sh  Usage: /root/common_trace.sh {module|funcs|funcs_stack|command|clear} /root/common_trace.sh module ext4 /root/common_trace.sh funcs sysfs /root/common_trace.sh funcs_stack sysfs /root/common_trace.sh command sysfs [functions] /root/common_trace.sh clear 脚本主要实现的功能有：\n 追踪指定模块，查看所有调用流程 追踪指定函数，查看该函数的调用链 追踪指定函数，获取堆栈信息 追踪用户命令，查看所有调用流程，并可选择指定函数来查看调用流程。   脚本除了command功能外，其他功能都需要手动调用common_trace.sh clear来停止追踪。\n  6、总结 #  以上，介绍了Ftrace的由来，实现原理，以及如何使用Ftrace，并最终提供了自动化测试脚本，希望对大家有所帮助。\n 欢迎关注【嵌入式艺术】，董哥原创！\r\u0026nbsp;\r"},{"id":3,"href":"/docs/linux/linux_api/linux-api-%E6%8F%AD%E7%A7%98module_init%E4%B8%8Emodule_exit/","title":"【Linux API 揭秘】module_init与module_exit","section":"Linux API 揭秘","content":" Linux Version：6.6\nAuthor：Donge\nGithub：linux-api-insides\n  1、函数作用 #  module_init和module_exit是驱动中最常用的两个接口，主要用来注册、注销设备驱动程序。\n并且这两个接口的实现机制是一样的，我们先以module_init为切入点分析。\n 2、module_init函数解析 #  2.1 module_init #  #ifndef MODULE /** * module_init() - driver initialization entry point * @x: function to be run at kernel boot time or module insertion * * module_init() will either be called during do_initcalls() (if * builtin) or at module insertion time (if a module). There can only * be one per module. */ #define module_init(x)\t__initcall(x);  ...... #else /* MODULE */ ...... /* Each module must use one module_init(). */ #define module_init(initfn)\t\\ static inline initcall_t __maybe_unused __inittest(void)\t\\ { return initfn; }\t\\ int init_module(void) __copy(initfn)\t\\ __attribute__((alias(#initfn)));\t\\ ___ADDRESSABLE(init_module, __initdata);  ...... #endif 函数名称：module_init\n文件位置：include/linux/module.h\n函数解析：\n 在Linux内核中，驱动程序可以以两种方式存在：内建(Builtin)和模块(Module)。内建驱动就是在编译时，直接编译进内核镜像中；而模块驱动则是在内核运行过程中动态加载卸载的。\n module_init函数的定义位置有两处，使用MODULE宏作为判断依据。MODULE是一个预处理器宏，仅当该驱动作为模块驱动时，编译的时候会加入MODULE的定义。\n 这里难免会有疑问：为什么会有两套实现呢？\n 其实，当模块被编译进内核时，代码是存放在内存的.init字段，该字段在内核代码初始化后，就会被释放掉了，所以当可动态加载模块需要加载时，就需要重新定义了。\n 2.1.1 模块方式 #  当驱动作为可加载模块时，MODULE宏被定义，我们简单分析一下相关代码\n#define module_init(initfn)\t\\ static inline initcall_t __maybe_unused __inittest(void)\t\\ { return initfn; }\t\\ int init_module(void) __copy(initfn)\t\\ __attribute__((alias(#initfn)));\t\\ ___ADDRESSABLE(init_module, __initdata);  static inline initcall_t __maybe_unused __inittest(void) { return initfn; }：一个内联函数，返回传入的initfn函数。  __maybe_unused ：编译器指令，用于告诉编译器，该函数可能不会使用，以避免编译器产生警告信息。   int init_module(void) __copy(initfn) __attribute__((alias(#initfn)));：init_module函数的声明  __copy(initfn)：编译器指令，也就是将我们的initfn函数代码复制到init_module中， __attribute__((alias(#initfn)))：编译器指令，将init_module函数符号的别名设置为initfn。   ___ADDRESSABLE(init_module, __initdata);：一个宏定义，主要用于将init_module函数的地址放入__initdata段，这样，当模块被加载时，init_module函数的地址就可以被找到并调用。  总的来说，如果是可加载的ko模块，module_init宏主要定义了init_module函数，并且将该函数与initfn函数关联起来，使得当模块被加载时，初始化函数可以被正确地调用。\n 2.1.2 内建方式 #  当模块编译进内核时，MODULE宏未被定义，所以走下面流程\n#define module_init(x)\t__initcall(x);  2.2 __initcall #  #define __initcall(fn) device_initcall(fn)  #define device_initcall(fn)\t__define_initcall(fn, 6)  #define __define_initcall(fn, id) ___define_initcall(fn, id, .initcall##id)  #define ___define_initcall(fn, id, __sec)\t\\ __unique_initcall(fn, id, __sec, __initcall_id(fn))  #define __unique_initcall(fn, id, __sec, __iid)\t\\ ____define_initcall(fn,\t\\ __initcall_stub(fn, __iid, id),\t\\ __initcall_name(initcall, __iid, id),\t\\ __initcall_section(__sec, __iid))  #define ____define_initcall(fn, __unused, __name, __sec)\t\\ static initcall_t __name __used \\ __attribute__((__section__(__sec))) = fn;  #define __initcall_stub(fn, __iid, id)\tfn  /* Format: \u0026lt;modname\u0026gt;__\u0026lt;counter\u0026gt;_\u0026lt;line\u0026gt;_\u0026lt;fn\u0026gt; */ #define __initcall_id(fn)\t\\ __PASTE(__KBUILD_MODNAME,\t\\ __PASTE(__,\t\\ __PASTE(__COUNTER__,\t\\ __PASTE(_,\t\\ __PASTE(__LINE__,\t\\ __PASTE(_, fn))))))  /* Format: __\u0026lt;prefix\u0026gt;__\u0026lt;iid\u0026gt;\u0026lt;id\u0026gt; */ #define __initcall_name(prefix, __iid, id)\t\\ __PASTE(__,\t\\ __PASTE(prefix,\t\\ __PASTE(__,\t\\ __PASTE(__iid, id))))  #define __initcall_section(__sec, __iid)\t\\ #__sec \u0026#34;.init\u0026#34;  /* Indirect macros required for expanded argument pasting, eg. __LINE__. */ #define ___PASTE(a,b) a##b #define __PASTE(a,b) ___PASTE(a,b) 函数名称：__initcall\n文件位置：include/linux/init.h\n函数解析：设备驱动初始化函数\n 2.2.1 代码调用流程 #  module_init(fn) |--\u0026gt; __initcall(fn) |--\u0026gt; device_initcall(fn) |--\u0026gt; __define_initcall(fn, 6) |--\u0026gt; ___define_initcall(fn, id, __sec) |--\u0026gt; __initcall_id(fn) |--\u0026gt; __unique_initcall(fn, id, __sec, __iid) |--\u0026gt; ____define_initcall(fn, __unused, __name, __sec) |--\u0026gt; __initcall_stub(fn, __iid, id) |--\u0026gt; __initcall_name(prefix, __iid, id) |--\u0026gt; __initcall_section(__sec, __iid) |--\u0026gt; ____define_initcall(fn, __unused, __name, __sec)   进行函数分析前，我们先要明白#和##的概念\n 2.2.2 #和##的作用 #     符号 作用 举例     ## ##符号 可以是连接的意思 例如 __initcall_##fn##id 为__initcall_fnid那么，fn = test_init，id = 6时，__initcall##fn##id 为 __initcall_test_init6   # #符号 可以是字符串化的意思 例如 #id 为 \u0026quot;id\u0026quot;，id=6 时，#id 为\u0026quot;6\u0026quot;      更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  2.2.3 函数解析 #   下面分析理解比较有难度的函数\n #define device_initcall(fn)\t__define_initcall(fn, 6) #define __define_initcall(fn, id) ___define_initcall(fn, id, .initcall##id)  .initcall##id：通过##来拼接两个字符串：.initcall6  #define ___define_initcall(fn, id, __sec)\t\\ __unique_initcall(fn, id, __sec, __initcall_id(fn))  /* Format: \u0026lt;modname\u0026gt;__\u0026lt;counter\u0026gt;_\u0026lt;line\u0026gt;_\u0026lt;fn\u0026gt; */ #define __initcall_id(fn)\t\\ __PASTE(__KBUILD_MODNAME,\t\\ __PASTE(__,\t\\ __PASTE(__COUNTER__,\t\\ __PASTE(_,\t\\ __PASTE(__LINE__,\t\\ __PASTE(_, fn))))))  /* Indirect macros required for expanded argument pasting, eg. __LINE__. */ #define ___PASTE(a,b) a##b #define __PASTE(a,b) ___PASTE(a,b)  ___PASTE：拼接两个字符串 __initcall_id：它用于生成一个唯一的标识符，这个标识符用于标记初始化函数。  __KBUILD_MODNAME：当前正在编译的模块的名称 __COUNTER__：一个每次使用都会递增计数器，用于确保生成名称的唯一性 __LINE__：当前代码的行号     #define __unique_initcall(fn, id, __sec, __iid)\t\\ ____define_initcall(fn,\t\\ __initcall_stub(fn, __iid, id),\t\\ __initcall_name(initcall, __iid, id),\t\\ __initcall_section(__sec, __iid))  #define ____define_initcall(fn, __unused, __name, __sec)\t\\ static initcall_t __name __used \\ __attribute__((__section__(__sec))) = fn;  #define __initcall_stub(fn, __iid, id)\tfn  /* Format: __\u0026lt;prefix\u0026gt;__\u0026lt;iid\u0026gt;\u0026lt;id\u0026gt; */ #define __initcall_name(prefix, __iid, id)\t\\ __PASTE(__,\t\\ __PASTE(prefix,\t\\ __PASTE(__,\t\\ __PASTE(__iid, id))))  #define __initcall_section(__sec, __iid)\t\\ #__sec \u0026#34;.init\u0026#34; __unique_initcall：调用____define_initcall，关键实现部分\n____define_initcall：定义一个名为 __name 的 initcall_t 类型的静态变量，并将其初始化为 fn，并放入特定的__sec段中。\n __initcall_stub：表示唯一的函数名fn __initcall_name：表示一个唯一的变量名 __initcall_section： 生成一个唯一的段名。 #__sec \u0026quot;.init\u0026quot;：将两个字符串拼接起来，比如：__sec=.initcall6，拼接后的段为：.initcall6.init，该段为最终存储的段。   字段通过链接器链接起来，形成一个列表进行统一管理。\n 这些字段我们可以在arch/arm/kernel/vmlinux.lds中查看。\n ...... __initcall6_start = .; KEEP(*(.initcall6.init)) KEEP(*(.initcall6s.init)) ......  3、module_exit函数解析 #   module_exit和module_init的实现机制几乎没有差别，下面就简单介绍一下。\n 3.1 module_exit #  #ifndef MODULE  /** * module_exit() - driver exit entry point * @x: function to be run when driver is removed * * module_exit() will wrap the driver clean-up code * with cleanup_module() when used with rmmod when * the driver is a module. If the driver is statically * compiled into the kernel, module_exit() has no effect. * There can only be one per module. */ #define module_exit(x)\t__exitcall(x);  ...... #else /* MODULE */ ...... /* This is only required if you want to be unloadable. */ #define module_exit(exitfn)\t\\ static inline exitcall_t __maybe_unused __exittest(void)\t\\ { return exitfn; }\t\\ void cleanup_module(void) __copy(exitfn)\t\\ __attribute__((alias(#exitfn)));\t\\ ___ADDRESSABLE(cleanup_module, __exitdata);  ...... #endif 函数名称：module_exit\n文件位置：include/linux/module.h\n3.1.1 模块方式 #  作为模块方式，与module_init的实现方式一样，定义cleanup_module与exitfn函数相关联，存放在__exitdata段内。\n 3.1.2 内建方式 #  当模块编译进内核时，MODULE宏未被定义，所以走下面流程\n#define module_exit(x)\t__exitcall(x);  3.2 __exitcall #  #define __exitcall(fn)\t\\ static exitcall_t __exitcall_##fn __exit_call = fn  #define __exit_call\t__used __section(\u0026#34;.exitcall.exit\u0026#34;) 函数名称：__initcall\n文件位置：include/linux/init.h\n函数解析：设备驱动卸载函数\n__exitcall_##fn：定义一个新的 exitcall_t 类型的静态变量，并赋值为fn\n__exit_call：__used __section(\u0026quot;.exitcall.exit\u0026quot;)，定义该函数存储的段\n 4、扩展 #   还记得__define_initcall的定义吗？\n #define pure_initcall(fn) __define_initcall(fn, 0)  #define core_initcall(fn) __define_initcall(fn, 1) #define core_initcall_sync(fn) __define_initcall(fn, 1s) #define postcore_initcall(fn) __define_initcall(fn, 2) #define postcore_initcall_sync(fn) __define_initcall(fn, 2s) #define arch_initcall(fn) __define_initcall(fn, 3) #define arch_initcall_sync(fn) __define_initcall(fn, 3s) #define subsys_initcall(fn) __define_initcall(fn, 4) #define subsys_initcall_sync(fn) __define_initcall(fn, 4s) #define fs_initcall(fn) __define_initcall(fn, 5) #define fs_initcall_sync(fn) __define_initcall(fn, 5s) #define rootfs_initcall(fn) __define_initcall(fn, rootfs) #define device_initcall(fn) __define_initcall(fn, 6) #define device_initcall_sync(fn) __define_initcall(fn, 6s) #define late_initcall(fn) __define_initcall(fn, 7) #define late_initcall_sync(fn) __define_initcall(fn, 7s)  #define __initcall(fn) device_initcall(fn) 不同的宏定义，被赋予了不同的调用等级，最后将不同的驱动初始化函数统一汇总到__initcallx_start字段统一管理，形成一个有序的列表。\n这样，我们在内核中，按照顺序遍历这个列表，最后执行对应的模块初始化函数fn即可实现驱动的初始化。\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":4,"href":"/docs/uboot/%E4%B8%80uboot%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3/","title":"一、uboot基础了解","section":"Uboot开发","content":"一、uboot基础了解 #  1. U-boot是什么 #  U-Boot，全称 Universal Boot Loader，是遵循GPL条款的从FADSROM、8xxROM、PPCBOOT逐步发展演化而来的 开放源码项目。\nU-boot，是一个主要用于嵌入式系统的引导加载程序，可以支持多种不同的计算机系统结构，其主要作用为：==引导系统的启动！==目前，U-Boot不仅支持Linux系统的引导，还支持NetBSD, VxWorks, QNX, RTEMS, ARTOS, LynxOS, android等多种嵌入式操作系统。\n2. U-boot主要特性及功能 #   开放：开放的源代码 多平台：支持多种嵌入式操作系统，如Linux、NetBSD、android等 生态：有丰富的设备驱动源码，如以太网、SDRAM、LCD等，同时也具有丰富的开发文档。  3. U-boot下载地址 #  Uboot开发源码：\n  https://source.denx.de/u-boot/u-boot\n  https://ftp.denx.de/pub/u-boot/\n  其他厂商定制的uboot源码：\n 野火  4. U-boot目录结构 #     目录 含义     arch 各个厂商的硬件信息，目录下包括支持的处理器类型   arch/arm/cpu/xxx **每一个子文件夹，包含一种cpu系列。**每个子文件夹下包含cpu.c（CPU初始化），interrupts.c（设置中断和异常），start.S（U-boot的启动文件，早期的初始化）。   board 与开发板有关，每一个子文件夹代表一个芯片厂家，芯片厂家下，每一个子文件夹，表示一个开发板   common 存放与处理器体系无关的通用代码，可以说为通用核心代码！   cmd 存放uboot的相关命令实现部分   drivers 存放外围芯片驱动，网卡，USB等   disk 存放驱动磁盘的分区处理代码   fs 本目录下存放文件系统相关代码，每一个子文件夹表示文件系统   net 网络协议相关代码   doc uboot说明文档   include 各种头文件   post 上电自检代码   api 外部扩展程序的API和示例   tools 编译S-Record或者U-boot镜像的相关工具    5. 如何编译Uboot #  make ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- distclean make ARCH=arm CORSS_COMPILE=arm-linux-gnueabihf- colibri-imx6ull_defconfig make V=1 ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- -j8 ARCH=arm：arm架构\nCROSS_COMPILE：使用的交叉编译器\n 如果编译出错，your compile older 6.0，可以参考【1】\n colibri-imx6ull_defconfig：指定一个config文件，作为相关版型的配置信息\nV=1：这个选项能显示出编译过程中的详细信息，即是verbose编译模式\n-j8：多核并行编译，可以提高编译速度，受硬件限制\n6. U-boot工作模式 #   U-boot的工作模式有：启动加载模式和下载模式\n  启动加载模式：  启动加载模式，为Bootloader正常工作模式，一款开发板，正常上电后，Bootloader将嵌入式操作系统==从FLASH中加载到SDRAM中==运行。\n 下载模式：  下载模式，就是Bootloader通过通信，将内核镜像、根文件系统镜像从PC机直接下载到目标板的FLASH中。\n7. U-boot的存放位置 #  嵌入式系统，一般使用Flash来作为启动设备，Flash上存储着U-boot、环境变量、内核映像、文件系统等。U-boot存放于Flash的起始地址，所在扇区由Soc规定。\n8. U-boot系列文章汇总 #   下面是进行U-boot开发期间，感觉比较不错的资料，总结分享一下！\n [1] : Uboot官网、Uboot官方指南、官方指南2\n[2] : https://blog.51cto.com/u_9291927/category5\n[3] : https://blog.csdn.net/ooonebook/category_6484145.html\n[4]：https://blog.csdn.net/qq_36310253/category_9332618.html\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":5,"href":"/docs/embeded_tech/embeded_interview/soc%E7%9A%84bringup%E6%B5%81%E7%A8%8B/","title":"Soc的Bring Up流程","section":"嵌入式面经","content":"1、Bring Up流程 #  SOC (System on a Chip) bring-up是一个复杂的过程，涉及到硬件、固件和软件的集成和验证，以下是一个基于BROM，SPL，UBOOT和Linux的启动流程的概述：\n BROM (Boot Read-Only Memory)启动：启动的最初阶段，在这个阶段，系统会执行芯片ROM里面的代码，这部分代码主要用来检查启动模式，包括NOR、Nand、Emmc等，然后从对应的存储介质中加载SPL(Secondary Program Loader)代码。 SPL (Secondary Program Loader)启动：SPL属于Uboot的一部分，它的主要作用就是：初始化硬件并加载完整的U-boot，主要体现在初始化时钟、看门狗、DDR、GPIO以及存储外设，最后将U-boot代码加载到DDR中执行。 U-Boot启动：U-boot的主要作用是：引导加载Kernel和DTS。U-boot在启动之后，同样初始化Soc硬件资源，然后会计时等待，并执行默认的启动命令，将Kernel和DTS信息从存储介质中读取出来并加载到内存中执行。 Kernel启动：在U-Boot加载了内核映像和设备树之后，系统会启动Linux。在这个阶段，系统会初始化各种硬件设备，加载驱动程序并启动用户空间应用程序。   更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  2、常见问题 #  Q：为什么上一个阶段已经初始化了硬件资源，下一个阶段为何重复初始化？\nA：\n  每个阶段的硬件初始化，其目标和需求都不同，硬件配置也会不一样，因此在不同阶段进行不同的初始化。\n  硬件状态可能会改变，在SOC启动过程中，硬件状态可能会因为电源管理、时钟管理等原因而改变，这可能需要在每个阶段都重新初始化以确保其正确工作\n  为了保证硬件资源的可靠性，最好每个阶段都重新初始化一次\n   Q：U-boot加载内核时，会进行重定位的操作，这一操作有何意义？\nA：\n U-boot的重定位，主要作用是为了 给内核提供一个连续的、大的内存空间，供内核和其他应用程序使用 U-boot的加载过程分两个阶段，即：SPL和U-boot，   在SPL阶段，主要将U-boot代码从Flash中加载到RAM指定位置 在U-boot阶段，U-boot会将自身从RAM的开始部分移动到RAM的末尾，占用高地址空间，从而让低地址空间可以作为一个连续的，大的内存空间供内核和其他应用程序使用。   Q：在Bring Up中，为了保证启动时间，如何裁剪？\nA：\n 启动时间的裁剪是一个重要的步骤，其主要目标是缩短从电源打开到操作系统完全启动的时间。\n  优化Bootloader：减小Bootloader的代码大小，减少硬件初始化（只初始化必要硬件设备）等 优化Kernel：减少启动服务数量，优化服务的启动顺序，使用预加载技术等方法来实现。 使用快速启动模式：一些SOC支持快速启动模式，这种模式下，SOC会跳过一些不必要的硬件初始化和自检过程，从而更快地启动。 使用休眠和唤醒技术：一些SOC还支持休眠和唤醒技术，这种技术可以将系统的状态保存到非易失性存储器中，然后关闭系统。当系统再次启动时，可以直接从非易失性存储器中恢复系统的状态，从而更快地启动。   "},{"id":6,"href":"/docs/embeded_tech/self_improve/","title":"嵌入式工程师养成记","section":"嵌入式","content":"Ubi loqui #  Mentem genus facietque salire tempus bracchia #  Lorem markdownum partu paterno Achillem. Habent amne generosi aderant ad pellem nec erat sustinet merces columque haec et, dixit minus nutrit accipiam subibis subdidit. Temeraria servatum agros qui sed fulva facta. Primum ultima, dedit, suo quisque linguae medentes fixo: tum petis.\nRapit vocant si hunc siste adspice #  Ora precari Patraeque Neptunia, dixit Danae Cithaeron armaque maxima in nati Coniugis templis fluidove. Effugit usus nec ingreditur agmen ac manus conlato. Nullis vagis nequiquam vultibus aliquos altera suum venis teneas fretum. Armos remotis hoc sine ferrea iuncta quam!\nLocus fuit caecis #  Nefas discordemque domino montes numen tum humili nexilibusque exit, Iove. Quae miror esse, scelerisque Melaneus viribus. Miseri laurus. Hoc est proposita me ante aliquid, aura inponere candidioribus quidque accendit bella, sumpta. Intravit quam erat figentem hunc, motus de fontes parvo tempestate.\niscsi_virus = pitch(json_in_on(eupViral), northbridge_services_troubleshooting, personal( firmware_rw.trash_rw_crm.device(interactive_gopher_personal, software, -1), megabit, ergonomicsSoftware(cmyk_usb_panel, mips_whitelist_duplex, cpa))); if (5) { managementNetwork += dma - boolean; kilohertz_token = 2; honeypot_affiliate_ergonomics = fiber; } mouseNorthbridge = byte(nybble_xmp_modem.horse_subnet( analogThroughputService * graphicPoint, drop(daw_bit, dnsIntranet), gateway_ospf), repository.domain_key.mouse(serverData(fileNetwork, trim_duplex_file), cellTapeDirect, token_tooltip_mashup( ripcordingMashup))); module_it = honeypot_driver(client_cold_dvr(593902, ripping_frequency) + coreLog.joystick(componentUdpLink), windows_expansion_touchscreen); bashGigabit.external.reality(2, server_hardware_codec.flops.ebookSampling( ciscNavigationBacklink, table + cleanDriver), indexProtocolIsp);  Placabilis coactis nega ingemuit ignoscat nimia non #  Frontis turba. Oculi gravis est Delphice; inque praedaque sanguine manu non.\nif (ad_api) { zif += usb.tiffAvatarRate(subnet, digital_rt) + exploitDrive; gigaflops(2 - bluetooth, edi_asp_memory.gopher(queryCursor, laptop), panel_point_firmware); spyware_bash.statePopApplet = express_netbios_digital( insertion_troubleshooting.brouter(recordFolderUs), 65); } recursionCoreRay = -5; if (hub == non) { portBoxVirus = soundWeb(recursive_card(rwTechnologyLeopard), font_radcab, guidCmsScalable + reciprocalMatrixPim); left.bug = screenshot; } else { tooltipOpacity = raw_process_permalink(webcamFontUser, -1); executable_router += tape; } if (tft) { bandwidthWeb *= social_page; } else { regular += 611883; thumbnail /= system_lag_keyboard; }  Caesorum illa tu sentit micat vestes papyriferi #  Inde aderam facti; Theseus vis de tauri illa peream. Oculos uberaque non regisque vobis cursuque, opus venit quam vulnera. Et maiora necemque, lege modo; gestanda nitidi, vero? Dum ne pectoraque testantur.\nVenasque repulsa Samos qui, exspectatum eram animosque hinc, aut manes, Assyrii. Cupiens auctoribus pariter rubet, profana magni super nocens. Vos ius sibilat inpar turba visae iusto! Sedes ante dum superest extrema.\n"},{"id":7,"href":"/docs/embeded_tech/embeded_interview/","title":"嵌入式面经","section":"嵌入式","content":"Ubi loqui #  Mentem genus facietque salire tempus bracchia #  Lorem markdownum partu paterno Achillem. Habent amne generosi aderant ad pellem nec erat sustinet merces columque haec et, dixit minus nutrit accipiam subibis subdidit. Temeraria servatum agros qui sed fulva facta. Primum ultima, dedit, suo quisque linguae medentes fixo: tum petis.\nRapit vocant si hunc siste adspice #  Ora precari Patraeque Neptunia, dixit Danae Cithaeron armaque maxima in nati Coniugis templis fluidove. Effugit usus nec ingreditur agmen ac manus conlato. Nullis vagis nequiquam vultibus aliquos altera suum venis teneas fretum. Armos remotis hoc sine ferrea iuncta quam!\nLocus fuit caecis #  Nefas discordemque domino montes numen tum humili nexilibusque exit, Iove. Quae miror esse, scelerisque Melaneus viribus. Miseri laurus. Hoc est proposita me ante aliquid, aura inponere candidioribus quidque accendit bella, sumpta. Intravit quam erat figentem hunc, motus de fontes parvo tempestate.\niscsi_virus = pitch(json_in_on(eupViral), northbridge_services_troubleshooting, personal( firmware_rw.trash_rw_crm.device(interactive_gopher_personal, software, -1), megabit, ergonomicsSoftware(cmyk_usb_panel, mips_whitelist_duplex, cpa))); if (5) { managementNetwork += dma - boolean; kilohertz_token = 2; honeypot_affiliate_ergonomics = fiber; } mouseNorthbridge = byte(nybble_xmp_modem.horse_subnet( analogThroughputService * graphicPoint, drop(daw_bit, dnsIntranet), gateway_ospf), repository.domain_key.mouse(serverData(fileNetwork, trim_duplex_file), cellTapeDirect, token_tooltip_mashup( ripcordingMashup))); module_it = honeypot_driver(client_cold_dvr(593902, ripping_frequency) + coreLog.joystick(componentUdpLink), windows_expansion_touchscreen); bashGigabit.external.reality(2, server_hardware_codec.flops.ebookSampling( ciscNavigationBacklink, table + cleanDriver), indexProtocolIsp);  Placabilis coactis nega ingemuit ignoscat nimia non #  Frontis turba. Oculi gravis est Delphice; inque praedaque sanguine manu non.\nif (ad_api) { zif += usb.tiffAvatarRate(subnet, digital_rt) + exploitDrive; gigaflops(2 - bluetooth, edi_asp_memory.gopher(queryCursor, laptop), panel_point_firmware); spyware_bash.statePopApplet = express_netbios_digital( insertion_troubleshooting.brouter(recordFolderUs), 65); } recursionCoreRay = -5; if (hub == non) { portBoxVirus = soundWeb(recursive_card(rwTechnologyLeopard), font_radcab, guidCmsScalable + reciprocalMatrixPim); left.bug = screenshot; } else { tooltipOpacity = raw_process_permalink(webcamFontUser, -1); executable_router += tape; } if (tft) { bandwidthWeb *= social_page; } else { regular += 611883; thumbnail /= system_lag_keyboard; }  Caesorum illa tu sentit micat vestes papyriferi #  Inde aderam facti; Theseus vis de tauri illa peream. Oculos uberaque non regisque vobis cursuque, opus venit quam vulnera. Et maiora necemque, lege modo; gestanda nitidi, vero? Dum ne pectoraque testantur.\nVenasque repulsa Samos qui, exspectatum eram animosque hinc, aut manes, Assyrii. Cupiens auctoribus pariter rubet, profana magni super nocens. Vos ius sibilat inpar turba visae iusto! Sedes ante dum superest extrema.\n"},{"id":8,"href":"/docs/embeded_tech/embeded_interview/cpu%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/","title":"CPU体系架构","section":"嵌入式面经","content":"CPU体系架构 #  2.1 CPU体系架构有哪些？ #   我们常见的CPU架构有哪些呢？\n 如果我们熟悉Linux，那么这个问题肯定不难回答！\n我们查看内核目录下的arch子目录，就可以看到Linux所支持的处理器架构，基本属于我们常见的类型了。\n# ls ./arch alpha arc arm arm64 c6x h8300 hexagon ia64 Kconfig m68k microblaze mips nds32 nios2 openrisc parisc powerpc riscv s390 sh sparc um unicore32 x86 xtensa  准确来说，CPU处理器架构主要有以下几种类型：\n CISC（复杂指令集计算机）：CISC架构的CPU设计理念是尽可能减少程序指令的数量，以降低CPU和内存之间的通信频率。这种架构的一个显著特点是拥有大量的寄存器和复杂的指令集。Intel的x86架构就是一个典型的CISC架构 RISC（精简指令集计算机）：RISC架构的CPU设计理念是通过简化指令集来提高CPU的运行效率。这种架构的一个显著特点是拥有较少的寄存器和简单的指令集。ARM架构就是一个典型的RISC架构 MISC（中间指令集计算机）：MISC架构的CPU设计理念是在CISC和RISC之间寻找一个平衡点，既不过于复杂也不过于简单。这种架构的一个显著特点是指令集的复杂度介于CISC和RISC之间 VLIW（超长指令字计算机）：VLIW架构的CPU设计理念是通过增大指令长度来提高并行执行的可能性。这种架构的一个显著特点是指令长度远大于其他架构的CPU EPIC（显式并行指令计算）：EPIC架构的CPU设计理念是通过显式标记并行指令来提高CPU的运行效率。这种架构的一个显著特点是指令集中包含了并行执行的信息。Intel的Itanium架构就是一个典型的EPIC架构 超标量架构：超标量架构的CPU设计理念是通过在一个时钟周期内执行多条指令来提高CPU的运行效率。这种架构的一个显著特点是CPU内部包含了多个执行单元，可以同时执行多条指令 超线程技术：超线程技术是Intel公司为其部分CPU所采用的一种使单一处理器像多个逻辑处理器那样并行处理多个线程的技术 多核心架构：多核心架构的CPU设计理念是在一个CPU芯片内集成多个处理器核心，以提高并行处理能力。这种架构的一个显著特点是CPU内部包含了多个独立的处理器核心，每个核心可以独立执行指令   这里就有一个疑问，我们什么时候说RISC架构，什么时候说ARM架构，这两个有什么区别呢？\n 以ARM和RISC为例：\n ARM架构和RISC架构的主要区别在于ARM实际上是RISC的一个具体实现，而RISC则是一个更广泛的处理器设计理念。换句话说，ARM是RISC的一个子集。\n同理，X86架构是CISC的一个子集。\n 2.2 常见的问题 #  Q1：你所熟知的处理器架构有哪些？\n我们常见的处理器架构有ARM、X86、mips架构等；\n Q2：STM32属于什么架构的？\nSTM32是ST公司开发的32位微控制器集成电路，基于 ARM 的 Cortex-M 系列内核。因此，STM32 属于 ARM 架构的微控制器。\n Q3：RISC和CISC的区别是什么？\n RISC：精简指令集架构，通过简化指令集，使得大多数的操作都能够在一个指令周期内完成，提高CPU运行效率 CISC：复杂指令集架构，指令集丰富，能够完成一些较为复杂的任务，并且可以降低CPU和内存之间的通信频率，提高性能。    欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":9,"href":"/docs/linux/linux_debug/%E4%B8%80%E6%96%87%E7%A7%92%E6%87%82top%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/","title":"【一文秒懂】TOP命令详解","section":"Linux 调试工具","content":"【一文秒懂】TOP命令详解 #  1、Top命令介绍 #  Linux系统中，Top命令主要用于实时运行系统的监控，包括Linux内核管理的进程或者线程的资源占用情况。\n这个命令对所有正在运行的进程和系统负荷提供不断更新的概览信息，包括系统负载、CPU利用分布情况、内存使用、每个进程的内容使用情况等信息。\n 2、Top命令使用 #  Top的命令介绍如下：\ntop -hv|-bcHiOSs -d secs -n max -u|U user -p pid -o fld -w [cols] 常用的Top指令有：\ntop：启动top命令 top -c：显示完整的命令行 top -b：以批处理模式显示程序信息 top -S：以累积模式显示程序信息 top -n 2：表示更新两次后终止更新显示 top -d 3：设置信息更新周期为3秒 top -p 139：显示进程号为139的进程信息，CPU、内存占用率等 top -n 10：显示更新十次后退出 除此之外，在top进程运行过程中，两个最重要的功能是查看帮助（h 或 ？）和退出（q 或 Ctrl+C）。\n 3、Top信息详解 #  top展示界面由从上到下3部分组成\n 概览区域 表头 任务区域 还有一个输入/消息行，位于概览区域和表头之间。  3.1 概览区详解 #  top - 14:46:08 up 5:46, 1 user, load average: 0.00, 0.00, 0.00  程序或者窗口的名称：top 当前时间和系统的启动时间：14:46:08 up 5:46 总共的用户数量：1 user 过去1、5和15分钟的系统平均负载：load average: 0.00, 0.00, 0.00  Tasks: 290 total, 1 running, 212 sleeping, 0 stopped, 0 zombie %Cpu(s): 0.0 us, 0.1 sy, 0.0 ni, 99.9 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st 这两行显示了任务数量和CPU状态\n 第一行该信息对Task进行分类，包括running、sleeping、stopped、zombie四类，显示了系统中正在运行的任务的状态统计信息。具体来说，这里有291个任务总数，其中有1个任务正在运行，212个任务正在睡眠，0个任务已停止，0个任务为僵尸进程。 第二行显示CPU的状态百分比  %Cpu(s): CPU使用率的统计信息。 us (user): 用户空间进程占用CPU的时间百分比。 sy (system): 内核空间进程占用CPU的时间百分比。 ni (nice): 用户进程以优先级调整过的占用CPU的时间百分比（通常不会有这个值）。 id (idle): CPU空闲的时间百分比。 wa (IO-wait): CPU等待I/O操作的时间百分比。 hi (hardware interrupt): CPU处理硬件中断的时间百分比。 si (software interrupt): CPU处理软件中断的时间百分比。 st: 被虚拟化环境偷取的时间百分比（通常不会有这个值）。    KiB Mem : 3994720 total, 525876 free, 595492 used, 2873352 buff/cache KiB Swap: 2097148 total, 2096624 free, 524 used. 3114400 avail Mem 这两行表示内存的使用情况\n 第一行表示物理内存，分为total、 free、 used 、 buff/cache 第二行表示虚拟内存，分为total、free、used、avail   默认单位是KiB，使用按键E可以切换为MiB、GiB、TiB、PiB、EiB\n KiB = kibibyte = 1024 bytes MiB = mebibyte = 1024 KiB = 1,048,576 bytes GiB = gibibyte = 1024 MiB = 1,073,741,824 bytes TiB = tebibyte = 1024 GiB = 1,099,511,627,776 bytes PiB = pebibyte = 1024 TiB = 1,125,899,906,842,624 bytes EiB = exbibyte = 1024 PiB = 1,152,921,504,606,846,976 bytes   更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  3.2 任务区 #  任务区是按照列的形式来显示的，并且有多个字段可以用来查看进程的状态信息。\n3.2.1 任务字段介绍 #    %CPU： CPU Usage，自上次屏幕更新以来任务占用的CPU时间份额，表示为总CPU时间的百分比。\n  %MEM： Memory Usage，进程使用的物理内存百分比\n  CODE：Code Size，可执行代码占用的物理内存量\n  COMMAND：Command Name or Command Line，用于显示输入的命令行或者程序名称\n  PID：Process Id，任务独立的ID，即进程ID\n  PPID：Parent Process Id，父进程ID\n  UID：User Id，任务所有者的用户ID\n  USER：User Name，用户名\n  RUSER：Real User Name，实际的用户名\n  TTY：Controlling Tty，控制终端名称\n  TIME：CPU TIME，该任务CPU总共运行的时间\n  TIME+：同TIME，其粒度更细\n  OOMa：Out of Memory Adjustment Factor，内存溢出调整机制，这个字段会被增加到当前内存溢出分数中，来决定什么任务会被杀掉，范围是-1000到+1000。\n  OOMs：Out of Memory Score，内存溢出分数，这个字段是用来选择当内存耗尽时杀掉的任务，范围是0到+1000。0的意思是绝不杀掉，1000的意思是总是杀掉。\n  S：Process Status，表示进程状态信息\n D： 不可中断休眠 I：空闲 R：运行中 S：休眠 T：被任务控制信号停止 t：在跟踪期间被调试器停止 Z：僵尸     相关属性有很多，可以使用man top查看，这里先列举这些。\n  3.2.2 字段管理 #  我们输入top后，默认只显示一部分属性信息，我们可以自行管理想要的属性信息。\n我们输入F或者f，进入字段管理功能，用于选择想要的字段信息\n   按键 功能     ↑、↓ 光标上下移动选择   空格、d 切换   s 设置为排序依据字段   a、w 在4种窗口中切换：1.默认，2.任务，3.内存，4.用户   Esc键、q 退出当前窗口     4、交互命令详解 #  top的功能很多，基本能够查看进程的各种状态信息，其中还有一些交互式的命令，方便我们更好的查看系统状态。\n 在top主界面中，我们输入下面的命令\n    命令 功能     h、? 帮助信息查看，涵盖所有的快捷键   空格、回车按键 手动刷新界面信息   q、ESC按键 退出   B 粗体显示功能   d、s 改变间隔时间   E、e 切换内存显示的单位，从KiB到EiB   g 然后输入1-4其中一个数字，选择哪种窗口（1.默认，2.任务，3.内存，4.用户）   H 进程、线程显示切换   k 输入PID信息，杀掉一个任务   Z 改变配色     上面介绍了一些比较常见的交互式命令，还有更多需要你去探索哦！\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":10,"href":"/docs/linux/linux_memory_manage/%E4%BA%8C%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E5%B8%83%E5%B1%80/","title":"二、虚拟地址空间布局","section":"Linux 内存管理","content":"Linux内存管理 | 二、虚拟地址空间布局 #  上一章，我们了解了内存管理的由来以及核心思想，下面我们按照顺序，先来介绍一下Linux虚拟内存空间的管理。\n 同样，我们知道Linux内核抽象出来虚拟内存空间，主要是为了让每个进程都独享该空间，那虚拟内存空间是如何布局的呢？\n 前提：针对于不同位数的CPU，寻址能力不同，抽象出来的虚拟内存空间大小也不同，我们以常见的32位的CPU为例。\n  1、虚拟内存空间布局 #  对于32位的CPU，寻址范围为0~2^32，也就是0x00000000-0xFFFFFFFF，即最多抽象出来4G的虚拟内存空间。\n这4GB的内存空间，在Linux中，又分为用户空间和内核空间，其中0x0000000-0xBFFFFFFF，共3G为用户空间，0xC00000000-0xFFFFFFFF，共1G为内核空间，如下：\n无论内核空间还是用户空间，其仍然是在虚拟内存空间基础之上进行划分的，其直接访问的依旧都是虚拟地址，而非物理地址！\n我们编写代码后，所生成的可执行程序，运行之后就成为一个系统进程，我们在\u0026quot;虚\u0026quot;的角度来看，每个进程都是独享这4G虚拟地址空间的，\n 2、用户态空间布局 #  如上所述，用户空间在虚拟内存中分布在0x0000000-0xBFFFFFFF，大小为3G。\n每一个用户进程，按照访问属性一致的地址空间存放在一起的原则，划分成5个不同的内存区域（访问属性一致指的是：可读，可写，可执行）：\n 代码段：Text Segment，也就是我们的二进制程序，代码段需要防止在运行时被非法修改，所以该段为只读。 数据段：Data Segment，主要存放初始化了的变量，主要包括：静态变量和全局变量，该段为读写。 BSS段：BSS Segment，主要存放未初始化的全局变量，在内存中 bss 段全部置零，该段为读写。 堆段：Heap Segment，主要存放进程运行过程中动态分配的内存段，大小不固定，可动态扩张和缩减，通常使用malloc和free来分配释放，并且堆的增长方向是向上的。 文件映射和匿名映射段：Memory Mapping Segment，主要存放进程使用到的文件或者依赖的动态库，从低地址向上增长。 栈段：Stack Segment，主要存放进程临时创建的局部变量，函数调用上下文信息等，栈向下增长。  一个可执行程序，可以通过size命令，查看编译出来的可执行文件大小，其中包括了代码段，数据段等数据信息，如下:\ndonge@Donge:$ size Donge-Demo text data bss dec hex filename 12538 1916 43632 58086 e2e6 Donge-Demo  text：代码段大小 data：数据段大小 bss：bss段大小 dec：十进制表示的可执行文件大小 hex：十六进制表示的可执行文件大小   运行该程序后，可以通过cat /proc/PID/maps命令，或者pmap PID命令，来查看该进程在虚拟内存空间中的分配情况，其中PID为进程的PID号，如下:\ndonge@Donge:$ cat /proc/16508/maps 55976ff9e000-55976ffa0000 r--p 00000000 08:10 184922 /home/donge/WorkSpace/Program/Donge_Programs/Donge_Demo/build/Donge-Demo 55976ffa0000-55976ffa2000 r-xp 00002000 08:10 184922 /home/donge/WorkSpace/Program/Donge_Programs/Donge_Demo/build/Donge-Demo 55976ffa2000-55976ffa3000 r--p 00004000 08:10 184922 /home/donge/WorkSpace/Program/Donge_Programs/Donge_Demo/build/Donge-Demo 55976ffa3000-55976ffa4000 r--p 00004000 08:10 184922 /home/donge/WorkSpace/Program/Donge_Programs/Donge_Demo/build/Donge-Demo 55976ffa4000-55976ffa5000 rw-p 00005000 08:10 184922 /home/donge/WorkSpace/Program/Donge_Programs/Donge_Demo/build/Donge-Demo 55976ffa5000-55976ffaf000 rw-p 00000000 00:00 0 559771d91000-559771db2000 rw-p 00000000 00:00 0 [heap] 7fec1ad84000-7fec1ad87000 rw-p 00000000 00:00 0 7fec1ad87000-7fec1adaf000 r--p 00000000 08:10 22282 /usr/lib/x86_64-linux-gnu/libc.so.6 7fec1adaf000-7fec1af44000 r-xp 00028000 08:10 22282 /usr/lib/x86_64-linux-gnu/libc.so.6 7fec1af44000-7fec1af9c000 r--p 001bd000 08:10 22282 /usr/lib/x86_64-linux-gnu/libc.so.6 7fec1af9c000-7fec1afa0000 r--p 00214000 08:10 22282 /usr/lib/x86_64-linux-gnu/libc.so.6 7fec1afa0000-7fec1afa2000 rw-p 00218000 08:10 22282 /usr/lib/x86_64-linux-gnu/libc.so.6 7fec1afa2000-7fec1afaf000 rw-p 00000000 00:00 0 7fec1afb5000-7fec1afb7000 rw-p 00000000 00:00 0 7fec1afb7000-7fec1afb9000 r--p 00000000 08:10 22068 /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 7fec1afb9000-7fec1afe3000 r-xp 00002000 08:10 22068 /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 7fec1afe3000-7fec1afee000 r--p 0002c000 08:10 22068 /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 7fec1afef000-7fec1aff1000 r--p 00037000 08:10 22068 /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 7fec1aff1000-7fec1aff3000 rw-p 00039000 08:10 22068 /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 7ffce385d000-7ffce387e000 rw-p 00000000 00:00 0 [stack] 7ffce394e000-7ffce3952000 r--p 00000000 00:00 0 [vvar] 7ffce3952000-7ffce3953000 r-xp 00000000 00:00 0 [vdso] 上面能大致看出该进程的代码段、堆、文件映射段，栈的内存分布等情况，以上就是我们的可执行程序被加载进入内存之后，在用户态虚拟内存空间的布局情况。\n  顺便介绍一下 我的圈子：高级工程师聚集地，期待大家的加入。\n 3、内核态空间布局 #  下面我们来看一下内核态的虚拟空间布局，首先我们要知道：\n 在Linux系统中，用户进程通常只能访问用户空间的虚拟地址，只有在执行内陷操作或系统调用时才能访问内核空间。 所有的进程通过系统调用进入内核态之后，看到的虚拟地址空间都是一样的，他们是共享内核态虚拟内存空间的。   32位的内核态虚拟空间在虚拟内存中分布在0xC00000000-0xFFFFFFFF上，大小为1G，其要分为以下几个区：\n 直接映射区（Direct Memory Region）：顾名思义，直接映射区就是直接与物理内存建立一一映射关系。从内核空间起始地址开始，到896M的内核空间地址区间，为直接内存映射区，该区域线性地址和分配的物理地址都是连续的。   896M以上的内核地址空间，又称为高端内存区域。\n   安全保护区：也成为内存空洞，大小为8M，其主要目的是为了避免 非连续区的非法访问，\n  动态映射区：也就是vmalloc Region，该区域由Vmalloc函数分配，特点是：虚拟地址空间连续，但是物理地址空间不一定连续。\n  永久映射区（Persistent Kernel Mapping Region）：该区域主要用于访问高端内存，通过alloc_page (_GFP_HIGHMEM)接口分配高端内存页，可以使用kmap函数将分配到的高端内存映射到该区域。\n  固定映射区（Fixing kernel Mapping Region）：该区域虚拟内存地址可以自由映射到物理内存的高端地址上，“固定”表现在“虚拟内存空间地址是固定的”，被映射的物理地址是可变的。\n   为什么会有固定映射这个概念呢 ?\n比如：在内核的启动过程中，有些模块需要使用虚拟内存并映射到指定的物理地址上，而且这些模块也没有办法等待完整的内存管理模块初始化之后再进行地址映射。因此，内核固定分配了一些虚拟地址，这些地址有固定的用途，使用该地址的模块在初始化的时候，将这些固定分配的虚拟地址映射到指定的物理地址上去。\n 4、总结 #  以上就是整个虚拟地址空间的划分，总结如下：\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":11,"href":"/docs/linux/linux_api/linux-api-%E6%8F%AD%E7%A7%98container_of%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/","title":"【Linux API 揭秘】container_of函数详解","section":"Linux API 揭秘","content":" Linux Version：6.6\nAuthor：Donge\nGithub：linux-api-insides\n  1、container_of函数介绍 #  container_of可以说是内核中使用最为频繁的一个函数了，简单来说，它的主要作用就是根据我们结构体中的已知的成员变量的地址，来寻求该结构体的首地址，直接看图，更容易理解。\n 下面我们看看linux是如何实现的吧\n 2、container_of函数实现 #  /** * container_of - cast a member of a structure out to the containing structure * @ptr:\tthe pointer to the member. * @type:\tthe type of the container struct this is embedded in. * @member:\tthe name of the member within the struct. * * WARNING: any const qualifier of @ptr is lost. */ #define container_of(ptr, type, member) ({\t\\ void *__mptr = (void *)(ptr);\t\\ static_assert(__same_type(*(ptr), ((type *)0)-\u0026gt;member) ||\t\\ __same_type(*(ptr), void),\t\\ \u0026#34;pointer type mismatch in container_of()\u0026#34;);\t\\ ((type *)(__mptr - offsetof(type, member))); })  函数名称：container_of\n文件位置：include/linux/container_of.h\n该函数里面包括了一些封装好的宏定义以及函数，比如：static_assert、__same_type、offsetof，以及一些指针的特殊用法，比如：(type *)0)，下面我们一一拆解来看。\n2.1 static_assert #  /** * static_assert - check integer constant expression at build time * * static_assert() is a wrapper for the C11 _Static_assert, with a * little macro magic to make the message optional (defaulting to the * stringification of the tested expression). * * Contrary to BUILD_BUG_ON(), static_assert() can be used at global * scope, but requires the expression to be an integer constant * expression (i.e., it is not enough that __builtin_constant_p() is * true for expr). * * Also note that BUILD_BUG_ON() fails the build if the condition is * true, while static_assert() fails the build if the expression is * false. */ #define static_assert(expr, ...) __static_assert(expr, ##__VA_ARGS__, #expr) #define __static_assert(expr, msg, ...) _Static_assert(expr, msg) 函数名称：static_assert\n文件位置：include/linux/build_bug.h\n函数解析：该宏定义主要用来 在编译时检查常量表达式，如果表达式为假，编译将失败，并打印传入的报错信息\n expr：该参数表示传入进来的常量表达式 ...：表示编译失败后，要打印的错误信息 _Static_assert：C11中引入的关键字，用于判断表达式expr并打印错误信息msg。  在container_of函数中，主要用来断言判断\nstatic_assert( __same_type(*(ptr), ((type *)0)-\u0026gt;member) || __same_type(*(ptr), void) , \u0026#34;pointer type mismatch in container_of()\u0026#34; );  2.2 __same_type #  /* Are two types/vars the same type (ignoring qualifiers)? */ #ifndef __same_type # define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b)) #endif 函数名称：__same_type\n文件位置：include/linux/compiler.h\n函数解析：该宏定义用于检查两个变量是否是同种类型\n __builtin_types_compatible_p：gcc的内建函数，判断两个参数的类型是否一致，如果是则返回1 typeof：gcc的关键字，用于获取变量的类型信息  了解完__same_type，想要理解__same_type(*(ptr), ((type *)0)-\u0026gt;member)，需要先弄明白(type *)0的含义。\n  更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  2.3 (type *)0 #  (type *)0，该如何理解这个表达式呢？\n 首先，type是我们传入进来的结构体类型，比如上面讲到的struct test，而这里所做的可以理解为强制类型转换：(struct test *)addr。 addr可以表示内存空间的任意的地址，我们在强制转换后，默认后面一片的内存空间存储的是该数据结构。   而(type *)0的作用，也就是默认将0地址处的内存空间，转换为该数据类型。   我们就把0，当作我们正常的addr地址变量来操作，((type *)0)-\u0026gt;member，就是获取我们结构体的成员对象。 ((type *)0)-\u0026gt;member：是一种常见的技巧，用于直接获取结构体type的成员member的类型，而不需要定义一个type类型的对象。   2.4 offsetof #  #ifndef offsetof #define offsetof(TYPE, MEMBER) ((size_t) \u0026amp;((TYPE *)0)-\u0026gt;MEMBER) #endif 函数名称：offsetof\n文件位置：include/linux/stddef.h\n函数解析：该宏定义用于获取结构体中指定的成员，距离该结构体偏移量。\n TYPE：表示结构体的类型 MEMBER：表示指定的结构体成员 __builtin_offsetof：gcc内置函数，直接返回偏移量。   在新的linux源码中，直接引用了gcc内置的函数，而在老的内核源码中，该偏移量的实现方式如下：\n#define offsetof(TYPE, MEMBER) ((size_t) \u0026amp;((TYPE *)0)-\u0026gt;MEMBER) 同样用到了((TYPE *)addr)，上面我们知道\n ((TYPE *)addr)-\u0026gt;MEMBER：表示获取该结构体的成员 \u0026amp;((TYPE *)addr)-\u0026gt;MEMBER)：加了一个\u0026amp;，表示地址，取该成员的内存地址。  比如我们addr=0x00000010，那么\u0026amp;((TYPE *)0x00000010)-\u0026gt;MEMBER)就相当于0x00000010+size 比如我们addr=0，那么\u0026amp;((TYPE *)0)-\u0026gt;MEMBER)就相当于size     到这里，我们对container_of函数内部涉及的相关知识了然于胸，下面我们再来看container_of，简直容易到起飞。\n 2.5 container_of #  #define container_of(ptr, type, member) ({\t\\ void *__mptr = (void *)(ptr);\t\\ static_assert(__same_type(*(ptr), ((type *)0)-\u0026gt;member) ||\t\\ __same_type(*(ptr), void),\t\\ \u0026#34;pointer type mismatch in container_of()\u0026#34;);\t\\ ((type *)(__mptr - offsetof(type, member))); })  static_assert：断言信息，避免我们传入的参数类型不对，而做的编译检查处理，直接忽略。  #define container_of(ptr, type, member) ({\t\\ void *__mptr = (void *)(ptr);\t\\ ((type *)(__mptr - offsetof(type, member))); })   offsetof(type, member)：计算的是结构体中的成员的偏移量，这里称为size\n  (__mptr - offsetof(type, member))：也就是根据我们已知的成员变量地址，计算出来结构体的首地址\n  ((type *)(__mptr - offsetof(type, member)))：最后强制转换为(type *)，结构体指针。\n   比如，我们已知的结构体成员的地址为0xffff0000，计算之后如下：\n 3、总结 #  linux内核中，小小的一个函数，内部包括的技巧如此之多：static_assert、__same_type、(type *)0、offsetof。\n了解完内部完整的实现手法之后，我们也可以手码一个container_of了 :)\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":12,"href":"/docs/uboot/%E4%BA%8Cuboot%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/","title":"二、uboot启动流程分析","section":"Uboot开发","content":"二、uboot启动流程分析 #   上一篇文章：（一）uboot基础了解 下一篇文章：（三）Uboot驱动模型\n 同大多数的Bootloader一样，uboot的启动过程也分为BL1、BL2两个阶段，分别对应着SPL和Uboot。\nSPL（BL1阶段）：负责开发板的基础配置和设备初始化，并且搬运Uboot到内存中，由汇编代码和少量的C语言实现\nUboot（BL2阶段）：主要负责初始化外部设备，引导Kernel启动，由纯C语言实现。\n 我们这篇文章，主要介绍Uboot（BL2阶段）的启动流程，BL1阶段启动流程的详细分析，可以见我的后续文章。想要深入了解的，可以好好研究下！\n 2.1、程序执行流程图 #  我们先总体来看一下Uboot的执行步骤，这里以EMMC作为启动介质，进行分析！\n无论是哪种启动介质，基本流程都相似，我们这就往下看！\n==打开图片，结合文档、图片、代码进行理解！==\n 2.2、u-boot.lds——Uboot的入口函数 #  u-boot.lds：是uboot工程的链接脚本文件，对于工程的编译和链接有非常重要的作用，决定了uboot的组装，并且u-boot.lds链接文件中的ENTRY(_start)指定了uboot程序的入口地址。\n 如果不知道u-boot.lds放到在哪里，可以通过find -name u-boot.lds查找，根目录要进入到uboot的源码的位置哦！\n如果查找结果有很多，结合自己的板子信息，确定自己使用的u-boot.lds。\n当然，准确的方法是查看Makefile文件，分析出来u-boot.lds所生成的位置。\n 在u-boot.lds的文件中，可以看到.text段，存放的就是执行的文本段。截取部分代码段如下：\nOUTPUT_FORMAT(\u0026#34;elf32-littlearm\u0026#34;, \u0026#34;elf32-littlearm\u0026#34;, \u0026#34;elf32-littlearm\u0026#34;) OUTPUT_ARCH(arm) ENTRY(_start) SECTIONS { . = 0x00000000;\t@起始地址  . = ALIGN(4);\t@四字节对齐  .text :\t{\t*(.__image_copy_start)\t@映像文件复制起始地址 *(.vectors)\t@异常向量表 arch/arm/cpu/armv7/start.o (.text*)\t@启动函数 } ...... }   ENTRY(_start)：程序的入口函数，_start在arch/arm/lib/vectors.S中定义.globl _start\n  SECTIONS定义了段，包括text文本段、data数据段、bss段等。\n  __image_copy_start在System.map和u-boot.map中均有定义\n  arch/arm/cpu/armv7/start.o对应文件arch/arm/cpu/armv7/start.S，该文件中定义了main函数的入口。\n   Tip：上面只进行大概分析，有汇编经验的朋友，可以详细进行分析！\n 2.3、board_init_f——板级前置初始化 #  跟随上文的程序执行流程图，我们看board_init_f这个函数。其位于common/board_f.c。\nvoid board_init_f(ulong boot_flags) { gd-\u0026gt;flags = boot_flags; gd-\u0026gt;have_console = 0; if (initcall_run_list(init_sequence_f)) hang(); } static const init_fnc_t init_sequence_f[] = { setup_mon_len, ... log_init, arch_cpu_init,\t/* basic arch cpu dependent setup */ env_init,\t/* initialize environment */ ... reloc_fdt, reloc_bootstage, reloc_bloblist, setup_reloc, ... } board_init_f()，其最核心的内容就是调用了init_sequence_f初始化序列，进行了一系列初始化的工作。\n主要包括：串口、定时器、设备树、DM驱动模型等，另外还包括global_data结构体相关对象的变量。\n 详细分析，可以看文末的参考文章[1]\n 我们需要注意的一点就是，在初始化队列末尾，执行了几个reloc_xxx的函数，这几个函数实现了Uboot的重定向功能。\n2.4、relocate_code重定向 #   重定向技术，可以说也算是Uboot的一个重点了，也就是将uboot自身镜像拷贝到ddr上的另外一个位置的动作。\n 2.4.1 为什么需要重定向呢？ #   一般需要重定向的条件如下：\n  uboot存储在只读存储器上，比如ROM、Nor flash，需要将代码拷贝到DDR上，才能完整运行Uboot。 为Kernel腾空间，Kernel一般会放在DDR的地段地址上，所以要把Uboot重定向到顶端地址，避免冲突。  2.4.2 Uboot是如何重定向的？ #  Uboot的重定向有如下几个步骤：\n 对relocate进行空间划分 计算uboot代码空间到relocate的位置的偏移 relocate旧的global_data到新的global_data空间上 relocate Uboot 修改relocate后的全局变量的label relocate中断向量表  运行大致流程：\narch/arm/lib/crt0.S文件内，主要实现了：\nENTRY(_main) bl board_init_f @@ 在board_init_f里面实现了 @@ （1）对relocate进行空间规划 @@ （2）计算uboot代码空间到relocation的位置的偏移 @@ （3）relocate旧的global_data到新的global_data的空间上 ldr sp, [r9, #GD_START_ADDR_SP] /* sp = gd-\u0026gt;start_addr_sp */  bic sp, sp, #7 /* 8-byte alignment for ABI compliance */  ldr r9, [r9, #GD_BD] /* r9 = gd-\u0026gt;bd */  sub r9, r9, #GD_SIZE /* new GD is below bd */ @@ 把新的global_data地址放在r9寄存器中 adr lr, here ldr r0, [r9, #GD_RELOC_OFF] /* r0 = gd-\u0026gt;reloc_off */  add lr, lr, r0 @@ 计算返回地址在新的uboot空间中的地址。b调用函数返回之后，就跳到了新的uboot代码空间中。 ldr r0, [r9, #GD_RELOCADDR] /* r0 = gd-\u0026gt;relocaddr */ @@ 把uboot的新的地址空间放到r0寄存器中，作为relocate_code的参数 b relocate_code @@ 跳转到relocate_code中，在这里面实现了 @@ （1）relocate旧的uboot代码空间到新的空间上去 @@ （2）修改relocate之后全局变量的label @@ 注意，由于上述已经把lr寄存器重定义到uboot新的代码空间中了，所以返回之后，就已经跳到了新的代码空间了！！！！！！ bl relocate_vectors @@ relocate中断向量表  setup_reloc——重定向地址查看（仿真有关）  在这里我们说明一下board_init_f里面的setup_reloc初始化函数\nstatic int setup_reloc(void) { if (gd-\u0026gt;flags \u0026amp; GD_FLG_SKIP_RELOC) { debug(\u0026#34;Skipping relocation due to flag\\n\u0026#34;); return 0; } #ifdef CONFIG_SYS_TEXT_BASE #ifdef ARM  gd-\u0026gt;reloc_off = gd-\u0026gt;relocaddr - (unsigned long)__image_copy_start; #elif defined(CONFIG_M68K)  /* * On all ColdFire arch cpu, monitor code starts always * just after the default vector table location, so at 0x400 */ gd-\u0026gt;reloc_off = gd-\u0026gt;relocaddr - (CONFIG_SYS_TEXT_BASE + 0x400); #elif !defined(CONFIG_SANDBOX)  gd-\u0026gt;reloc_off = gd-\u0026gt;relocaddr - CONFIG_SYS_TEXT_BASE; #endif #endif  memcpy(gd-\u0026gt;new_gd, (char *)gd, sizeof(gd_t)); debug(\u0026#34;Relocation Offset is: %08lx\\n\u0026#34;, gd-\u0026gt;reloc_off); if (is_debug_open()) { printf(\u0026#34;Relocating to %08lx, new gd at %08lx, sp at %08lx\\n\u0026#34;, gd-\u0026gt;relocaddr, (ulong)map_to_sysmem(gd-\u0026gt;new_gd), gd-\u0026gt;start_addr_sp); } return 0; } 由于，Uboot进行了重定向，所以按照常规的地址仿真的话，我们可能访问到错误的内存空间，通过setup_reloc的Relocating to %08lx打印，我们可以得到重定向后的地址，方便我们仿真。\nUboot的重定向也有相当大的一部分知识点，上面也仅仅是简单介绍了relocate的基本步骤和流程，后续看大家需要，如果大家想了解，我再补上这一部分。\n2.4.3 Uboot重定向作用 #  总之，Uboot重定向之后，把Uboot整体搬运到了高端内存区，为Kernel的加载提供空间，避免内存践踏。\n2.5、board_init_r——板级后置初始化 #   我们接着跟着流程图往下看，重定向之后，Uboot运行于新的地址空间，接着我们执行board_init_r，主要作为Uboot运行的最后初始化步骤。\n board_init_r这个函数，同样位于common/board_f.c，主要用于初始化各类外设信息\nvoid board_init_r(gd_t *new_gd, ulong dest_addr) {\tif (initcall_run_list(init_sequence_r)) hang(); /* NOTREACHED - run_main_loop() does not return */ hang(); } static init_fnc_t init_sequence_r[] = { initr_reloc, initr_reloc_global_data, board_init,\t/* Setup chipselects */ initr_dm, initr_mmc, ... run_main_loop } 与board_init_f相同，同样有一个init_sequence_r初始化列表，包括：initr_dmDM模型初始化，initr_mmcMMC驱动初始化，等等。\n最终，uboot就运行到了run_main_loop，进而执行main_loop这个函数。\n2.6、main_loop——Uboot主循环 #   该函数为Uboot的最终执行函数，无论是加载kernel还是uboot的命令行体系，均由此实现。\n void main_loop(void) { const char *s; bootstage_mark_name(BOOTSTAGE_ID_MAIN_LOOP, \u0026#34;main_loop\u0026#34;); if (IS_ENABLED(CONFIG_VERSION_VARIABLE)) env_set(\u0026#34;ver\u0026#34;, version_string); /* set version variable */ cli_init(); if (IS_ENABLED(CONFIG_USE_PREBOOT)) run_preboot_environment_command(); if (IS_ENABLED(CONFIG_UPDATE_TFTP)) update_tftp(0UL, NULL, NULL); s = bootdelay_process(); if (cli_process_fdt(\u0026amp;s)) cli_secure_boot_cmd(s); autoboot_command(s); cli_loop(); panic(\u0026#34;No CLI available\u0026#34;); } env_set：设置环境变量，两个参数分别为name和value\ncli_init：用于初始化hash shell的一些变量\nrun_preboot_environment_command：执行预定义的环境变量的命令\nbootdelay_process：加载延时处理，一般用于Uboot启动后，有几秒的倒计时，用于进入命令行模式。\ncli_loop：命令行模式，主要作用于Uboot的命令行交互。\n2.6.1 bootdelay_process #   记得对照文章开始的执行流程图哦！\n 详细解释标注于代码中\u0026hellip;\u0026hellip;\nconst char *bootdelay_process(void) { char *s; int bootdelay; bootcount_inc(); s = env_get(\u0026#34;bootdelay\u0026#34;);\t//先判断是否有bootdelay环境变量，如果没有，就使用menuconfig中配置的CONFIG_BOOTDELAY时间  bootdelay = s ? (int)simple_strtol(s, NULL, 10) : CONFIG_BOOTDELAY; if (IS_ENABLED(CONFIG_OF_CONTROL))\t//是否使用设备树进行配置  bootdelay = fdtdec_get_config_int(gd-\u0026gt;fdt_blob, \u0026#34;bootdelay\u0026#34;, bootdelay); debug(\u0026#34;### main_loop entered: bootdelay=%d\\n\\n\u0026#34;, bootdelay); if (IS_ENABLED(CONFIG_AUTOBOOT_MENU_SHOW)) bootdelay = menu_show(bootdelay); bootretry_init_cmd_timeout(); #ifdef CONFIG_POST  if (gd-\u0026gt;flags \u0026amp; GD_FLG_POSTFAIL) { s = env_get(\u0026#34;failbootcmd\u0026#34;); } else #endif /* CONFIG_POST */ if (bootcount_error()) s = env_get(\u0026#34;altbootcmd\u0026#34;); else s = env_get(\u0026#34;bootcmd\u0026#34;);\t//获取bootcmd环境变量，用于后续的命令执行  if (IS_ENABLED(CONFIG_OF_CONTROL)) process_fdt_options(gd-\u0026gt;fdt_blob); stored_bootdelay = bootdelay; return s; } 2.6.2 autoboot_command #  详细解释标注于代码中\u0026hellip;\u0026hellip;\nvoid autoboot_command(const char *s) { debug(\u0026#34;### main_loop: bootcmd=\\\u0026#34;%s\\\u0026#34;\\n\u0026#34;, s ? s : \u0026#34;\u0026lt;UNDEFINED\u0026gt;\u0026#34;); if (stored_bootdelay != -1 \u0026amp;\u0026amp; s \u0026amp;\u0026amp; !abortboot(stored_bootdelay)) { bool lock; int prev; lock = IS_ENABLED(CONFIG_AUTOBOOT_KEYED) \u0026amp;\u0026amp; !IS_ENABLED(CONFIG_AUTOBOOT_KEYED_CTRLC); if (lock) prev = disable_ctrlc(1); /* disable Ctrl-C checking */ run_command_list(s, -1, 0); if (lock) disable_ctrlc(prev);\t/* restore Ctrl-C checking */ } if (IS_ENABLED(CONFIG_USE_AUTOBOOT_MENUKEY) \u0026amp;\u0026amp; menukey == AUTOBOOT_MENUKEY) { s = env_get(\u0026#34;menucmd\u0026#34;); if (s) run_command_list(s, -1, 0); } } 我们看一下判断条件stored_bootdelay != -1 \u0026amp;\u0026amp; s \u0026amp;\u0026amp; !abortboot(stored_bootdelay\n stored_bootdelay：为环境变量的值，或者menuconfig设置的值 s：为环境变量bootcmd的值，为后续运行的指令 abortboot(stored_bootdelay)：主要用于判断是否有按键按下。如果按下，则不执行bootcmd命令，进入cli_loop 命令行模式；如果不按下，则执行bootcmd命令，跳转到加载Linux启动。  2.6.3 cli_loop #  void cli_loop(void) { bootstage_mark(BOOTSTAGE_ID_ENTER_CLI_LOOP); #ifdef CONFIG_HUSH_PARSER  parse_file_outer(); /* This point is never reached */ for (;;);\t//死循环 #elif defined(CONFIG_CMDLINE)  cli_simple_loop(); #else  printf(\u0026#34;## U-Boot command line is disabled. Please enable CONFIG_CMDLINE\\n\u0026#34;); #endif /*CONFIG_HUSH_PARSER*/} 如上代码，程序只执行parse_file_outer来处理用户的输入、输出信息。\n 好啦，基本到这里，我们已经对Uboot的启动流程了然于胸了吧！\n当然，更深层次的不建议去深入了解，有时间可以慢慢去研究。\n 大家有疑问，可以评论区交流\u0026hellip;\u0026hellip;\n参考文章：\n[1]：boadr_init_f介绍\n[2]：启动流程参考\n[3]：main_loop相关\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":13,"href":"/docs/embeded_tech/embeded_interview/linux%E7%94%A8%E6%88%B7%E6%80%81%E5%92%8C%E5%86%85%E6%A0%B8%E6%80%81%E4%BA%A4%E4%BA%92%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/","title":"Linux用户态和内核态交互的几种方式","section":"嵌入式面经","content":"Linux用户态和内核态交互的几种方式 #  Linux分为内核态Kernel Mode和用户态User Mode，其通信方式主要有：\n 系统调用System Call：最常见的用户态和内核态之间的通信方式。通过系统调用接口（open、read、write、fork等）请求内核执行特定的动作。 中断Interrupts：中断包括软中断和硬中断，每当中断到来的时候，CPU会暂停当前执行的用户态代码，切换到内核态来处理中断。 信号Signal：内核通过Signal通知用户态进程发生了某些事件，用户态注册信号处理函数，来响应特定的信号事件。如 SIGTERM、SIGINT 等。 共享内存Share Memory：允许多个进程在它们的地址空间中共享一块内存区域，从而实现用户态和内核态之间的高效通信。这种方式避免了用户态和内核态之间频繁切换的问题，但是也需要考虑到数据的同步问题，保证数据一致性。   用户态User Mode访问内核态Kernel Mode的数据交互的方式有：\n  procfs进程文件系统：一个伪文件系统，因为其不占用外部存储空间，只占有少量的内存，挂载在/proc目录下\n  sysctl：它也是一个Linux命令，主要用来修改内核的运行时参数，也就是在内核运行时，动态修改内核参数。\n 和 procfs 的区别在于：procfs 主要是输出只读数据，而 sysctl 输出的大部分信息是可写的。\n   sysfs虚拟文件系统：通过/sys来完成用户态和内核的通信，和 procfs 不同的是，sysfs 是将一些原本在 procfs 中的，关于设备和驱动的部分，独立出来，以 “设备树” 的形式呈现给用户。\n  netlink 接口：也是最常用的一种方式，本质是socket接口，使用netlink用于网络相关的内核和用户进程之间的消息传递。\n  共享内存Share Memory：允许多个进程在它们的地址空间中共享一块内存区域，从而实现用户态和内核态之间的高效数据传输。\n   欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":14,"href":"/docs/uboot/%E4%B8%89uboot%E9%A9%B1%E5%8A%A8%E6%A8%A1%E5%9E%8B/","title":"三、Uboot驱动模型","section":"Uboot开发","content":"三、Uboot驱动模型 #   全文耗时一周，精心汇总，希望对大家有所帮助，感觉可以的点赞，关注，不迷路，后续还有更多干货！\n看文章前，答应我，静下心来，慢慢品！\n 3.1、什么是Uboot驱动模型 #  学过Linux的朋友基本都知道Linux的设备驱动模型，Uboot根据Linux的驱动模型架构，也引入了Uboot的驱动模型（driver model ：DM）。\n**这种驱动模型为驱动的定义和访问接口提供了统一的方法。**提高了驱动之间的兼容性以及访问的标准型，uboot驱动模型和kernel中的设备驱动模型类似。\n3.2、为什么要有驱动模型呢 #   无论是Linux还是Uboot，一个新对象的产生必定有其要解决的问题，驱动模型也不例外！\n  提高代码的可重用性：为了能够使代码在不同硬件平台，不同体系架构下运行，必须要最大限度的提高代码的可重用性。 高内聚，低耦合：分层的思想也是为了达到这一目标，低耦合体现在对外提供统一的抽象访问接口，高内聚将相关度紧密的集中抽象实现。 便于管理：在不断发展过程中，硬件设备越来越多，驱动程序也越来越多，为了更好的管理驱动，也需要一套优秀的驱动架构！  3.3、如何使用uboot的DM模型 #   DM模型的使用，可以通过menuconfig来配置。\nmake menuconfig\n ①：menuconfig配置全局DM模型 #  Device Drivers -\u0026gt; Generic Driver Options -\u0026gt; Enable Driver Model 通过上面的路径来打开Driver Model模型，最终配置在.config文件中，CONFIG_DM=y\n②：指定某个驱动的DM模型 #  全局的DM模型打开后，我们对于不通的驱动模块，使能或者失能DM功能。如MMC驱动为例：\nDevice Drivers -\u0026gt; MMC Host controller Support -\u0026gt; Enable MMC controllers using Driver Model 最终反映在.config文件中的CONFIG_DM_MMC=y\n在对应的驱动中，可以看到判断#if !CONFIG_IS_ENABLED(DM_MMC)，来判断是否打开DM驱动模型。\n在管理驱动的Makefile文件中，也能看到obj-$(CONFIG_$(SPL_)DM_MMC) += mmc-uclass.o，来判断是否将驱动模型加入到编译选项中。\n总之，我们要打开DM模型，最后反映在几个配置信息上：\n CONFIG_DM=y，全局DM模型打开 CONFIG_DM_XXX=y，某个驱动的DM模型的打开 可以通过Kconifg、Makefile来查看对应宏的编译情况  3.4、DM模型数据结构 #  要想了解DM模型整套驱动框架，我们必须先了解它的一砖一瓦！也就是组成驱动框架的各个数据结构。\n① global_data #  typedef struct global_data { ... #ifdef CONFIG_DM  struct udevice\t*dm_root;\t/* Root instance for Driver Model */ struct udevice\t*dm_root_f;\t/* Pre-relocation root instance */ struct list_head uclass_root;\t/* Head of core tree */ #endif ... } global_data，管理着整个Uboot的全局变量，其中dm_root，dm_root_f，uclass_root用来管理整个DM模型。这几个变量代表什么意思呢？\n dm_root：DM模型的根设备 dm_root_f：重定向前的根设备 uclass_root：uclass链表的头  这几个变量，最终要的作用就是：管理整个模型中的udevice设备信息和uclass驱动类。\n② uclass #  我们首先看一下uclass这个结构体\n/** * struct uclass - a U-Boot drive class, collecting together similar drivers * * A uclass provides an interface to a particular function, which is * implemented by one or more drivers. Every driver belongs to a uclass even * if it is the only driver in that uclass. An example uclass is GPIO, which * provides the ability to change read inputs, set and clear outputs, etc. * There may be drivers for on-chip SoC GPIO banks, I2C GPIO expanders and * PMIC IO lines, all made available in a unified way through the uclass. * * @priv: Private data for this uclass * @uc_drv: The driver for the uclass itself, not to be confused with a * \u0026#39;struct driver\u0026#39; * @dev_head: List of devices in this uclass (devices are attached to their * uclass when their bind method is called) * @sibling_node: Next uclass in the linked list of uclasses */ struct uclass { void *priv;\t//uclass的私有数据  struct uclass_driver *uc_drv;\t//uclass类的操作函数集合  struct list_head dev_head;\t//该uclass的所有设备  struct list_head sibling_node;\t//下一个uclass的节点 }; 根据注释，我们就可以了解到，uclass相当于老师，管理着==对应某一个类别下==的所有的udevice。\n 例如：一个IIC驱动程序，其驱动程序框架是一致的，只有一种，但是IIC驱动的设备可以有很多，如EEPROM，MCU6050等；\n所有在这里呢，dev_head链表就是用来管理该驱动类下的所有的设备。\n 总结：uclass，来管理该类型下的所有设备，并且有对应的uclass_driver驱动。\n  定义 #    uclass是uboot自动生成的，并且不是所有uclass都会生成，有对应uclass_driver并且有被udevice匹配到的uclass才会生成。\n  存放 #    所有生成的uclass都会被挂载gd-\u0026gt;uclass_root链表上。\n  相关API #     直接遍历链表gd-\u0026gt;uclass_root链表并且根据uclass_id来获取到相应的uclass。\n int uclass_get(enum uclass_id key, struct uclass **ucp); // 从gd-\u0026gt;uclass_root链表获取对应的ucla ss ③ uclass_driver #  正如上面，我们看到了uclass类所包含uclass_driver结构体，uclass_driver正如其名，它就是uclass的驱动程序。其主要作用是：为uclass提供统一管理的接口，结构体如下：\n/** * struct uclass_driver - Driver for the uclass * * A uclass_driver provides a consistent interface to a set of related * drivers. */ struct uclass_driver { const char *name; // 该uclass_driver的命令  enum uclass_id id; // 对应的uclass id /* 以下函数指针主要是调用时机的区别 */ int (*post_bind)(struct udevice *dev); // 在udevice被绑定到该uclass之后调用  int (*pre_unbind)(struct udevice *dev); // 在udevice被解绑出该uclass之前调用  int (*pre_probe)(struct udevice *dev); // 在该uclass的一个udevice进行probe之前调用  int (*post_probe)(struct udevice *dev); // 在该uclass的一个udevice进行probe之后调用  int (*pre_remove)(struct udevice *dev);// 在该uclass的一个udevice进行remove之前调用  int (*child_post_bind)(struct udevice *dev); // 在该uclass的一个udevice的一个子设备被绑定到该udevice之后调用  int (*child_pre_probe)(struct udevice *dev); // 在该uclass的一个udevice的一个子设备进行probe之前调用  int (*init)(struct uclass *class); // 安装该uclass的时候调用  int (*destroy)(struct uclass *class); // 销毁该uclass的时候调用  int priv_auto_alloc_size; // 需要为对应的uclass分配多少私有数据  int per_device_auto_alloc_size; //  int per_device_platdata_auto_alloc_size; //  int per_child_auto_alloc_size; //  int per_child_platdata_auto_alloc_size; //  const void *ops; //操作集合  uint32_t flags; // 标识为 };   定义 #    uclass_driver主要通过UCLASS_DRIVER来定义，这里就简单说明一下底层代码，耐心看哦！\n 下面以pinctrl为例\n UCLASS_DRIVER(pinctrl) = { .id = UCLASS_PINCTRL, .post_bind = pinctrl_post_bind, .flags = DM_UC_FLAG_SEQ_ALIAS, .name = \u0026#34;pinctrl\u0026#34;, }; /* Declare a new uclass_driver */ #define UCLASS_DRIVER(__name)\t\\ ll_entry_declare(struct uclass_driver, __name, uclass)  #define ll_entry_declare(_type, _name, _list)\t\\ _type _u_boot_list_2_##_list##_2_##_name __aligned(4)\t\\ __attribute__((unused,\t\\ section(\u0026#34;.u_boot_list_2_\u0026#34;#_list\u0026#34;_2_\u0026#34;#_name))) 上面基本上就是我们的底层代码了，稍微有点绕，但是也不难！我们只需要将宏进行替换就行了！\n通过上面的定义，我们替换掉宏之后，最终得到的定义如下：\nstruct uclass_driver _u_boot_list_2_uclass_2_pinctrl = { .id = UCLASS_PINCTRL, .post_bind = pinctrl_post_bind, .flags = DM_UC_FLAG_SEQ_ALIAS, .name = \u0026#34;pinctrl\u0026#34;, } //同时存放在段._u_boot_list_2_uclass_2_pinctrl中，也就是section段的内容   存放 #    由上面结构体可得，其定义之后都被存放在了段._u_boot_list_2_uclass_2_pinctrl中，那么去哪里可以看到呢？\n在u-boot.map文件中搜索，._u_boot_list_2_uclass_2_pinctrl，就可以查到程序中定义的所有驱动程序。\n这里相信大家会有疑问，为什么是uclass_2呢？我们大概看一下，也会看到uclass_1和uclass_3，这两个代表什么呢？往下看！\n  相关API #     想要获取uclass_driver需要先获取uclass_driver table。\n struct uclass_driver *uclass = ll_entry_start(struct uclass_driver, uclass); // 会根据.u_boot_list_2_uclass_1的段地址来得到uclass_driver table的地址  const int n_ents = ll_entry_count(struct uclass_driver, uclass); // 获得uclass_driver table的长度  struct uclass_driver *lists_uclass_lookup(enum uclass_id id) // 从uclass_driver table中获取uclass id为id的uclass_driver。 正如注释描述，上文中提到的uclass_1和uclass_3起到定位作用，用于计算uclass_2的长度！\n上述的API，主要用于根据uclass_id来查找到对应的uclass_driver，进而操作对应的uclass下的udevice。\n④ uclass_id #  我们在uclass_driver中，看到一个uclass_id类型，这种类型与uclass有什么关系呢？\n我们知道，uclass代表驱动的一个类别，uclass_driver是uclass的驱动程序，为uclass提供统一操作接口。而对于不同类型的驱动，就需要uclass_id来区分了！\n事实上，每一种类型的设备uclass都有唯一对应的uclass_id，贯穿设备模型，也是udevice与uclass相关联的关键之处。\nenum uclass_id { /* These are used internally by driver model */ UCLASS_ROOT = 0, UCLASS_DEMO, UCLASS_TEST, UCLASS_TEST_FDT, UCLASS_TEST_BUS, UCLASS_TEST_PROBE, ...... /* U-Boot uclasses start here - in alphabetical order */ UCLASS_ACPI_PMC,\t/* (x86) Power-management controller (PMC) */ UCLASS_ADC,\t/* Analog-to-digital converter */ UCLASS_AHCI,\t/* SATA disk controller */ UCLASS_AUDIO_CODEC,\t/* Audio codec with control and data path */ UCLASS_AXI,\t/* AXI bus */ UCLASS_BLK,\t/* Block device */ UCLASS_BOARD,\t/* Device information from hardware */ ...... }; 在这里，我们就把他当作一个设备识别的标志即可！\n 最后，压轴的两个结构体出来了，也是DM模型最终操作的对象。\n ⑤ udevice #  /** * struct udevice - An instance of a driver * * This holds information about a device, which is a driver bound to a * particular port or peripheral (essentially a driver instance). * */ struct udevice { const struct driver *driver;\t//device 对应的driver  const char *name;\t//device 的名称  void *platdata; void *parent_platdata; void *uclass_platdata; ofnode node;\t//设备树节点  ulong driver_data; struct udevice *parent;\t//父设备  void *priv;\t// 私有数据的指针  struct uclass *uclass;\t//驱动所属的uclass  void *uclass_priv; void *parent_priv; struct list_head uclass_node; struct list_head child_head; struct list_head sibling_node; uint32_t flags; int req_seq; int seq; #ifdef CONFIG_DEVRES  struct list_head devres_head; #endif };   定义 #   **硬编码：**代码中调用U_BOOT_DEVICE宏来定义设备资源，实际为一个设备实例。 **设备树：**将设备描述信息写在对应的DTS文件中，然后编译成DTB，最终由uboot解析设备树后动态生成的。 传参方式：通过命令行或者接口将设备资源信息传递进来，非常灵活。    存放 #    udevice是最基础的一个设备单元，我们把它作为一个独立的个体，上层所有的操作，最终都与该结构体有关。\n我们创建一个设备后，为了服从统一的管理，该结构体会被连接到DM模型下，并入到机制中。那么udevice会被连接到哪里呢？\n 将udevice连接到对应的uclass中，uclass主要用来管理着同一类的驱动 除此之外，有父子关系的udevice，还会连接到udevice-\u0026gt;child_head链表下，方便调用  大概可以理解为下面这样：\n  相关API #    #define uclass_foreach_dev(pos, uc) \\ list_for_each_entry(pos, \u0026amp;uc-\u0026gt;dev_head, uclass_node)  #define uclass_foreach_dev_safe(pos, next, uc) \\ list_for_each_entry_safe(pos, next, \u0026amp;uc-\u0026gt;dev_head, uclass_node)  int uclass_get_device(enum uclass_id id, int index, struct udevice **devp); // 通过索引从uclass中获取udevice int uclass_get_device_by_name(enum uclass_id id, const char *name, // 通过设备名从uclass中获取udevice  struct udevice **devp); int uclass_get_device_by_seq(enum uclass_id id, int seq, struct udevice **devp); int uclass_get_device_by_of_offset(enum uclass_id id, int node, struct udevice **devp); int uclass_get_device_by_phandle(enum uclass_id id, struct udevice *parent, const char *name, struct udevice **devp); int uclass_first_device(enum uclass_id id, struct udevice **devp); int uclass_first_device_err(enum uclass_id id, struct udevice **devp); int uclass_next_device(struct udevice **devp); int uclass_resolve_seq(struct udevice *dev); 这些相关的API，主要作用就是根据uclass_id，查找对应的uclass，然后根据索引值或者名称，来查找到对应的udevice\n③ driver #  struct driver { char *name;\t//驱动名称  enum uclass_id id;\t//驱动所对应的uclass_id\t const struct udevice_id *of_match;\t//匹配函数  int (*bind)(struct udevice *dev);\t//绑定函数  int (*probe)(struct udevice *dev);\t//注册函数  int (*remove)(struct udevice *dev); int (*unbind)(struct udevice *dev); int (*ofdata_to_platdata)(struct udevice *dev); int (*child_post_bind)(struct udevice *dev); int (*child_pre_probe)(struct udevice *dev); int (*child_post_remove)(struct udevice *dev); int priv_auto_alloc_size; int platdata_auto_alloc_size; int per_child_auto_alloc_size; int per_child_platdata_auto_alloc_size; const void *ops;\t/* driver-specific operations */ uint32_t flags; #if CONFIG_IS_ENABLED(ACPIGEN)  struct acpi_ops *acpi_ops; #endif };   定义 #    driver对象，主要通过U_BOOT_DRIVER来定义\n 以pinctrl来举例\n U_BOOT_DRIVER(xxx_pinctrl) = { .name\t= \u0026#34;xxx_pinctrl\u0026#34;, .id\t= UCLASS_PINCTRL, .of_match\t= arobot_pinctrl_match, .priv_auto_alloc_size = sizeof(struct xxx_pinctrl), .ops\t= \u0026amp;arobot_pinctrl_ops, .probe\t= arobot_v2s_pinctrl_probe, .remove = arobot_v2s_pinctrl_remove, }; /* Declare a new U-Boot driver */ #define U_BOOT_DRIVER(__name)\t\\ ll_entry_declare(struct driver, __name, driver)  #define ll_entry_declare(_type, _name, _list)\t\\ _type _u_boot_list_2_##_list##_2_##_name __aligned(4)\t\\ __attribute__((unused,\t\\ section(\u0026#34;.u_boot_list_2_\u0026#34;#_list\u0026#34;_2_\u0026#34;#_name))) 通过上面的定义，最终我们定义的结构体如下：\nstruct driver _u_boot_list_2_driver_2_xxx_pinctrl = { .name\t= \u0026#34;xxx_pinctrl\u0026#34;, .id\t= UCLASS_PINCTRL, .of_match\t= arobot_pinctrl_match, .priv_auto_alloc_size = sizeof(struct xxx_pinctrl), .ops\t= \u0026amp;arobot_pinctrl_ops, .probe\t= arobot_v2s_pinctrl_probe, .remove = arobot_v2s_pinctrl_remove, } //同时存放在段._u_boot_list_2_driver_2_xxx_pinctrl中   存放 #    由上面结构体可得，其定义之后都被存放在了段._u_boot_list_2_driver_2_xxx中，那么去哪里可以看到呢？\n在u-boot.map文件中搜索，._u_boot_list_2_driver，就可以查到程序中定义的所有驱动程序。\n最终，所有driver结构体以列表的形式被放在.u_boot_list_2_driver_1和.u_boot_list_2_driver_3的区间中。\n  相关API #    /*先获取driver table 表*/ struct driver *drv = ll_entry_start(struct driver, driver);\t// 会根据.u_boot_list_2_driver_1的段地址来得到uclass_driver table的地址  const int n_ents = ll_entry_count(struct driver, driver);\t// 通过.u_boot_list_2_driver_3的段地址 减去 .u_boot_list_2_driver_1的段地址 获得driver table的长度  /*遍历所有的driver*/ struct driver *lists_driver_lookup_name(const char *name)\t// 从driver table中获取名字为name的driver。 正如注释描述，上文中提到的driver_1和driver_3起到定位作用，用于计算driver_2的长度！\n上述的API，主要用于根据name来查找到对应的driver驱动程序。\n综上，DM模型相关的数据结构介绍完毕，整体设计的架构如下：\n正如红线部分，如何实现driver和udevice的绑定、uclass、uclass_driver的绑定呢？\n要想真正搞懂这些，我们不得不去深入到DM的初始化流程。\n3.5、DM驱动模型之上帝视角 #   对于DM模型，我们站在上帝视角来观察整套模型框架是如何的！\n 从对象设计的角度来看，Uboot的驱动模型可以分为静态形式和动态形式。\n  **静态模式：**对象是离散的，和其他对象分隔开，减小对象复杂度，利于模块化设计。\n  动态模式：运行态表达形式的对象是把所有的静态对象组合成层次视图，有清晰的数据关联视图\n  在静态模式下，驱动模型主要将对象分为udevice和driver，即设备和驱动程序，两个就像火车的两条轨道，永远也不会产生交集，驱动和设备可以想注册多少就注册多少。\n我们看一下udevice的描述：\n/** * struct udevice - An instance of a driver * * This holds information about a device, which is a driver bound to a * particular port or peripheral (essentially a driver instance). * */ udevice是driver的一个实例，两个不相交的铁轨，终归也是想要发生爱情的。那么如何让其产生交集呢？这就是动态模式需要做的工作了！\n**在动态模式下，**引入了uclass和uclass_driver两个数据结构，实现了对udevice和driver的管理。\n看一下uclass和uclass_driver两个结构体的说明：\n/** * struct uclass - a U-Boot drive class, collecting together similar drivers * */ /** * struct uclass_driver - Driver for the uclass * * A uclass_driver provides a consistent interface to a set of related * drivers. * */  **uclass：**设备组公共属性对象，作为udevice的一个属性，主要用来管理某个驱动类的所有的设备。 **uclass_driver：**设备组公共行为对象，uclass的驱动程序，主要将uclass管理的设备和驱动实现绑定、注册，移除等操作。  通过这两个结构体的引入，可以将毫不相关的udevice是driver关联起来！\nudevice与driver的绑定：通过驱动的of_match和compatible属性来配对，绑定。\nudevice与uclass的绑定：udevice内的driver下的uclass_id，来与uclass对应的uclass_driver的uclass_id进行匹配。\nuclass与uclass_driver的绑定：已知udevice内的driver下的uclass_id，创建uclass的同时，通过``uclass_id找到对应的uclass_driver对象，然后将uclass_driver绑定到uclass`上！\n整体结构如下：\n3.6、DM模型——Udevice与driver绑定 #   相信站在上帝视角看完DM的整体架构，大家都对DM框架有一定了解，下面我们来看看具体的实现细节！\n DM的初始化分为两个部分，一个是在relocate重定向之前的初始化：initf_dm，一个是在relocate重定向之后的初始化：initr_dm。\n我们对比这两个函数：\nstatic int initf_dm(void) { #if defined(CONFIG_DM) \u0026amp;\u0026amp; CONFIG_VAL(SYS_MALLOC_F_LEN)  int ret; bootstage_start(BOOTSTAGE_ID_ACCUM_DM_F, \u0026#34;dm_f\u0026#34;); ret = dm_init_and_scan(true);\t//这里为true  bootstage_accum(BOOTSTAGE_ID_ACCUM_DM_F); if (ret) return ret; #endif #ifdef CONFIG_TIMER_EARLY  ret = dm_timer_init(); if (ret) return ret; #endif  return 0; } static int initr_dm(void) { int ret; /* Save the pre-reloc driver model and start a new one */ gd-\u0026gt;dm_root_f = gd-\u0026gt;dm_root; gd-\u0026gt;dm_root = NULL; #ifdef CONFIG_TIMER  gd-\u0026gt;timer = NULL; #endif  bootstage_start(BOOTSTAGE_ID_ACCUM_DM_R, \u0026#34;dm_r\u0026#34;); ret = dm_init_and_scan(false);\t//这里为false  bootstage_accum(BOOTSTAGE_ID_ACCUM_DM_R); if (ret) return ret; return 0; } 两个均调用了dm_init_and_scan这个接口，这两个的关键区别在于参数的不同。\n 首先说明一下dts节点中的“u-boot,dm-pre-reloc”属性，当设置了这个属性时，则表示这个设备在relocate之前就需要使用。 当dm_init_and_scan的参数为true时，只会对带有“u-boot,dm-pre-reloc”属性的节点进行解析。而当参数为false的时候，则会对所有节点都进行解析。  DM初始化的大体步骤如下：\n 如上程序执行流程图，下面我们详细讲解几个函数。\n ① dm_init #  int dm_init(bool of_live) { int ret; if (gd-\u0026gt;dm_root) { dm_warn(\u0026#34;Virtual root driver already exists!\\n\u0026#34;); return -EINVAL; } INIT_LIST_HEAD(\u0026amp;DM_UCLASS_ROOT_NON_CONST); #if defined(CONFIG_NEEDS_MANUAL_RELOC)  fix_drivers(); fix_uclass(); fix_devices(); #endif  ret = device_bind_by_name(NULL, false, \u0026amp;root_info, \u0026amp;DM_ROOT_NON_CONST);\t//查找root_driver驱动，并绑定  if (ret) return ret; #if CONFIG_IS_ENABLED(OF_CONTROL) # if CONFIG_IS_ENABLED(OF_LIVE)  if (of_live) DM_ROOT_NON_CONST-\u0026gt;node = np_to_ofnode(gd-\u0026gt;of_root); else #endif  DM_ROOT_NON_CONST-\u0026gt;node = offset_to_ofnode(0); #endif  ret = device_probe(DM_ROOT_NON_CONST);\t//probe激活root_driver驱动  if (ret) return ret; return 0; } dm_init这个函数，名字起的容易让人误导，这个函数主要做的就是初始化了根设备root_driver，根据这个跟设备，初始化了global_data中的dm_root、uclass_root。\n② lists_bind_fdt #   我们通常会使用设备树来定义各种设备，所以这个函数才是主角。\n 这个函数主要用来查找子设备，并且根据查找到的子设备，进而查找对应驱动进行绑定！即：实现了driver和device的绑定。\nint lists_bind_fdt(struct udevice *parent, ofnode node, struct udevice **devp, bool pre_reloc_only) { struct driver *driver = ll_entry_start(struct driver, driver);\t//获得驱动列表的起始地址  const int n_ents = ll_entry_count(struct driver, driver);\t//获得驱动列表的总数量  const struct udevice_id *id; struct driver *entry; struct udevice *dev; bool found = false; const char *name, *compat_list, *compat; int compat_length, i; int result = 0; int ret = 0; if (devp) *devp = NULL; name = ofnode_get_name(node); log_debug(\u0026#34;bind node %s\\n\u0026#34;, name); compat_list = ofnode_get_property(node, \u0026#34;compatible\u0026#34;, \u0026amp;compat_length);\t//得到compatible属性，用于匹配driver驱动  if (!compat_list) { if (compat_length == -FDT_ERR_NOTFOUND) { log_debug(\u0026#34;Device \u0026#39;%s\u0026#39; has no compatible string\\n\u0026#34;, name); return 0; } dm_warn(\u0026#34;Device tree error at node \u0026#39;%s\u0026#39;\\n\u0026#34;, name); return compat_length; } /* * Walk through the compatible string list, attempting to match each * compatible string in order such that we match in order of priority * from the first string to the last. */ for (i = 0; i \u0026lt; compat_length; i += strlen(compat) + 1) { compat = compat_list + i; log_debug(\u0026#34; - attempt to match compatible string \u0026#39;%s\u0026#39;\\n\u0026#34;, compat); for (entry = driver; entry != driver + n_ents; entry++) {\t//循环判断所有驱动是否匹配\t ret = driver_check_compatible(entry-\u0026gt;of_match, \u0026amp;id, compat); if (!ret) break; } if (entry == driver + n_ents) continue; if (pre_reloc_only) { if (!ofnode_pre_reloc(node) \u0026amp;\u0026amp; !(entry-\u0026gt;flags \u0026amp; DM_FLAG_PRE_RELOC)) { log_debug(\u0026#34;Skipping device pre-relocation\\n\u0026#34;); return 0; } } log_debug(\u0026#34; - found match at \u0026#39;%s\u0026#39;: \u0026#39;%s\u0026#39; matches \u0026#39;%s\u0026#39;\\n\u0026#34;, entry-\u0026gt;name, entry-\u0026gt;of_match-\u0026gt;compatible, id-\u0026gt;compatible); ret = device_bind_with_driver_data(parent, entry, name, id-\u0026gt;data, node, \u0026amp;dev);\t//该函数，用于创建udevice对象，并与查找到的driver绑定  if (ret == -ENODEV) { log_debug(\u0026#34;Driver \u0026#39;%s\u0026#39; refuses to bind\\n\u0026#34;, entry-\u0026gt;name); continue; } if (ret) { dm_warn(\u0026#34;Error binding driver \u0026#39;%s\u0026#39;: %d\\n\u0026#34;, entry-\u0026gt;name, ret); return ret; } else { found = true; if (devp) *devp = dev; } break; } if (!found \u0026amp;\u0026amp; !result \u0026amp;\u0026amp; ret != -ENODEV) log_debug(\u0026#34;No match for node \u0026#39;%s\u0026#39;\\n\u0026#34;, name); return result; } lists_bind_fdt这个函数，主要用来扫描设备树中的各个节点；\n根据扫描到的udevice设备信息，通过compatible来匹配compatible相同的driver，匹配成功后，就会创建对应的struct udevice结构体，它会同时指向设备资源和driver，这样设备资源和driver就绑定在一起了。\n3.7、DM模型——probe探测函数的执行 #   上述，完成了DM模型的初始化，但是我们只是建立了driver和udevice的绑定关系，那么何时调用到我们驱动中的probe探测函数呢？uclass与driver又何时匹配的呢？\n 上文呢，dm_init只是负责初始化并绑定了udevice和driver，那么probe探测函数的执行，当然是在该驱动初始化的时候喽！\n 下文以mmc驱动为例！其初始化流程如下：\n 详细代码在这里就不展开来叙述了！\n在MMC驱动初始化后，有没有注意到mmc_probe这个函数，该函数就是间接调用了我们驱动编写的probe函数。\n执行流程在上面已经很清楚了：根据uclass_id，调用``uclass_get_device_by_seq来得到udevice，进而调用device_probe来找到对应驱动的probe`。\nint device_probe(struct udevice *dev) { const struct driver *drv; int ret; int seq; if (!dev) return -EINVAL; if (dev-\u0026gt;flags \u0026amp; DM_FLAG_ACTIVATED) return 0; drv = dev-\u0026gt;driver;\t//获取driver  assert(drv); ret = device_ofdata_to_platdata(dev); if (ret) goto fail; /* Ensure all parents are probed */ if (dev-\u0026gt;parent) {\t//父设备probe  ret = device_probe(dev-\u0026gt;parent); if (ret) goto fail; /* * The device might have already been probed during * the call to device_probe() on its parent device * (e.g. PCI bridge devices). Test the flags again * so that we don\u0026#39;t mess up the device. */ if (dev-\u0026gt;flags \u0026amp; DM_FLAG_ACTIVATED) return 0; } seq = uclass_resolve_seq(dev); if (seq \u0026lt; 0) { ret = seq; goto fail; } dev-\u0026gt;seq = seq; dev-\u0026gt;flags |= DM_FLAG_ACTIVATED; /* * Process pinctrl for everything except the root device, and * continue regardless of the result of pinctrl. Don\u0026#39;t process pinctrl * settings for pinctrl devices since the device may not yet be * probed. */ if (dev-\u0026gt;parent \u0026amp;\u0026amp; device_get_uclass_id(dev) != UCLASS_PINCTRL) pinctrl_select_state(dev, \u0026#34;default\u0026#34;); if (CONFIG_IS_ENABLED(POWER_DOMAIN) \u0026amp;\u0026amp; dev-\u0026gt;parent \u0026amp;\u0026amp; (device_get_uclass_id(dev) != UCLASS_POWER_DOMAIN) \u0026amp;\u0026amp; !(drv-\u0026gt;flags \u0026amp; DM_FLAG_DEFAULT_PD_CTRL_OFF)) { ret = dev_power_domain_on(dev); if (ret) goto fail; } ret = uclass_pre_probe_device(dev); if (ret) goto fail; if (dev-\u0026gt;parent \u0026amp;\u0026amp; dev-\u0026gt;parent-\u0026gt;driver-\u0026gt;child_pre_probe) { ret = dev-\u0026gt;parent-\u0026gt;driver-\u0026gt;child_pre_probe(dev); if (ret) goto fail; } /* Only handle devices that have a valid ofnode */ if (dev_of_valid(dev)) { /* * Process \u0026#39;assigned-{clocks/clock-parents/clock-rates}\u0026#39; * properties */ ret = clk_set_defaults(dev, 0); if (ret) goto fail; } if (drv-\u0026gt;probe) {\tret = drv-\u0026gt;probe(dev);\t//调用驱动的probe  if (ret) goto fail; } ret = uclass_post_probe_device(dev); if (ret) goto fail_uclass; if (dev-\u0026gt;parent \u0026amp;\u0026amp; device_get_uclass_id(dev) == UCLASS_PINCTRL) pinctrl_select_state(dev, \u0026#34;default\u0026#34;); return 0; fail_uclass: if (device_remove(dev, DM_REMOVE_NORMAL)) { dm_warn(\u0026#34;%s: Device \u0026#39;%s\u0026#39; failed to remove on error path\\n\u0026#34;, __func__, dev-\u0026gt;name); } fail: dev-\u0026gt;flags \u0026amp;= ~DM_FLAG_ACTIVATED; dev-\u0026gt;seq = -1; device_free(dev); return ret; } 主要工作归纳如下：\n 根据udevice获取driver 然后判断是否父设备被probe 对父设备进行probe 调用driver的probe函数  3.8、DM模型——uclass与uclass_driver绑定 #   上述完成了driver的probe函数调用，基本底层都已经准备好了，uclass何时与uclass_driver绑定，给上层提供统一的API呢？\n uclass与uclass_driver绑定，也是在驱动probe之后，确保该驱动存在，设备存在，最后为该驱动绑定uclass与uclass_driver，为上层提供统一接口。\n 以根据MMC驱动为例\n 回到上文的驱动流程图，看到mmc_do_preinit这个函数了嘛？里面调用了ret = uclass_get(UCLASS_MMC, \u0026amp;uc);，该函数才是真正的将uclass与uclass_driver绑定。\nint uclass_get(enum uclass_id id, struct uclass **ucp) { struct uclass *uc; *ucp = NULL; uc = uclass_find(id); if (!uc) return uclass_add(id, ucp); *ucp = uc; return 0; } uclass_get主要实现了：根据uclass_id查找对应的uclass是否被添加到global_data-\u0026gt;uclass_root链表中，如果没有添加到，就调用uclass_add函数，实现uclass与uclass_driver的绑定，并将其添加到global_data-\u0026gt;uclass_root链表中。\nstatic int uclass_add(enum uclass_id id, struct uclass **ucp) { struct uclass_driver *uc_drv; struct uclass *uc; int ret; *ucp = NULL; uc_drv = lists_uclass_lookup(id);\t//根据uclass_id查找到对应的driver  if (!uc_drv) { debug(\u0026#34;Cannot find uclass for id %d: please add the UCLASS_DRIVER() declaration for this UCLASS_... id\\n\u0026#34;, id); /* * Use a strange error to make this case easier to find. When * a uclass is not available it can prevent driver model from * starting up and this failure is otherwise hard to debug. */ return -EPFNOSUPPORT; } uc = calloc(1, sizeof(*uc)); if (!uc) return -ENOMEM; if (uc_drv-\u0026gt;priv_auto_alloc_size) { uc-\u0026gt;priv = calloc(1, uc_drv-\u0026gt;priv_auto_alloc_size); if (!uc-\u0026gt;priv) { ret = -ENOMEM; goto fail_mem; } } uc-\u0026gt;uc_drv = uc_drv;\t//uclass与uclass_driver绑定  INIT_LIST_HEAD(\u0026amp;uc-\u0026gt;sibling_node); INIT_LIST_HEAD(\u0026amp;uc-\u0026gt;dev_head); list_add(\u0026amp;uc-\u0026gt;sibling_node, \u0026amp;DM_UCLASS_ROOT_NON_CONST);\t//添加到global_data-\u0026gt;uclass_root链表中  if (uc_drv-\u0026gt;init) { ret = uc_drv-\u0026gt;init(uc); if (ret) goto fail; } *ucp = uc; return 0; fail: if (uc_drv-\u0026gt;priv_auto_alloc_size) { free(uc-\u0026gt;priv); uc-\u0026gt;priv = NULL; } list_del(\u0026amp;uc-\u0026gt;sibling_node); fail_mem: free(uc); return ret; } 好啦，到这里基本就把Uboot的DM模型全部理清楚啦，耗时一个周，总感觉想要自己去讲明白，真的不是一件容易的事情呢！\n如果对你们有帮助，记得点个赞哦！\n3.9 参考文档 #  [1] : https://www.dazhuanlan.com/archevalier/topics/1323360\n[2] : https://www.cnblogs.com/gs1008612/p/8253213.html\n[3] : https://blog.csdn.net/kunkliu/article/details/103168591\n[4] : https://blog.csdn.net/ooonebook/article/details/53234020\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":15,"href":"/docs/linux/linux_memory_manage/%E4%B8%89%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E7%AE%A1%E7%90%86/","title":"三、虚拟地址空间管理","section":"Linux 内存管理","content":"Linux内存管理 | 三、虚拟地址空间管理 #  上一节，我们主要了解了虚拟内存空间的布局情况，趁热打铁，我们直接从源代码的视角，来看一下Linux内核是如何管理虚拟内存空间的。\n废话不多说，直接开始！\n1、用户态空间管理 #  读完上一节我们知道，用户态的布局情况如下：\n我们运行的可执行程序，被加载进内存后，会作为一个进程存在，这个进程Linux内核会将其抽象成一个结构体。没错，它就是task_struct。\n1.1 task_struct结构体 #  task_struct结构体是进程的抽象，进程所涉及到的内容非常多，下面只列举出一些重要的数据结构，方面理解。\n// include/linux/sched.h struct task_struct { ... pid_t\tpid;\t//\t进程PID  pid_t\ttgid;\t//\t线程PID  struct files_struct\t*files;\t// 进程打开的文件信息  struct mm_struct\t*mm;\t//\t进程虚拟内存空间的内存描述符  ... } 如上，进程抽象为task_struct结构体，通过mm_struct结构体来管理虚拟内存空间。\n1.2 mm_struct结构体 #  每个进程都有唯一的 mm_struct 结构体，也就是前边提到的每个进程的虚拟地址空间都是独立，互不干扰的。\nmm_struct的结构体如下：\n//\tinclude/linux/mm_types.h struct mm_struct { ... struct { ... unsigned long task_size;\t/* size of task vm space */ ... unsigned long mmap_base;\t/* base of mmap area */ unsigned long total_vm;\t/* Total pages mapped */ unsigned long locked_vm;\t/* Pages that have PG_mlocked set */ unsigned long pinned_vm;\t/* Refcount permanently increased */ unsigned long data_vm;\t/* VM_WRITE \u0026amp; ~VM_SHARED \u0026amp; ~VM_STACK */ unsigned long exec_vm;\t/* VM_EXEC \u0026amp; ~VM_WRITE \u0026amp; ~VM_STACK */ unsigned long stack_vm;\t/* VM_STACK */ unsigned long start_code, end_code, start_data, end_data; unsigned long start_brk, brk, start_stack; unsigned long arg_start, arg_end, env_start, env_end; ... struct vm_area_struct *mmap;\t/* list of VMAs */ struct rb_root mm_rb; ... }__randomize_layout; ... }  1.3 内核态和用户态的划分 #  mm_struct里面定义的task_size变量，就是用来划分虚拟内存的用户空间和内核空间的。\nunsigned long task_size; task_size也就是两者的分界线，下面我们看下task_size是如何被赋值的。\n 当我们执行一个新的进程的时候，Linux内核会执行load_elf_binary的API接口，进而调用setup_new_exec函数来实现新进程的创建。\n在setup_new_exec函数中，会执行\ncurrent-\u0026gt;mm-\u0026gt;task_size = TASK_SIZE; 这个TASK_SIZE就是我们设置的内核空间地址和用户空间地址的分界线，由我们自定义配置。\n#ifdef CONFIG_X86_32 /* * User space process size: 3GB (default). */ #define TASK_SIZE\tPAGE_OFFSET #define TASK_SIZE_MAX\tTASK_SIZE /* config PAGE_OFFSET hex default 0xC0000000 depends on X86_32 */ #else /* * User space process size. 47bits minus one guard page. */ #define TASK_SIZE_MAX\t((1UL \u0026lt;\u0026lt; 47) - PAGE_SIZE) #define TASK_SIZE\t(test_thread_flag(TIF_ADDR32) ? \\ IA32_PAGE_OFFSET : TASK_SIZE_MAX) ...... 这里我们只需要知道TASK_SIZE默认值3为PAGE_OFFSET，并且默认为0xC0000000为分界线的，即用户空间3GB，内核空间1GB；当然这个可以由我们动态配置，可以配置PAGE_OFFSET为0x80000000，即用户空间和内核空间均为2GB，取决于我们的应用场合，当你看到与我们讲解不同时，也不用大惊小怪。\n 以上，表达的概念很简单，如下图：\n  1.4 位置信息描述 #  我们知道用户态内存空间分为几个区域：代码段、数据段、BSS段、堆、文件映射和匿名映射区、栈等几个部分，同样在mm_struct中，定义了这些区域的统计信息和位置。\nunsigned long mmap_base;\t/* base of mmap area */ unsigned long total_vm;\t/* Total pages mapped */ unsigned long locked_vm;\t/* Pages that have PG_mlocked set */ unsigned long pinned_vm;\t/* Refcount permanently increased */ unsigned long data_vm;\t/* VM_WRITE \u0026amp; ~VM_SHARED \u0026amp; ~VM_STACK */ unsigned long exec_vm;\t/* VM_EXEC \u0026amp; ~VM_WRITE \u0026amp; ~VM_STACK */ unsigned long stack_vm;\t/* VM_STACK */ unsigned long start_code, end_code, start_data, end_data; unsigned long start_brk, brk, start_stack; unsigned long arg_start, arg_end, env_start, env_end;  total_vm：总映射页面的数目。（这么大的虚拟内存空间，不可能全部映射到真实的物理内存，都是按需映射的，这里表示当前映射的页面总数目）   由于物理内存比较小，当内存吃紧的时候，就会发生换入换出的操作，即将暂时不用的页面换出到硬盘上，有的页面比较重要，不能换出。\n  locked_vm：被锁定不能换出的页面 pinned_vm ：不能换出、也不能移动的页面 data_vm：存放数据页的页的数目 exec_vm：存放可执行文件的页的数目 stack_vm：存放堆栈信息页的数目 start_code、end_code：表示可执行代码开始和结束的位置 start_data、end_data：表示已初始化数据的开始位置和结束位置 start_brk、brk：堆的起始地址，结束地址 start_stack：是栈的起始位置，在 RBP 寄存器中存储，栈的结束位置也就是栈顶指针，在 RSP 寄存器中存储。在栈中内存地址的增长方向也是由高地址向低地址增长。 arg_start、arg_end：参数列表的起始位置和结束位置 env_start、env_end：环境变量的起始位置和结束位置   整体的布局情况如下：\n 1.5 区域属性描述 #  尽管已经有了一些变量来描述每一个段的信息，但是Linux内核在mm_struct结构体里面，还有一个专门的数据结构vm_area_struct来管理每个区域的属性。\nstruct vm_area_struct *mmap;\t/* list of VMAs */ struct rb_root mm_rb; mmap：为一个单链表，将所有的区域串联起来\nmm_rb：为一个红黑树，方便查找和修改内存区域。\n 下面看一下vm_area_struct数据结构：\nstruct vm_area_struct { /* The first cache line has the info for VMA tree walking. */ unsigned long vm_start;\t/* Our start address within vm_mm. */ unsigned long vm_end;\t/* The first byte after our end address within vm_mm. */ /* linked list of VM areas per task, sorted by address */ struct vm_area_struct *vm_next, *vm_prev; struct rb_node vm_rb; struct mm_struct *vm_mm;\t/* The address space we belong to. */ struct list_head anon_vma_chain; /* Serialized by mmap_sem \u0026amp; * page_table_lock */ struct anon_vma *anon_vma;\t/* Serialized by page_table_lock */ /* Function pointers to deal with this struct. */ const struct vm_operations_struct *vm_ops; struct file * vm_file;\t/* File we map to (can be NULL). */ void * vm_private_data;\t/* was vm_pte (shared mem) */ } __randomize_layout;   vm_start、vm_end：为该区域在用户空间的起始和结束地址\n  vm_next、vm_prev：将该区域添加到链表上，便于管理。\n  vm_rb：将这个区域放到红黑树上\n  vm_ops：对该区域可以进行的内存操作\n  anon_vma：匿名映射\n  vm_file：文件映射\n   用户态空间的每个区域都由该结构体来管理，最终形成下面的这个结构：\n  顺便介绍一下 我的圈子：高级工程师聚集地，期待大家的加入。\n 2、内核态空间管理 #  上面，我们从源码角度了解了用户态空间管理，下面我们看内核态空间管理。\n回顾一下，我们内核态的布局情况是怎么样的呢，还记得吗？\n我们要知道：\n 内核态的虚拟空间和任何一个进程都没有关系，所有的进程看到的内核态虚拟空间都是一样的。 在内核态，我们直接操作的依旧是虚拟地址，而非物理地址 不同CPU结构下，内核态空间的布局格式是不变的，但是大小会有所调整，比如ARM和X86的大小空间有所不同。   内核态空间管理并不像用户态那样使用结构体来统一管理，而是直接使用宏来定义每个区域的分界线，\n 下面我们以x86架构来分析内核态空间的管理\n 2.1 分界线定义 #  /* * User space process size: 3GB (default). */ #define TASK_SIZE\tPAGE_OFFSET  /* PAGE_OFFSET - the virtual address of the start of the kernel image */ #define PAGE_OFFSET\t((unsigned long)__PAGE_OFFSET)  #define __PAGE_OFFSET\t__PAGE_OFFSET_BASE  #define __PAGE_OFFSET_BASE\t_AC(CONFIG_PAGE_OFFSET, UL)  config PAGE_OFFSET hex default 0xB0000000 if VMSPLIT_3G_OPT default 0x80000000 if VMSPLIT_2G default 0x78000000 if VMSPLIT_2G_OPT default 0x40000000 if VMSPLIT_1G default 0xC0000000 depends on X86_32 TASK_SIZE：内核态空间与用户态空间的分界线\nPAGE_OFFSET：该宏表示内核镜像起始的虚拟地址。\nCONFIG_PAGE_OFFSET：这个宏定义的值，根据实际情况自行设定，默认为0XC0000000，可以设置为0X80000000等。\n以上，TASK_SIZE就被定义为0XC0000000作为用户态空间和内核态空间的分界线，将4G虚拟内存分配为3G/1G结构。\n2.2 直接映射区定义 #  直接映射区是定义在PAGE_OFFSET和high_memory之间的区域。\n PAGE_OFFSET：表示内核镜像的起始地址，上文已经说明。 high_memory也是表示的就是896M这个值，表示高端内存的分界线。   顺便说明以下，TASK_SIZE和PAGE_OFFSET在不同架构下是不同的，在ARM架构下，两者并不相等，本文以X86架构为例\n 2.3 安全保护区定义 #  系统会在high_memory和VMALLOC_START之间预留8M的安全保护区，防止访问越界。\nVMALLOC_OFFSET表示的是内核动态映射区的偏移，也就是所谓的安全保护区。\n#define VMALLOC_START\t(((unsigned long)high_memory + VMALLOC_OFFSET) \u0026amp; ~(VMALLOC_OFFSET-1))  #define VMALLOC_OFFSET\t(8*1024*1024) 可以很清楚的看到VMALLOC_OFFSET定义了8M的空间，VMALLOC_START在high_memory基础上，偏移了VMALLOC_OFFSET 8M空间大小作为安全保护区，以防越界访问。\n2.3 动态映射区定义 #  VMALLOC_START和VMALLOC_END之间称为内核动态映射区。\n和用户态进程使用 malloc 申请内存一样，在这块动态映射区内核是使用 vmalloc 进行内存分配。\n#define VMALLOC_START\t(((unsigned long)high_memory + VMALLOC_OFFSET) \u0026amp; ~(VMALLOC_OFFSET-1))  #ifdef CONFIG_HIGHMEM # define VMALLOC_END\t(PKMAP_BASE - 2 * PAGE_SIZE) #else # define VMALLOC_END\t(LDT_BASE_ADDR - 2 * PAGE_SIZE) #endif  PKMAP_BASE：是永久映射区的起始地址。\nVMALLOC_END：在永久映射区的起始地址下，偏移2个PAGE_SIZE作为安全保护区。\n2.4 永久映射区定义 #  PKMAP_BASE 到 FIXADDR_START 的空间称为永久内核映射，在内核的这段虚拟地址空间中允许建立与物理高端内存的长期映射关系。\n 比如内核通过 alloc_pages() 函数在物理内存的高端内存中申请获取到的物理内存页，这些物理内存页可以通过调用 kmap 映射到永久映射区中。\n #define PKMAP_BASE\t\\ ((LDT_BASE_ADDR - PAGE_SIZE) \u0026amp; PMD_MASK)  #define LDT_BASE_ADDR\t\\ ((CPU_ENTRY_AREA_BASE - PAGE_SIZE) \u0026amp; PMD_MASK)  #define CPU_ENTRY_AREA_BASE\t\\ ((FIXADDR_TOT_START - PAGE_SIZE * (CPU_ENTRY_AREA_PAGES + 1)) \\ \u0026amp; PMD_MASK)  #define FIXADDR_TOT_START\t(FIXADDR_TOP - FIXADDR_TOT_SIZE)  #define FIXADDR_TOP\t((unsigned long)__FIXADDR_TOP)  #define FIXADDR_TOT_SIZE\t(__end_of_fixed_addresses \u0026lt;\u0026lt; PAGE_SHIFT)  unsigned long __FIXADDR_TOP = 0xfffff000; #define PMD_MASK\t(~(PMD_SIZE - 1))  #define PAGE_SHIFT\t12 #define PAGE_SIZE\t(_AC(1,UL) \u0026lt;\u0026lt; PAGE_SHIFT)  #define CPU_ENTRY_AREA_PAGES\t(NR_CPUS * 40)  #define FIXADDR_START\t(FIXADDR_TOP - FIXADDR_SIZE)   PKMAP_BASE：是永久映射区的起始地址，它经过一系列的计算得到，具体可以看上面的宏定义，我们大概了解就行了，不同体系结构的定义位置还不一样。 FIXADDR_START：是固定映射区的起始地址，也是永久映射区的结束地址。  2.5 固定映射区定义 #  FIXADDR_START到FIXADDR_TOP的空间称为固定映射区，主要用于满足特殊的需求。\n#define FIXADDR_TOP\t((unsigned long)__FIXADDR_TOP)  unsigned long __FIXADDR_TOP = 0xfffff000; 固定映射区中的虚拟地址，可以自由映射到物理内存的高端地址空间上，特点是其映射的虚拟地址是不变的，物理地址是可以改变的。\n2.6 临时映射区定义 #  最后FIXADDR_TOP到0xFFFFFFFF之间的区域称为临时映射区。\n 它主要用来做什么呢，网上举的一个例子，大家参考以下。\n 假设用户态的进程要映射一个文件到内存中，先要映射用户态进程空间的一段虚拟地址到物理内存，然后将文件内容写入这个物理内存供用户态进程访问。\n给用户态进程分配物理内存页可以通过 alloc_pages()，分配完毕后，按说将用户态进程虚拟地址和物理内存的映射关系放在用户态进程的页表中，就完事大吉了。这个时候，用户态进程可以通过用户态的虚拟地址，也即 0 至 3G 的部分，经过页表映射后访问物理内存，并不需要内核态的虚拟地址里面也划出一块来，映射到这个物理内存页。\n但是如果要把文件内容写入物理内存，这件事情要内核来干了，这就只好通过 kmap_atomic 做一个临时映射，写入物理内存完毕后，再 kunmap_atomic 来解映射即可。\n以上，就是内核态空间的布局以及管理。\n3、总结 #  该篇文章，主要从源码角度来了解用户态空间和内核态空间是如何管理的，挪用大佬的一个图片，结合上面所讲的，相信很快就能茅塞顿开。\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":16,"href":"/docs/uboot/%E5%9B%9Buboot%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%A8%A1%E5%BC%8F%E5%88%86%E6%9E%90/","title":"四、Uboot命令行模式分析","section":"Uboot开发","content":"四、Uboot命令行模式分析 #   前几篇文章，我们也了解了Uboot的启动流程，那么这节就主要讲讲Uboot的命令行模式。\n另外，文章末尾还提供eMMC5.1官方标准协议.pdf和eMMC4.51官方标准协议-中文.pdf下载渠道，方便深入了解底层协议。\n正文如下：\n  4.1 如何进入命令行模式 #  我们正常启动流程，默认是直接跳过Uboot命令行模式的，因为Uboot主要的作用是引导Kernel，一般我们不进行uboot开发时，都默认跳过进入命令行模式。\n 那么，我们要想进入Uboot命令行模式，需要进行哪些配置呢？\n 打开我们准备好一份Uboot源码，进入menuconfig配置菜单，主要设置下列几个配置信息！\n  CONFIG_CMDLINE：命令行模式开关 CONFIG_SYS_PROMPT：命令行模式提示符 CONFIG_HUSH_PARSER：使用hush shell 来对命令进行解析 BOOTDELAY：设置启动延时   Tip：meneconfig中查找苦难？实时/符号，输入1或2或3，直接查找指定标识。\n   打开之后，重新编译，并将Uboot镜像烧录到开发板中，再次启动，我们就能够看到倒计时。\n[2022-03-02:13:33:47]U-Boot 2020.10-rc1-00043-ge62a6d17c6-dirty (Feb 08 2022 - 10:14:14 +0800) [2022-03-02:13:33:47] [2022-03-02:13:33:47]Model: xxxxxx [2022-03-02:13:33:47]MMC: mmc1@xxxxxx: 1 [2022-03-02:13:33:47]In: serial [2022-03-02:13:33:47]Out: serial [2022-03-02:13:33:47]Err: serial [2022-03-02:13:33:47]Model: xxxxxx [2022-03-02:13:33:49]Hit any key to stop autoboot: 2 Hit any key to stop autoboot：我们在倒计时结束前，任意键入一个按键，即可进入！\n 4.2 Uboot基本命令解析 #  进入Uboot命令行模式后，键入help或者?，可以查看所有支持的Uboot命令。\n注意：Uboot支持的命令大都远远超过显示的，还有好多没有打开，可以在menuconfig中，打开相应的功能，如mmc相关的，md内存相关的。\n 常用命令如下：\nversion\t#查看uboot版本 reset #重启Uboot printenv\t#打印uboot环境变量 setenv name value\t#设置环境变量 md addr\t#查看内存指令 nm addr\t#修改内存值 mm addr\t#自增修改内存值 mmc dev id\t#选择mmc卡 mmc rescan\t#扫描卡 echo $name\t#打印环境变量  更多指令使用，可以见文末整理的文档\n  4.3 命令行模式代码执行流程分析 #   结合下面的程序执行流程图，代码，一起分析。\n 上图为Uboot命令行模式的代码具体执行流程，结合 专栏系列（二）uboot启动流程分析，文章内已经详细分析函数内部实现。\n static int abortboot(int bootdelay) { int abort = 0; if (bootdelay \u0026gt;= 0) { if (IS_ENABLED(CONFIG_AUTOBOOT_KEYED)) abort = abortboot_key_sequence(bootdelay); else abort = abortboot_single_key(bootdelay);\t//按键检测 \t} if (IS_ENABLED(CONFIG_SILENT_CONSOLE) \u0026amp;\u0026amp; abort) gd-\u0026gt;flags \u0026amp;= ~GD_FLG_SILENT; return abort; } static int abortboot_single_key(int bootdelay) { int abort = 0; unsigned long ts; printf(\u0026#34;Hit any key to stop autoboot: %2d \u0026#34;, bootdelay);\t//打印倒计时  /* * Check if key already pressed */ if (tstc()) {\t/* we got a key press\t*/\t//获取按键 \t(void) getc(); /* consume input\t*/ puts(\u0026#34;\\b\\b\\b0\u0026#34;); abort = 1;\t/* don\u0026#39;t auto boot\t*/ } while ((bootdelay \u0026gt; 0) \u0026amp;\u0026amp; (!abort)) { --bootdelay; /* delay 1000 ms */ ts = get_timer(0); do { if (tstc()) {\t/* we got a key press\t*/\t//获取按键 \tint key; abort = 1;\t/* don\u0026#39;t auto boot\t*/ bootdelay = 0;\t/* no more delay\t*/ key = getc(); /* consume input\t*/ if (IS_ENABLED(CONFIG_USE_AUTOBOOT_MENUKEY)) menukey = key; break; } udelay(10000); } while (!abort \u0026amp;\u0026amp; get_timer(ts) \u0026lt; 1000);\t//延时1S  printf(\u0026#34;\\b\\b\\b%2d \u0026#34;, bootdelay); } putc(\u0026#39;\\n\u0026#39;); return abort; } abortboot_single_key：该函数主要用于while循环检测按键，如果有按键按下，将abort标志位置1，最后运行cli_loop命令行模式的函数。\n如果按键不按下，标志位abort不起作用，直接运行run_command_list(s, -1, 0);，s = env_get(\u0026quot;bootcmd\u0026quot;);，直接跳转到我们设置的环境变量bootcmd所设定的指令，而不执行cli_loop函数。\n 对照运行流程图看代码，容易理解！！！\n  4.4 如何添加Uboot命令 #  如何自定义一个Uboot命令呢？\n我们暂且先不考虑实现的原理，就仅仅照葫芦画瓢来实现一个简单的Uboot命令！\n 第一步：照葫芦 #  我们打开Uboot的源码文件，进入cmd目录，没错，所有的命令实现都存放在该目录下。\n有没有看到help.C这个文件呢，我们就拿help这个文件来类比。\nU_BOOT_CMD：用来定义一个命令\nhelp：用于命令行键入的指令\ndo_help：键入指令后，执行的函数\n要想进一步使用该命令，我们不得不去了解每个参数的含义。\nstruct cmd_tbl_s { char\t*name;\t/* Command Name\t*/ int\tmaxargs;\t/* maximum number of arguments\t*/ int\trepeatable;\t/* autorepeat allowed?\t*/ /* Implementation function\t*/ int\t(*cmd)(struct cmd_tbl_s *, int, int, char *[]); char\t*usage;\t/* Usage message\t(short)\t*/ char\t*help;\t/* Help message\t(long)\t*/ /* do auto completion on the arguments */ int\t(*complete)(int argc, char *argv[], char last_char, int maxv, char *cmdv[]); }; typedef struct cmd_tbl_s\tcmd_tbl_t; 每个参数分别对应了：命令名、可接收的最大参数、命令可重复、响应函数、使用示例、帮助信息。\n 第二步：画瓢 #  弄明白这个道理，假如我们想加入一个helpme的指令，该怎么做？\n 定义一个指令  U_BOOT_CMD( helpme,\tCONFIG_SYS_MAXARGS,\t1,\tdo_helpme, \u0026#34;helpme dong\u0026#34;, \u0026#34;\\n\u0026#34; \u0026#34;\t- print brief description of all commands\\n\u0026#34; \u0026#34;helpme command ...\\n\u0026#34; \u0026#34;\t- print detailed usage of \u0026#39;command\u0026#39;\u0026#34; );  定义一个执行函数  static int do_helpme(struct cmd_tbl *cmdtp, int flag, int argc, char *const argv[]) { printf(\u0026#34;Cmd test ok!\\r\\n\u0026#34;); printf(\u0026#34;argc = %d\\r\\n\u0026#34;, argc); printf(\u0026#34;argv = \u0026#34;); for(int i = 0; i \u0026lt; argc; ++i) { printf(\u0026#34;%s\\t\u0026#34;, argv[i]); } printf(\u0026#34;\\r\\n\u0026#34;); } 这样，就可以编译-\u0026gt;烧录-\u0026gt;运行了。\n进入Uboot命令行，键入help查看添加的命令helpme。\n 键入命令测试  =\u0026gt; helpme 123456 123 Cmd test ok! argc = 3 argv = helpme 123456 123  第三步：优雅 #  如果我们只是暂时测试，这样添加无伤大雅；如果我们需要投入正规项目使用，这么做有点激进了。\n更加合理的做法是：\n 在uboot/cmd目录下，建立一个文件XXX.c 将要添加的命令写入XXX.c该文件中 修改Makefile文件，编译该文件：obj-y += XXX.o 重新编译，烧录   说白了，就是创建一个文件，将自定义指令添加进去，尽量不修改源码！\n  4.5 Uboot命令底层实现分析 #  上面写了傻瓜式添加命令的方法，对于进行Uboot开发，当然我们需要去了解一下内部的实现原理。\n4.5.1 U_BOOT_CMD #  查看U_BOOT_CMD宏定义\n#define U_BOOT_CMD(_name, _maxargs, _rep, _cmd, _usage, _help)\t\\ U_BOOT_CMD_COMPLETE(_name, _maxargs, _rep, _cmd, _usage, _help, NULL) #define U_BOOT_CMD_COMPLETE(_name, _maxargs, _rep, _cmd, _usage, _help, _comp) \\ ll_entry_declare(struct cmd_tbl, _name, cmd) =\t\\ U_BOOT_CMD_MKENT_COMPLETE(_name, _maxargs, _rep, _cmd,\t\\ _usage, _help, _comp); #define U_BOOT_CMD_MKENT_COMPLETE(_name, _maxargs, _rep, _cmd,\t\\ _usage, _help, _comp)\t\\ { #_name, _maxargs,\t\\ _rep ? cmd_always_repeatable : cmd_never_repeatable,\t\\ _cmd, _usage, _CMD_HELP(_help) _CMD_COMPLETE(_comp) } #define ll_entry_declare(_type, _name, _list)\t\\ _type _u_boot_list_2_##_list##_2_##_name __aligned(4)\t\\ __attribute__((unused,\t\\ section(\u0026#34;.u_boot_list_2_\u0026#34;#_list\u0026#34;_2_\u0026#34;#_name)))   乍一看，都是宏定义，为什么看起来这么吃力？\n 在这里，不得不提到#和##的区别 #   #：转换为字符串  ... #define TO_STR(x) #x int main() { int value = 123; printf(\u0026#34;TO_STR(value) = %s\\n\u0026#34;, TO_STR(value)); printf(\u0026#34;TO_STR(123) = %s\\n\u0026#34;, TO_STR(123)); } //打印 TO_STR(value) = value; TO_STR(123) = 123;  ##：两个字符拼接  #define CONNECT(x,y) x##y #define VAR(y) data##y int main() { int xy = 123; printf(\u0026#34;xy = %d\\n\u0026#34;, CONNECT(x, y)); CONNECT(x, y) = 123456; printf(\u0026#34;xy = %d\\n\u0026#34;, CONNECT(x, y)); int VAR(1) = 100; printf(\u0026#34;VAR(1) = data1 = %d\\n\u0026#34;, data1); } //打印 xy = 123 xy = 123456 VAR(1) = data1 = 100  回到正文\n 上面的宏定义，简单来看，转换流程就是：\nU_BOOT_CMD -\u0026gt; U_BOOT_CMD_COMPLETE -\u0026gt; ll_entry_declare = U_BOOT_CMD_MKENT_COMPLETE -\u0026gt; _type xxx = {aaa, bbb, ccc , ...}\n其本质就是： struct my_struct test = {1, 2, 3};结构体赋值语句。\n 以help命令为例：\nU_BOOT_CMD( help,\tCONFIG_SYS_MAXARGS,\t1,\tdo_help, \u0026#34;print command description/usage\u0026#34;, \u0026#34;\\n\u0026#34; \u0026#34;\t- print brief description of all commands\\n\u0026#34; \u0026#34;help command ...\\n\u0026#34; \u0026#34;\t- print detailed usage of \u0026#39;command\u0026#39;\u0026#34; ); 直接展开来看：\nstruct cmd_tbl _u_boot_list_2_cmd_2_help __aligned(4) __attribute__((unused, section(\u0026#34;.u_boot_list_2_cmd_2_help\u0026#34;))) = {\u0026#34;help\u0026#34;, CONFIG_SYS_MAXARGS, cmd_always_repeatable, do_help, \u0026#34;xxx\u0026#34;, \u0026#34;xxx\u0026#34;}; 也就相当于，我们定义一个命令，给其赋值。\n定义的命令存放在哪里呢？ #  根据上面展开来看，section(\u0026quot;.u_boot_list_2_cmd_2_help\u0026quot;)，存放在段.u_boot_list_2_cmd_2_help中，打开u-boot.map文件，我们可以查找得到。\n有没有觉得很熟悉，没错，跟前面讲过的驱动模型很像。\n我们定义的命令，被u_boot_list_2_cmd_1和u_boot_list_2_cmd_3两个段所包括，用于遍历，最终查找得到我们想要的命令。\n4.6 Uboot命令响应流程 #  命令响应流程见图：\n 根据4.3 命令行模式代码执行流程分析，我们可以知道，命令行模式最终执行cli_loop函数，实现与用户的交互。\nvoid cli_loop(void) { bootstage_mark(BOOTSTAGE_ID_ENTER_CLI_LOOP); #ifdef CONFIG_HUSH_PARSER \tparse_file_outer(); /* This point is never reached */ for (;;); #elif defined(CONFIG_CMDLINE) \tcli_simple_loop(); #else \tprintf(\u0026#34;## U-Boot command line is disabled. Please enable CONFIG_CMDLINE\\n\u0026#34;); #endif /*CONFIG_HUSH_PARSER*/} 通过分析代码，Uboot的命令行有两种模式：一种是HUSH解析，另一种是通用解析。\n HUSH解析：调用parse_file_outer并不断循环 通用解析：调用cli_simple_loop并不断循环。   无论哪种命令行解析，说白了就是输入输出的处理，必定会读取数据，执行相应命令，打印出对应数据\n HUSH模式\n 输入数据处理：parse_stream 输出数据处理：run_list  通用模式：\n 输入数据处理：cli_readline 输出数据处理：run_command_repeatable   具体实现流程，参照上面的流程图！\n命令行模式的深入解析，准备在下节详细介绍！\n 目前，我们已经对命令行的整体运行流程进行梳理，熟悉整体的运行逻辑，并且能够添加自定义命令喽。\n 4.6 推荐文档 #  [1]：https://www.pianshen.com/article/21471247431/\n[2]：https://blog.csdn.net/weixin_44895651/article/details/108211268\n[3]：https://blog.51cto.com/u_2847568/4917530?b=totalstatistic\n[4]：https://blog.csdn.net/SilverFOX111/article/details/86892231\n[5]：https://blog.csdn.net/andy_wsj/article/details/8614905\n 另外，如果有同学想了解Emmc协议的，可以【戳这里】下载eMMC5.1官方标准协议.pdf和eMMC4.51官方标准协议-中文.pdf\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":17,"href":"/docs/linux/linux_memory_manage/%E5%9B%9B%E7%89%A9%E7%90%86%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%9E%8B/","title":"四、物理地址空间设计模型","section":"Linux 内存管理","content":"Linux内存管理 | 四、物理地址空间设计模型 #  前面几篇文章，主要讲解了虚拟内存空间的布局和管理，下面同步来聊聊物理内存空间的布局和管理。\n 1、物理内存 #   什么是物理内存？\n 我们平时聊的内存，也叫随机访问存储器（random-access memory），也叫RAM。\nRAM分为两类：\n SRAM：静态RAM，其主要用于CPU高速缓存 L1Cache，L2Cache，L3Cache，其特点是访问速度快，访问速度为 1 - 30 个时钟周期，但是容量小，造价高。   DRAM：动态RAM，其主要用于我们常说的主存上，其特点的是访问速度慢（相对高速缓存），访问速度为 50 - 200 个时钟周期，但是容量大，造价便宜些（相对高速缓存）。  DRAM经过组合起来，就作为我们的计算机内存，也是物理内存。\n 2、物理内存访问模型 #  上面介绍了物理内存的基本组成，那么CPU是如何访问物理内存的呢？\n对于CPU访问物理内存，Linux提供了两种架构：UMA(Uniform Memory Access)一致内存访问，NUMA(Non-Uniform Memory Access)非一致内存访问。\n2.1 UMA #  在UMA架构下，多核处理器中的多个CPU，位于总线的一侧，所有的内存条组成的物理内存位于总线的另一侧。\n所有的CPU访问内存都要经过总线，并且距离都是一样的，所以在UMA架构下，所有CPU具有相同的访问特性，即对内存的访问具有相同的速度。\n2.2 NUMA #  这种架构，系统中的各个处理器都有本地内存，处理器与处理器之间也通过总线连接，以便于其他处理器对本地内存的访问。\n与UMA不同的是，处理器访问本地内存的速度要快于对其他处理器本地内存的访问。\n3、物理内存组织模型 #  内存页是物理内存管理中最小单位，有时也成为页帧（Page Frame）。\n内核对物理内存划分为一页一页的连续的内存块，每页大小4KB，并且使用struct page结构体来表示页结构，其中封装了每个页的状态信息，包括：组织结构，使用信息，统计信息等。\n page结构体较为复杂，我们后续再深入了解。\n   更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  3.1 FLATMEM平坦内存模型 #   FLATMEM即：flat memory model。\n 我们把物理内存想象成它是由连续的一页一页的块组成的，我们从0开始对物理页编号，这样每个物理页都会有页号。\n由于物理地址是连续的，页也是连续的，每个页大小也是一样的。因而对于任何一个地址，只要直接除一下每页的大小，很容易直接算出在哪一页。\n如果是这样，整个物理内存的布局就非常简单、易管理，这就是最经典的平坦内存模型（Flat Memory Model）。\n如上图，平坦内存模型中，内核使用一个mem_map的全局数组，来组织所有划分出来的物理内存页，下标由PFN表示。\n在平坦内存模型下 ，page_to_pfn 与 pfn_to_page 的计算逻辑就非常简单，本质就是基于 mem_map 数组进行偏移操作。\n#ifndef ARCH_PFN_OFFSET #define ARCH_PFN_OFFSET\t(0UL) #endif  #if defined(CONFIG_FLATMEM) #define __pfn_to_page(pfn) (mem_map + ((pfn)-ARCH_PFN_OFFSET)) #define __page_to_pfn(page) ((unsigned long)((page)-mem_map) + ARCH_PFN_OFFSET) #endif  ARCH_PFN_OFFSET 是 PFN 的起始偏移量。\n  3.2 DISCONTIGMEM 不连续内存模型 #   DISCONTIGMEM即：discontiguous memory model。\n 我们早期内核使用的是FLATMEM模型，该模型对于较小的，连续的物理空间是方便使用的，但是当物理内存不连续时，使用mem_map管理，就会出现空洞，这会浪费mem_map数组本身占用的内存空间。\n对于NUMA访问内存模型，物理内存分布就是不连续的，为了有效管理，DISCONTIGMEM 不连续内存模型出现了。\n在不连续的物理内存中，DISCONTIGMEM不连续内存模型，将物理内存分成了一个个的node，然后每个node管理一块连续的物理内存，连续的物理内存仍然使用FLATMEM平坦内存模型来管理，从而避免了内存空洞的浪费。\n 我们可以看出 DISCONTIGMEM 非连续内存模型其实就是 FLATMEM 平坦内存模型的一种扩展。\n DISCONTIGMEM是个稍纵即逝的内存模型，在SPARSEMEM出现后即被完全替代。\n 3.3 SPARSEMEM稀疏内存模型 #  随着内存技术的发展，内核可以支持物理内存的热插拔了（像我们的内存条，可以直接插入拔出），这样不连续物理内存已然称为常态。\nSPARSEMEM稀疏内存模型的核心思想就是对粒度更小的连续内存块进行精细的管理，用于管理连续内存块的单元被称作 section 。\n 物理页大小为 4k 的情况下， section 的大小为 128M ，物理页大小为 16k 的情况下， section 的大小为 512M。\n  在内核中，使用struct mem_section结构体表示SPARSEMEM模型中的section\nstruct mem_section { unsigned long section_mem_map; ... }   每个mem_section管理一片小的，物理内存连续的区域，并且支持对该区域的offline/online状态\n  所有的mem_section都保存在一个全局数组中\n   整体的框架如下：\n 在 SPARSEMEM 稀疏内存模型下 page_to_pfn 与 pfn_to_page 的计算逻辑又发生了变化。\n#if defined(CONFIG_SPARSEMEM) /* * Note: section\u0026#39;s mem_map is encoded to reflect its start_pfn. * section[i].section_mem_map == mem_map\u0026#39;s address - start_pfn; */ #define __page_to_pfn(pg)\t\\ ({\tconst struct page *__pg = (pg);\t\\ int __sec = page_to_section(__pg);\t\\ (unsigned long)(__pg - __section_mem_map_addr(__nr_to_section(__sec)));\t\\ })  #define __pfn_to_page(pfn)\t\\ ({\tunsigned long __pfn = (pfn);\t\\ struct mem_section *__sec = __pfn_to_section(__pfn);\t\\ __section_mem_map_addr(__sec) + __pfn;\t\\ }) #endif  在 page_to_pfn 的转换中，首先需要通过 page_to_section 根据 struct page 结构定位到 mem_section 数组中具体的 section 结构。然后在通过 section_mem_map 定位到具体的 PFN。 在 pfn_to_page 的转换中，首先需要通过 __pfn_to_section 根据 PFN 定位到 mem_section 数组中具体的 section 结构。然后在通过 PFN 在 section_mem_map 数组中定位到具体的物理页 Page 。   4、总结 #  以上，我们先对物理内存空间有一个基础的了解，明白物理内存空间的内存访问模型和组织模型，下面我们再详细介绍物理内存空间的布局和管理。\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":18,"href":"/docs/linux/linux_memory_manage/%E4%BA%94%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E7%A9%BA%E9%97%B4%E5%B8%83%E5%B1%80%E5%8F%8A%E7%AE%A1%E7%90%86/","title":"五、物理内存空间布局及管理","section":"Linux 内存管理","content":"Linux内存管理 | 五、物理内存空间布局及管理 #  上章，我们介绍了物理内存的访问内存模型和组织内存模型，我们再来回顾一下：\n物理内存的访问内存模型分为：\n UMA：一致内存访问 NUMA：非一致内存访问  物理内存的组织模型：\n FLATMEM：平坦内存模型 DISCONTIGMEM：不连续内存模型 SMARSEMEM：稀疏内存模型  Linux内核为了用统一的代码获取最大程度的兼容性，对物理内存的定义方面，引入了：内存结点（node）、内存区域（zone），内存页（page）的概念，下面我们来一一探究。\n 更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  1、内存节点node #  内存节点的引入，是Linux为了最大程度的提高兼容性，将UMA和NUMA系统统一起来，对于UMA而言是只有一个节点的系统。\n 下面的代码部分，我们尽可能的只保留暂时用的到的部分，不涉及太多的体系架相关的细节。\n 在Linux内核中，我们使用 typedef struct pglist_data pg_data_t表示一个节点\n/* * On NUMA machines, each NUMA node would have a pg_data_t to describe * it\u0026#39;s memory layout. On UMA machines there is a single pglist_data which * describes the whole memory. * * Memory statistics and page replacement data structures are maintained on a * per-zone basis. */ typedef struct pglist_data { ... int node_id; struct page *node_mem_map; unsigned long node_start_pfn; unsigned long node_present_pages; /* total number of physical pages */ unsigned long node_spanned_pages; /* total size of physical page range, including holes */ ... } pg_data_t;   node_id：每个节点都有自己的ID\n  node_mem_map：当前节点的struct page数组，用来管理这个节点的所有的页\n  node_start_pfn：这个节点的起始页号\n  node_present_pages：这个节点的真正可用的物理内存的页面数\n  node_spanned_pages：这个节点所包含的物理内存的页面数，包括不连续的内存空洞\n   例如，64M 物理内存隔着一个 4M 的空洞，然后是另外的 64M 物理内存。\n这样换算成页面数目就是，16K 个页面隔着 1K 个页面，然后是另外 16K 个页面。\n这种情况下，node_spanned_pages 就是 33K 个页面，node_present_pages 就是 32K 个页面。\n  内核使用了一个大小为 MAX_NUMNODES ，类型为 struct pglist_data 的全局数组 node_data[] 来管理所有的 NUMA 节点。\n2、内存区域zone #  2.1 各区域的布局 #  每一个节点，都被分成了一个个区域zone，我们看一下zone的定义：\nenum zone_type { #ifdef CONFIG_ZONE_DMA  ZONE_DMA, #endif #ifdef CONFIG_ZONE_DMA32  ZONE_DMA32, #endif  ZONE_NORMAL, #ifdef CONFIG_HIGHMEM  ZONE_HIGHMEM, #endif  ZONE_MOVABLE, #ifdef CONFIG_ZONE_DEVICE  ZONE_DEVICE, #endif  __MAX_NR_ZONES }; ZONE_DMA：用作DMA的内存。\n DMA 是这样一种机制：要把外设的数据读入内存或把内存的数据传送到外设，原来都要通过 CPU 控制完成，但是这会占用 CPU，影响 CPU 处理其他事情，所以有了 DMA 模式。\nCPU 只需向 DMA 控制器下达指令，让 DMA 控制器来处理数据的传送，数据传送完毕再把信息反馈给 CPU，这样就可以解放 CPU。\n ZONE_DMA32：对于 64 位系统，有两个 DMA 区域。除了上面说的 ZONE_DMA，还有 ZONE_DMA32。\nZONE_NORMAL：直接映射区，也就i是之前讲的从物理内存到虚拟内存的内核区域，通过加上一个常量直接映射。\nZONE_HIGHMEM：高端内存区\nZONE_MOVABLE：可移动区域，通过将物理内存划分为可移动分配区域和不可移动分配区域来避免内存碎片。\n2.2 各区域的管理 #  上面我们大致了解了，每个zone的布局情况，下面我们来看看内核是如何对其进行管理的。\n 接着上面介绍的pglist_data结构体\n /* * On NUMA machines, each NUMA node would have a pg_data_t to describe * it's memory layout. On UMA machines there is a single pglist_data which * describes the whole memory. * * Memory statistics and page replacement data structures are maintained on a * per-zone basis. */ typedef struct pglist_data { ... int node_id; struct page *node_mem_map; unsigned long node_start_pfn; unsigned long node_present_pages; /* total number of physical pages */ unsigned long node_spanned_pages; /* total size of physical page range, including holes */ ... struct zone node_zones[MAX_NR_ZONES]; struct zonelist node_zonelists[MAX_ZONELISTS]; int nr_zones; ... } pg_data_t;  nr_zones：用于统计 NUMA 节点内包含的物理内存区域个数，不是每个 NUMA 节点都会包含以上介绍的所有物理内存区域，NUMA 节点之间所包含的物理内存区域个数是不一样的。   事实上只有第一个 NUMA 节点可以包含所有的物理内存区域，其它的节点并不能包含所有的区域类型，因为有些内存区域比如：ZONE_DMA，ZONE_DMA32 必须从物理内存的起点开始。这些在物理内存开始的区域可能已经被划分到第一个 NUMA 节点了，后面的物理内存才会被依次划分给接下来的 NUMA 节点。因此后面的 NUMA 节点并不会包含 ZONE_DMA，ZONE_DMA32 区域。\nZONE_NORMAL、ZONE_HIGHMEM 和 ZONE_MOVABLE 是可以出现在所有 NUMA 节点上的。\n  node_zones[MAX_NR_ZONES]：node_zones该数组包括了所有的zone物理内存区域 node_zonelists[MAX_ZONELISTS]：是 struct zonelist 类型的数组，它包含了备用 NUMA 节点和这些备用节点中的物理内存区域。    下面我们看一下struct zone结构体\n struct zone { ...... struct pglist_data\t*zone_pgdat; struct per_cpu_pageset __percpu *pageset; unsigned long\tzone_start_pfn; /* * spanned_pages is the total pages spanned by the zone, including * holes, which is calculated as: * spanned_pages = zone_end_pfn - zone_start_pfn; * * present_pages is physical pages existing within the zone, which * is calculated as: *\tpresent_pages = spanned_pages - absent_pages(pages in holes); * * managed_pages is present pages managed by the buddy system, which * is calculated as (reserved_pages includes pages allocated by the * bootmem allocator): *\tmanaged_pages = present_pages - reserved_pages; * */ unsigned long\tmanaged_pages; unsigned long\tspanned_pages; unsigned long\tpresent_pages; const char\t*name; ...... /* free areas of different sizes */ struct free_area\tfree_area[MAX_ORDER]; /* zone flags, see below */ unsigned long\tflags; /* Primarily protects free_area */ spinlock_t\tlock; ...... } ____cacheline_internodealigned_in_  zone_start_pfn：表示属于这个zone的第一个页 spanned_pages：看注释我们可以知道，spanned_pages = zone_end_pfn - zone_start_pfn，表示该区域的所有物理内存的页面数，包括内存空洞 present_pages：看注释我们可以知道，present_pages = spanned_pages - absent_pages(pages in holes)，表示该区域真实存在的物理内存页面数，不包括空洞 managed_pages：看注释我们可以知道，managed_pages = present_pages - reserved_pages，被伙伴系统管理的所有页面数。 per_cpu_pageset：用于区分冷热页，   什么叫冷热页呢？咱们讲 x86 体系结构的时候讲过，为了让 CPU 快速访问段描述符，在 CPU 里面有段描述符缓存。CPU 访问这个缓存的速度比内存快得多。同样对于页面来讲，也是这样的。如果一个页被加载到 CPU 高速缓存里面，这就是一个热页（Hot Page），CPU 读起来速度会快很多，如果没有就是冷页（Cold Page）。由于每个 CPU 都有自己的高速缓存，因而 per_cpu_pageset 也是每个 CPU 一个。\n   更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  3、内存页page #  内存页是物理内存最小单位，有时也叫页帧（page frame），Linux会为系统的物理内存的每一个页都创建了struct page对象，并用全局变量struct page *mem_map来存放所有物理页page对象的指针，页的大小取决于MMU（Memory Management Unit），后者主要用来将虚拟地址空间转换为物理地址空间。\n 看一下page的结构体\n struct page { unsigned long flags;\t/* Atomic flags, some possibly * updated asynchronously */ union { struct {\t/* Page cache and anonymous pages */ /** * @lru: Pageout list, eg. active_list protected by * zone_lru_lock. Sometimes used as a generic list * by the page owner. */ struct list_head lru; /* See page-flags.h for PAGE_MAPPING_FLAGS */ struct address_space *mapping; pgoff_t index;\t/* Our offset within mapping. */ /** * @private: Mapping-private opaque data. * Usually used for buffer_heads if PagePrivate. * Used for swp_entry_t if PageSwapCache. * Indicates order in the buddy system if PageBuddy. */ unsigned long private; }; struct {\t/* slab, slob and slub */ union { struct list_head slab_list;\t/* uses lru */ struct {\t/* Partial pages */ struct page *next; #ifdef CONFIG_64BIT  int pages;\t/* Nr of pages left */ int pobjects;\t/* Approximate count */ #else  short int pages; short int pobjects; #endif  }; }; struct kmem_cache *slab_cache; /* not slob */ /* Double-word boundary */ void *freelist;\t/* first free object */ union { void *s_mem;\t/* slab: first object */ unsigned long counters;\t/* SLUB */ struct {\t/* SLUB */ unsigned inuse:16; unsigned objects:15; unsigned frozen:1; }; }; }; ..... } 我们能够看到struct page有很多union组成，union 结构是在 C 语言中被用于同一块内存根据情况保存不同类型数据的一种方式。这里之所以用了 union，是因为一个物理页面使用模式有多种。\n 第一种模式：直接用一整页，这一整页的物理内存直接与虚拟地址空间建立映射关系，我们把这种称为匿名页（Anonymous Page）。或者用于关联一个文件，然后再和虚拟地址空间建立映射关系，这样的文件，我们称为内存映射文件（Memory-mapped File），这种分配页级别的，Linux采用一种被称为伙伴系统（Buddy System）的技术。 第二种模式：仅需要分配小的内存块。有时候，我们不需要一下子分配这么多的内存，例如分配一个 task_struct 结构，只需要分配小块的内存，去存储这个进程描述结构的对象。为了满足对这种小内存块的需要，Linux 系统采用了一种被称为slab allocator的技术   上面说的两种，都是页的分配方式，也就是物理内存的分配方式，下一章，我们继续深入分析物理内存的这两种分配方式。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":19,"href":"/docs/linux/linux_memory_manage/%E5%85%AD%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E4%BC%99%E4%BC%B4%E7%B3%BB%E7%BB%9F/","title":"六、物理内存分配——伙伴系统","section":"Linux 内存管理","content":"Linux内存管理 | 六、物理内存分配——伙伴系统 #  上一章，我们了解了物理内存的布局以及Linux内核对其的管理方式，页（page）也是物理内存的最小单元，Linux内核对物理内存的分配主要分为两种：一种是整页的分配，采用的是伙伴系统，另一种是小内存块的分配，采用的是slab技术。\n下面我们先来看看什么是伙伴系统！\n 1、伙伴系统（Buddy System） #  Linux系统中，对物理内存进行分配的核心是建立在页面级的伙伴系统之上。Linux内存管理的页大小为4KB，把所有的空闲页分组为11个页块链表，每个链表分别包含很多个大小的页块，有 1、2、4、8、16、32、64、128、256、512 和 1024 个连续页的页块，最大可以申请 1024 个连续页，对应 4MB 大小的连续内存。每个页块的第一个页的物理地址是该页块大小的整数倍。\n如下图所示：\n 第 i 个页块链表中，页块中页的数目为 2^i。——仔细理解这个页块的含义。\n  在struct zone结构体中，有下面定义\nstruct free_area\tfree_area[MAX_ORDER]; #define MAX_ORDER 11 free_area：存放不同大小的页块\nMAX_ORDER：就是指数\n 当向内核请求分配 (2^(i-1)，2^i] 数目的页块时，按照 2^i 页块请求处理。如果对应的页块链表中没有空闲页块，那我们就在更大的页块链表中去找。当分配的页块中有多余的页时，伙伴系统会根据多余的页块大小插入到对应的空闲页块链表中。\n举个例子：\n例如，要请求一个 128 个页的页块时，先检查 128 个页的页块链表是否有空闲块。如果没有，则查 256 个页的页块链表；如果有空闲块的话，则将 256 个页的页块分成两份，一份使用，一份插入 128 个页的页块链表中。如果还是没有，就查 512 个页的页块链表；如果有的话，就分裂为 128、128、256 三个页块，一个 128 的使用，剩余两个插入对应页块链表。\n 上面的这套机制就是伙伴系统所做的事情，它主要负责对物理内存页面进行跟踪，记录哪些是被内核使用的页面，哪些是空闲页面。\n 2、页面分配器（Page Allocator） #  由上一章我们知道，物理内存被分为了几个区域：ZONE_DMA、ZONE_NORMAL、ZONE_HIGHMEM，其中前两个区域的物理页面与虚拟地址空间是线性映射的。\n页面分配器主要的工作原理如下：\n 如果页面分配器分配的物理页面在ZONE_DMA、ZONE_NORMAL区域，那么对应的虚拟地址到物理地址映射的页目录已经建立，因为是线性映射，两者之间有一个差值PAGE_OFFSET。 如果页面分配器分配的物理页面在ZONE_HIGHMEM区域，那么内核此时还没有对该页面进行映射，因此页面分配器的调用者，首先在虚拟地址空间的动态映射区或者固定映射区分配一个虚拟地址，然后映射到该物理页面上。   以上就是页面分配器的原理，对于我们只需要调用相关接口函数就可以了。\n页面分配函数主要有两个：alloc_pages和__get_free_pages，而这两个函数最终也会调用到alloc_pages_node，其实现原理完全一样。\n 下面我们从代码层面来看页面分配器的工作原理\n   更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  3、gfp_mask #  我们先来了解一下gfp_mask，它并不是页面分配器函数，而只是这些页面分配函数中一个重要的参数，是个用于控制分配行为的掩码，并可以告诉内核应该到哪个zone中分配物理内存页面。\n/* Plain integer GFP bitmasks. Do not use this directly. */ #define ___GFP_DMA\t0x01u #define ___GFP_HIGHMEM\t0x02u #define ___GFP_DMA32\t0x04u #define ___GFP_MOVABLE\t0x08u #define ___GFP_RECLAIMABLE\t0x10u #define ___GFP_HIGH\t0x20u #define ___GFP_IO\t0x40u #define ___GFP_FS\t0x80u #define ___GFP_WRITE\t0x100u #define ___GFP_NOWARN\t0x200u #define ___GFP_RETRY_MAYFAIL\t0x400u #define ___GFP_NOFAIL\t0x800u #define ___GFP_NORETRY\t0x1000u #define ___GFP_MEMALLOC\t0x2000u #define ___GFP_COMP\t0x4000u #define ___GFP_ZERO\t0x8000u #define ___GFP_NOMEMALLOC\t0x10000u #define ___GFP_HARDWALL\t0x20000u #define ___GFP_THISNODE\t0x40000u #define ___GFP_ATOMIC\t0x80000u #define ___GFP_ACCOUNT\t0x100000u #define ___GFP_DIRECT_RECLAIM\t0x200000u #define ___GFP_KSWAPD_RECLAIM\t0x400000u #ifdef CONFIG_LOCKDEP #define ___GFP_NOLOCKDEP\t0x800000u #else #define ___GFP_NOLOCKDEP\t0 #endif   ___GFP_DMA：在ZONE_DMA标识的内存区域中查找空闲页。\n  ___GFP_HIGHMEM：在ZONE_HIGHMEM标识的内存区域中查找空闲页。\n  ___GFP_MOVABLE：内核将分配的物理页标记为可移动的。\n  ___GFP_HIGH：内核允许使用紧急分配链表中的保留内存页。该请求必须以原子方式完成，意味着请求过程不允许被中断。\n  ___GFP_IO：内核在查找空闲页的过程中可以进行I/O操作，如此内核可以将换出的页写到硬盘。\n  ___GFP_FS：查找空闲页的过程中允许执行文件系统相关操作。\n  ___GFP_ZERO：用0填充成功分配出来的物理页。\n   通常意义上，这些以“__”打头的GFP掩码只限于在内存管理组件内部的代码使用，对于提供给外部的接口，比如驱动程序中所使用的页面分配函数，gfp_mask掩码以“GFP_”的形式出现，而这些掩码基本上就是上面提到的掩码的组合。\n例如内核为外部模块提供的最常使用的几个掩码如下：\n#define GFP_ATOMIC\t(__GFP_HIGH|__GFP_ATOMIC|__GFP_KSWAPD_RECLAIM) #define GFP_KERNEL\t(__GFP_RECLAIM | __GFP_IO | __GFP_FS) #define GFP_KERNEL_ACCOUNT (GFP_KERNEL | __GFP_ACCOUNT) #define GFP_NOWAIT\t(__GFP_KSWAPD_RECLAIM) #define GFP_NOIO\t(__GFP_RECLAIM) #define GFP_NOFS\t(__GFP_RECLAIM | __GFP_IO) #define GFP_USER\t(__GFP_RECLAIM | __GFP_IO | __GFP_FS | __GFP_HARDWALL) #define GFP_DMA\t__GFP_DMA #define GFP_DMA32\t__GFP_DMA32 #define GFP_HIGHUSER\t(GFP_USER | __GFP_HIGHMEM) #define GFP_HIGHUSER_MOVABLE\t(GFP_HIGHUSER | __GFP_MOVABLE) #define GFP_TRANSHUGE_LIGHT\t((GFP_HIGHUSER_MOVABLE | __GFP_COMP | \\ __GFP_NOMEMALLOC | __GFP_NOWARN) \u0026amp; ~__GFP_RECLAIM) #define GFP_TRANSHUGE\t(GFP_TRANSHUGE_LIGHT | __GFP_DIRECT_RECLAIM)  GFP_ATOMIC：内核模块中最常使用的掩码之一，用于原子分配。此掩码告诉页面分配器，在分配内存页时，绝对不能中断当前进程或者把当前进程移出调度器。 GFP_KERNEL：内核模块中最常使用的掩码之一，带有该掩码的内存分配可能导致当前进程进入睡眠状态。 GFP_USER：用于为用户空间分配内存页，可能引起进程的休眠。 GFP_NOIO：在分配过程中禁止I/O操作 GFP_NOFS：禁止文件系统相关的函数调用 GFP_DMA：限制页面分配器只能在ZONE_DMA域中分配空闲物理页面，用于分配适用于DMA缓冲区的内存。   通过gfp_mask掩码，更加方便我们控制页面分配器到哪个区域去分配物理内存，分配内存的优先级如下：\n 指定__GFP_HIGHMEM：先在ZONE_HIGHMEM域中查找空闲页，如果无法满足当前分配，页分配器将回退到ZONE_NORMAL域中继续查找，如果依然无法满足当前分配，分配器将回退到ZONE_DMA域，或者成功或者失败。 指定__GFP_DMA：只能在ZONE_DMA中分配物理页面，如果无法满足，则分配失败。 没有__GFP_NORMAL这样的掩码，但是前面已经提到，如果gfp_mask中没有明确指定__GFP_HIGHMEM或者是__GFP_DMA，默认就相当于__GFP_NORMAL，优先在ZONE_NORMAL域中分配，其次是ZONE_DMA域。   4、alloc_pages #  alloc_pages函数负责分配2^order个连续的物理页面并返回起始页的struct page实例。\nalloc_pages的实现源码如下：\nstatic inline struct page * alloc_pages(gfp_t gfp_mask, unsigned int order) { return alloc_pages_current(gfp_mask, order); } /** * alloc_pages_current - Allocate pages. * *\t@gfp: *\t%GFP_USER user allocation, * %GFP_KERNEL kernel allocation, * %GFP_HIGHMEM highmem allocation, * %GFP_FS don\u0026#39;t call back into a file system. * %GFP_ATOMIC don\u0026#39;t sleep. *\t@order: Power of two of allocation size in pages. 0 is a single page. * *\tAllocate a page from the kernel page pool. When not in *\tinterrupt context and apply the current process NUMA policy. *\tReturns NULL when no page can be allocated. */ struct page *alloc_pages_current(gfp_t gfp, unsigned order) { struct mempolicy *pol = \u0026amp;default_policy; struct page *page; if (!in_interrupt() \u0026amp;\u0026amp; !(gfp \u0026amp; __GFP_THISNODE)) pol = get_task_policy(current); /* * No reference counting needed for current-\u0026gt;mempolicy * nor system default_policy */ if (pol-\u0026gt;mode == MPOL_INTERLEAVE) page = alloc_page_interleave(gfp, order, interleave_nodes(pol)); else page = __alloc_pages_nodemask(gfp, order, policy_node(gfp, pol, numa_node_id()), policy_nodemask(gfp, pol)); return page; } EXPORT_SYMBOL(alloc_pages_current); alloc_pages调用alloc_pages_current，其中\n gfp参数：即上文的gfp_mask，表明我们想要在哪个物理内存区域进行内存分配 order参数：表示分配 2 的 order 次方个页。  __alloc_pages_nodemask为伙伴系统的核心实现，它会调用 get_page_from_freelist。\nstatic struct page * get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags, const struct alloc_context *ac) { ...... for_next_zone_zonelist_nodemask(zone, z, ac-\u0026gt;zonelist, ac-\u0026gt;high_zoneidx, ac-\u0026gt;nodemask) { struct page *page; ...... page = rmqueue(ac-\u0026gt;preferred_zoneref-\u0026gt;zone, zone, order, gfp_mask, alloc_flags, ac-\u0026gt;migratetype); ...... } 这里面的逻辑也很容易理解，就是在一个循环中先看当前节点的 zone。如果找不到空闲页，则再看备用节点的 zone。\n 每一个 zone，都有伙伴系统维护的各种大小的队列，就像上面伙伴系统原理里讲的那样。\n 这里调用 rmqueue 就很好理解了，就是找到合适大小的那个队列，把页面取下来。\n 伙伴系统的实现代码，感兴趣的可以深入探究。\n 在调用这个函数的时候，有几种情况：\n 如果gfp_mask中没有指定__GFP_HIGHMEM，那么分配的物理页面必然来自ZONE_NORMAL或者ZONE_DMA，由于这两个区域内核在初始化的时候就已经建立了映射关系，所以内核很容易就能找到对应的虚拟地址KVA（Kernel Virtual Address） 如果gfp_mask中指定了__GFP_HIGHMEM，那么页分配器将优先在ZONE_HIGHMEM域中分配物理页，但也不排除因为ZONE_HIGHMEM没有足够的空闲页导致页面来自ZONE_NORMAL与ZONE_DMA域的可能性。对于新分配出的高端物理页面，由于内核尚未在页表中为之建立映射关系，所以此时需要：  在内核的动态映射区分配一个KVA 通过操作页表，将第一步中的KVA映射到该物理页面上，通过kmap实现     5、__get_free_pages #  __get_free_pages该函数负责分配2^ordev个连续的物理页面，返回起始页面所在内核线性地址。\n函数的实现如下：\n/* * Common helper functions. Never use with __GFP_HIGHMEM because the returned * address cannot represent highmem pages. Use alloc_pages and then kmap if * you need to access high mem. */ unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order) { struct page *page; page = alloc_pages(gfp_mask \u0026amp; ~__GFP_HIGHMEM, order); if (!page) return 0; return (unsigned long) page_address(page); } EXPORT_SYMBOL(__get_free_pages); 我们可以看到，函数内部调用了alloc_pages函数，并且不能从__GFP_HIGHMEM高端内存分配物理页，最后通过page_address来返回页面的起始页面的内核线性地址。\n 6、get_zeroed_page #  get_zeroed_page用于分配一个物理页同时将页面对应的内容填充为0，函数返回页面所在的内核线性地址。\n可以看下内核代码：\nunsigned long get_zeroed_page(gfp_t gfp_mask) { return __get_free_pages(gfp_mask | __GFP_ZERO, 0); } EXPORT_SYMBOL(get_zeroed_page); 仅仅是在__get_free_pages基础上，使用了 __GFP_ZERO标志，来初始化分配页面的初始内容。\n 7、总结 #  以上，就是建立在伙伴系统之上的页面级分配器，常用的函数有：alloc_pages、__get_free_pages、get_zeroed_page、__get_dma_pages等，其底层实现都是一样的，只是gfp_mask不同。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":20,"href":"/docs/embeded_tech/self_improve/10w+%E9%98%85%E8%AF%BB%E8%80%97%E6%97%B6%E4%B8%80%E5%91%A8%E6%80%BB%E7%BB%93%E7%9A%84%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF%E8%B6%85%E8%AF%A6%E7%BB%86/","title":"【10W+阅读】耗时一周总结的嵌入式学习路线，超详细","section":"嵌入式工程师养成记","content":"【10W+阅读】耗时一周总结的嵌入式学习路线，超详细 #  人们常说：“人生就是一场场游戏，我们要做的，就是打怪，升级，通关”，学习嵌入式的过程也是如此。\n1、前言 #  最近看到知乎上，给我推送了很多类似的回答，借此机会，也想着重新审视一下自己的学习历程，以及结合自身和大牛，分享一些学习经验，希望对大家有所启发和帮助。\n本文主要目的是为了：\n 提供一张嵌入式学习路线地图 提供不同阶段的学习建议 推荐不同阶段的学习资料  3000余字，耗时1周，建议收藏；码字不易，如有帮助，慷慨三连\n 本文将嵌入式学习路线分为几个方面：\n 嵌入式基础必备知识 51单片机 STM32单片机 小而美的RTOS ARM+LINUX   在这个快节奏的时代，能静下心，耐住性子看看文章，实属不易。\n  2、嵌入式基础必备知识 #  老子曰：“合抱之木，生于毫末：九层之台，起于垒土；千里之行，始于足下”，根基的重要性不言而喻。\n那么对于嵌入式这条路线而言，如何建立一个稳固的根基？\n 2.1、学习内容 #   C语言基础  该部分，主要包括几个核心知识点：三大语法结构、常用的数据类型、函数、结构体、指针、文件操作等。\n 硬件基础知识  该部分，核心知识点在于：电路基础知识、数电模电基础知识、常用的电子元器件等。\n 数据结构  核心知识点：数组、队列、链表、堆栈、树、图、散列表等。\n 操作系统  核心知识点：进程管理、内存管理、文件管理、输入输出管理等。\n 计算机原理  核心知识点：数据表示和运算、存储系统、指令系统、总线系统、中央处理器、输入输出系统等。\n 2.2、学习建议 #   对于C语言基础学习，一定要重点熟练掌握，根基的牢固直接决定了我们的代码质量。 对于硬件基础学习，要适当了解，要能够看懂一些简单的电路结构，认识常用的电子元器件。 对于数据结构学习，前五个是必备学习的，可能在刚开始学习的时候，可能会感觉不到作用在哪里，但是随着接触到嵌入式底层设计以及算法设计的时候，才会恍然大悟。 对于操作系统学习，重点学习其思想，对相关知识点有一个大概的了解，后续接触到继续重点学习，这些无论是RTOS，还是Linux，都有涉及到的。 对于计算机原理学习，可以将其看作是嵌入式系统的各个模块的详解，会让你对嵌入式有一个整体的了解，每一个部分都值得深究。   2.3、学习资料 #   C语言基础：推荐经典书籍**《C语言程序设计》（第2版）谭浩强版本**。 硬件基础：大学里面的《数电模电》书籍所涉及的知识即可。 数据结构：推荐经典书籍**《数据结构》——严蔚敏版**。 操作系统、计算机原理：我用的是**《王道》的系列丛书**，个人感觉不错。  计算机组成、数据结构、操作系统、数据库是嵌入式或者说计算机的入门必读书籍，并且也被列入高校教材内，是真正的基础知识。\n 以上，不一定是全部看完才能体验编程的乐趣，这个基础是一个循序渐进的过程，也不是一朝一夕就能完成的，可以先有一个大概，后续做项目时，哪里不懂补哪里！\n这里涉及到一个重要的学习方法：项目导向的学习法。\n 3、嵌入式入门篇——51单片机 #  在上面的基础知识进行熟悉之后（C语言基础、计算机组成、硬件基础必备），我们准备叩开嵌入式世界的大门。\n入门篇，依旧推荐51单片机，当然有人会说，直接上STM32岂不更好？\n我的看法：建议新手还是以51单片机来入门，因为STM32体系架构比51大很多，对于新手刚开始可能会不太容易适应。\n 3.1、学习内容 #  该部分，主要在最小嵌入式系统中，实现各种有趣的实验。通过51单片机的学习，我们要做到：\n 软件类：  主要知识点有：认识单片机、熟悉逻辑运算、点亮一颗LED灯、按键检测、串口通信、定时器、中断等。\n 硬件类：  主要知识点有：电阻元器件了解，基本模块电路了解，时钟电路，尝试绘制51单片机原理图和PCB\n 3.2、学习建议 #   对于软件类，我们主要做到：认识单片机，熟悉单片机的GPIO的输入、输出操作，串口通信协议掌握等，这些部分都是任何一款嵌入式设备的必备技能。 对于硬件类：我们主要做到：能看懂电路图，熟悉一些简单模块的设计电路，了解Altium Designer的使用方法。   3.3、学习资料 #  51单片机：郭天祥的51单片机教程，经典著作，经久不衰，强烈推荐。\n 庄子说：“水之积也不厚，则其负大舟也无力。“\n该部分，是嵌入式领域的基石，只有将基础打牢，才能负得起Linux泰坦号。\n 4、STM32进阶篇 #  STM32是C51的进阶版，拥有C51的基础知识，开发STM32会得心应手。\nSTM32的系统架构以及硬件设计相比于C51来说，都是上升了一个维度的，这也是为什么我推荐入门学习C51的原因。\n 以STM32F407平台为基础，去学习目前嵌入式主流的一些技术，探寻底层的原理，做到不同平台，都能够得心应手。\n 4.1、学习内容 #   基础练习  该部分，主要练习：点亮LED灯、GPIO的输入输出操作、中断操作、UART通信、IIC通信等\n 进阶练习  该部分，主要练习：DMA通信、SPI通信、CAN通信、LCD显示屏，ADC等\n 高阶练习  该部分，主要学习：STM32时钟架构、总线架构、电源管理、代码框架、SDIO通信、USB通信等。\n 4.2、学习建议 #   对于基础练习，主要目的是为了方便让我们从C51到STM32环境的过渡。 对于进阶练习，主要练习一些通信类相关的协议，可以结合一些传感器进行开发。 对于高阶练习，主要目的是为了熟悉单片机的设计架构，编程的框架，以及一些更复杂的通信技术。  另外，STM32会有寄存器和库函数两个版本，建议交叉学习，理解会更加深刻。\n 4.3、学习资料 #  STM32单片机：推荐正点原子、野火的STM32F103或者STM32F407系列。\n两家的学习资料都非常丰富，既有详细的文档说明，也有完整的学习视频教程，非常适合新手入门学习。\n 俗话说：“有道无术，术尚可求，有术无道，止于术”。要明白道和术的区别，不要本末倒置。\n 5、小而美的RTOS #  RTOS，实时操作系统，可以理解为STM32与Linux之间的桥梁，由于其实现思想大都取之于Linux，所以也称之为精简版的Linux。\n我们常用的有实时操作系统有：UCOS，VxWork，FreeRtos，近些年RT-Thread也异军突起。\n学习这些简单的嵌入式系统，一来能够帮助我们为学习Linux操作系统打下基础，二来也能够扩宽我们的职业道路。\n前面也说过了，无论是UCOS、FreeRtos、Rt-thread，其内部的设计思想大同小异，下面主要以Ucos为例。\n5.1、学习内容 #   实时系统学习  该部分，主要学习：移植Ucos系统、多任务管理、调度算法、消息队列、信号量互斥量、事件、内存管理等。\n 5.2、学习建议 #   对于实时系统学习，除了上述的那些核心知识点外，还要结合2.1 基础必备知识的操作系统书籍加深理解。   5.3、学习资料 #  RTOS的学习：依旧推荐正点原子，野火，因为这些实时操作系统开发，可以基于STM32开发板，同时也有非常详细的文档和视频教学。\n 6、ARM+Linux篇 #  学习完RTOS后，基本嵌入式所涉及的技术已经掌握一半了，你也可以独立完成一些小的项目，也可以找到一个不错的工作，但是一定不要自我满足，有机会一定要接触Linux。\n还是那句话：ARM+Linux，也是最为复杂的东西，如果你不去接触Linux，你永远不知道嵌入式的魅力。\n Linux开发又分为驱动开发，内核开发，应用开发，每一个方向都需要几年甚至几十年的积累。\n 作为初学者，我们要做的就是宏观了解，扩大我们的知识面，然后去选择自己感兴趣的方面。\n 6.1、学习内容 #   Linux基础篇  该部分主要学习：Linux常用命令、VIM学习、Linux的Shell编程、Gcc编译、Makefile等。\n 驱动篇  该部分主要学习：内核模块编译原理、字符设备驱动框架、平台设备驱动、设备树、Pinctrl子系统、I2C子系统、中断子系统、块设备驱动框架、Bootloader等\n 内核篇  该部分主要学习：系统调用、存储管理、进程管理、内存管理、文件管理等。\n 应用篇  该部分主要学习：QT编程、TCP/IP协议、HTTP协议等。\n 6.2、学习建议 #   对于基础学习，刚接触到Linux，一般比较难上手，与之前的单片机完全不同，需要一个熟悉环境的过程。 对于驱动学习，重要在于明白“如何在Linux环境下编写驱动程序”，驱动的底层原理还是那样，加了一层层的框架，需要我们去熟悉。 对于内核学习，上述也是系统的几大核心特色，重点在于\u0026quot;如何使Linux性能最优\u0026quot; 对于应用学习，上述的几个方面也是基础，重点还在于开发什么应用，去学习哪方面的知识，没有定论。  对于Linux，有句老话“学习Linux，3年才算入门，5年才勉强算Linux工程师，对于不太熟悉的领域，博主也不敢妄加断言。”\n 6.3、学习资料 #   对于基础学习，推荐**《鸟哥的Linux私房菜》，《Unix环境高级编程》**等入门书籍。 对于驱动开发，推荐**《Linux设备驱动开发详解》**，Linux内核源码详解等。 对于内核学习，推荐**《Linux Shell脚本攻略》、《深入理解Linux内核》**等。 对于应用开发，推荐**《嵌入式Linux应用开发完全手册》、《Unix网络编程》**等。 另外，推荐正点原子，野火，韦东山三个Linux开发教程，韦老师的课程好评居多，但还是看哪个更适合自己。   7、总结 #  全文整体的学习路线：嵌入式基础学习 -\u0026gt; 51单片机 -\u0026gt; STM32单片机 -\u0026gt; RTOS篇 -\u0026gt; ARM+Linux\n每一个部分，也都从学习内容，学习建议，学习资料三个方面来展开，层层深入，步步指引。\n文章既是我的学习历程，又结合了一些大佬的学习分享，不断调整总结出来的，如有异同，可以讨论。\n全文3000余字，耗时1周，如有帮助，望不吝点赞关注。\n最后，文章所涉及的学习资料以及整理的思维导图，全部会在我的星球【嵌入式艺术】分享！\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":21,"href":"/about/index_zh/","title":"About","section":"Abouts","content":"1、个人介绍 #  🙍🏻‍♂️ 大家好，我是董哥，一名工作多年的嵌入式Linux开发工程师。以下是我的基本信息介绍：\n 参加全国机器人大赛（Robocon），两次获得全国一等奖 毕业后斩获科沃斯，石头，格力等多家头部机器人公司的offer，后入职世界五百强格力电器担任嵌入式开发工程师 现今就职于独角兽芯片企业，担任嵌入式Linux驱动开发工程师 熟练使用C/C++语言开发，熟悉各类MCU开发，如STM32，ARM，SOC等，熟悉Ucos，RT-thread实时操作系统等 目前主要负责Linux驱动，系统开发，WiFi\u0026amp;BT开发等相关工作，同时跟进并参与多款百万级量产项目的研发。 荣获优质嵌入式领域创作者称号，拿下2022年度博客之星嵌入式领域TOP 5，全网收获超百万读者。  2、技术与分享 #  记录Blog是一项值得挑战的事情，一方面是对自我技术的沉淀，另一方面也是四万万嵌入式开发者前行路上的加速剂；并且网上大多数文章七零八落，每个人对技术的理解程度不同，因此好的文章，永不过时！\n我的一些自媒体平台：\n CSDN：卍一十二画卍 知乎：嵌入式艺术 公众号：嵌入式艺术 知识星球：嵌入式艺术  3、我的星球 #  🚩 【嵌入式艺术】星球，目前是处于起步阶段，我们的目标是：携手共创高质量的嵌入式基地，兼收并蓄，群英荟萃，实现升职加薪创业梦！\n🛎️ 我们提供的服务有：\n 提供一个高级嵌入式工程师聚集地，聚焦嵌入式工程师成长与发展。 高质量嵌入式项目、技术的拆解与分析 高效率的嵌入式开发工具分享 AIGC + 嵌入式 应用，跟上时代的脚步 嵌入式的行业趋势与热点分析  🛎️ 我们后续要做的事情：\n 引入更多嵌入式领域大咖加入我们的星球，为大家提供更好的服务！ 引入更多优质公司的内推岗位，以便大家走内部推荐通道，加入头部企业！ 拆解更多嵌入式项目，为大家提供实战经验，以目标为导向，实现更好的学习效果！ 星球不定期举办激励活动，有实物激励以及现金激励两种，希望大家踊跃参加！  "}]