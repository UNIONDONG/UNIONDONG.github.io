[{"id":0,"href":"/docs/linux/linux_driver_develop_basic/%E4%B8%80%E6%96%87%E7%A7%92%E6%87%82linux%E5%AD%97%E7%AC%A6%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8/","title":"【一文秒懂】Linux字符设备驱动","section":"Linux 驱动开发基础","content":"【一文秒懂】Linux字符设备驱动 #  1、前言 #  众所周知，Linux内核主要包括三种驱动模型，字符设备驱动，块设备驱动以及网络设备驱动。\n其中，Linux字符设备驱动，可以说是Linux驱动开发中最常见的一种驱动模型。\n我们该系列文章，主要为了帮助大家快速入门Linux驱动开发，该篇主要来了解一些字符设备驱动的框架和机制。\n 系列文章基于Kernel 4.19\n  2、关键数据结构 #  2.1 cdev #  struct cdev { struct kobject kobj; struct module *owner; const struct file_operations *ops; struct list_head list; dev_t dev; unsigned int count; } __randomize_layout; 结构体名称：cdev\n文件位置：include/linux/cdev.h\n主要作用：cdev可以理解为char device，用来抽象一个字符设备。\n核心成员及含义：\n kobj：表示一个内核对象。 owner：指向该模块的指针 ops：指向文件操作的指针，包括open、read、write等操作接口 list：用于将该设备加入到内核模块链表中 dev：设备号，由主设备号和次设备号构成 count：表示有多少个同类型设备，也间接表示设备号的范围 __randomize_layout：一个编译器指令，用于随机化结构体的布局，以增加安全性。   2.2 file_operations #  struct file_operations { struct module *owner; loff_t (*llseek) (struct file *, loff_t, int); ssize_t (*read) (struct file *, char __user *, size_t, loff_t *); ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *); ssize_t (*read_iter) (struct kiocb *, struct iov_iter *); ssize_t (*write_iter) (struct kiocb *, struct iov_iter *); int (*iterate) (struct file *, struct dir_context *); int (*iterate_shared) (struct file *, struct dir_context *); __poll_t (*poll) (struct file *, struct poll_table_struct *); long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long); long (*compat_ioctl) (struct file *, unsigned int, unsigned long); int (*mmap) (struct file *, struct vm_area_struct *); unsigned long mmap_supported_flags; int (*open) (struct inode *, struct file *); int (*flush) (struct file *, fl_owner_t id); int (*release) (struct inode *, struct file *); int (*fsync) (struct file *, loff_t, loff_t, int datasync); int (*fasync) (int, struct file *, int); int (*lock) (struct file *, int, struct file_lock *); ssize_t (*sendpage) (struct file *, struct page *, int, size_t, loff_t *, int); unsigned long (*get_unmapped_area)(struct file *, unsigned long, unsigned long, unsigned long, unsigned long); int (*check_flags)(int); int (*flock) (struct file *, int, struct file_lock *); ssize_t (*splice_write)(struct pipe_inode_info *, struct file *, loff_t *, size_t, unsigned int); ssize_t (*splice_read)(struct file *, loff_t *, struct pipe_inode_info *, size_t, unsigned int); int (*setlease)(struct file *, long, struct file_lock **, void **); long (*fallocate)(struct file *file, int mode, loff_t offset, loff_t len); void (*show_fdinfo)(struct seq_file *m, struct file *f); #ifndef CONFIG_MMU  unsigned (*mmap_capabilities)(struct file *); #endif  ssize_t (*copy_file_range)(struct file *, loff_t, struct file *, loff_t, size_t, unsigned int); int (*clone_file_range)(struct file *, loff_t, struct file *, loff_t, u64); int (*dedupe_file_range)(struct file *, loff_t, struct file *, loff_t, u64); int (*fadvise)(struct file *, loff_t, loff_t, int); } __randomize_layout; 结构体名称：file_operations\n文件位置：include/linux/fs.h\n主要作用：正如其名，主要用来描述文件操作的各种接口，Linux一切接文件的思想，内核想要操作哪个文件，都需要通过这些接口来实现。\n核心成员及含义：\n open：打开文件的函数 read：读取文件的函数。 write：写入文件的函数。 release：关闭文件的函数。 flush：刷新文件的函数，通常在关闭文件时调用。 llseek：改变文件读写指针位置的函数。 fsync：将文件数据同步写入磁盘的函数。 poll：询问文件是否可被非阻塞读写   2.3 dev_t #  typedef u32 __kernel_dev_t; typedef __kernel_dev_t\tdev_t; 类型名称：dev_t\n文件位置：include/linux/types.h\n主要作用：表示字符设备对应的设备号，其中包括主设备号和次设备号。\n 3、数据结构之间关系 #   上图绘制是对字符设备驱动程序的数据结构以及API的关系图，\n有需要原始文件，可在公~号【嵌入式艺术】获取。\n  4、字符设备驱动整体架构 #  4.1 加载与卸载函数 #  驱动首先实现的就是加载和卸载函数，也是驱动程序的入口函数。\n我们一般这么定义驱动的加载卸载函数：\nstatic int __init xxx_init(void) { } static void __exit xxx_exit(void) { } module_init(xxx_init); module_exit(xxx_exit); 这段代码就是实现一个通用驱动的加载与卸载，关于module_init和module_exit的实现机制，可以查看之前总结文章。\n 4.2 设备号管理 #  4.2.1 设备号的概念 #  每一类字符设备都有一个唯一的设备号，其中设备号又分为主设备号和次设备号，那么这两个分别作用是什么呢？\n 主设备号：用于标识设备的类型， 次设备号：用于区分同类型的不同设备   简单来说，主设备号用于区分是IIC设备还是SPI设备，而次设备号用于区分IIC设备下，具体哪一个设备，是MPU6050还是EEPROM。\n  4.2.2 设备号的分配 #   了解了设备号的概念，Linux中设备号有那么多，那么我们该如何去使用正确的设备号呢？\n 设备号的分配方式有两种，一种是动态分配，另一种是静态分配，也可以理解为一种是内核自动分配，一种是手动分配。\n静态分配函数：\nint register_chrdev_region(dev_t from, unsigned count, const char *name);  from：表示已知的一个设备号 count：表示连续设备编号的个数，（同类型的设备有多少个） name：表示设备或者驱动的名称  函数作用：以from设备号开始，连续分配count个同类型的设备号\n 动态分配函数：\nint alloc_chrdev_region(dev_t *dev, unsigned baseminor, unsigned count, const char *name);  dev：设备号的指针，用于存放分配的设备号的值 baseminor：次设备号开始分配的起始值 count：表示连续设备编号的个数，（同类型的设备有多少个） name：表示设备或者驱动的名称  函数作用：从baseminor次设备号开始，连续分配count个同类型的设备号，并自动分配一个主设备号，将主、次组成的设备号信息赋值给*dev\n 这两个函数最大的区别在于：\n register_chrdev_region：调用前，已预先定义好了主设备号和次设备号，调用该接口后，会将自定义的设备号登记加入子系统中，方便系统追踪系统设备号的使用情况。 alloc_chrdev_region：调用前，未定义主设备号和次设备号；调用后，主设备号以0来表示，以自动分配，并且将自动分配的设备号，同样加入到子系统中，方便系统追踪系统设备号的使用情况。   这两个函数的共同点在于：\n系统维护了一个数组列表，用来登记所有的已使用的设备号信息，这两个接口归根到底也是将其设备号信息，登记到系统维护的设备号列表中，以免后续冲突使用。\n在Linux中，我们可以通过cat /proc/devices命令，查看所有i登记的设备号列表。\n  后面有时间，我们可以详细聊设备号的自动分配机制，管理机制。\n  4.2.3 设备号的注销 #  设备号作为一种系统资源，当所对应的设备卸载时，当然也要将其所占用的设备号归还给系统，无论时静态分配，还是动态分配，最终都是调用下面函数来注销的。\nvoid unregister_chrdev_region(dev_t from, unsigned count);  from：表示已知的一个设备号 count：表示连续设备编号的个数，（同类型的设备有多少个）  函数作用：要注销from主设备号下的连续count个设备\n 4.2.4 设备号的获取 #  设备号的管理很简单，在关键数据结构中，我们看到设备号的类型是dev_t，也就是u32类型表示的一个数值。\n其中主设备号和次设备号的分界线，由MINORBITS宏定义指定：\n#define MINORBITS\t20 也就是主设备号占用高12bit，次设备号占用低20bit\n并且，内核还提供了相关API接口，来获取主设备号和次设备号，以及生成设备号的接口，如下：\n#define MINORMASK\t((1U \u0026lt;\u0026lt; MINORBITS) - 1)  #define MAJOR(dev)\t((unsigned int) ((dev) \u0026gt;\u0026gt; MINORBITS)) #define MINOR(dev)\t((unsigned int) ((dev) \u0026amp; MINORMASK)) #define MKDEV(ma,mi)\t(((ma) \u0026lt;\u0026lt; MINORBITS) | (mi))  以上，通过移位操作，来实现主次设备号的获取。\n  4.2.4 通用代码实现 #  #define CUSTOM_DEVICE_NUM 0 #define DEVICE_NUM 1 #device DEVICE_NAME \u0026#34;XXXXXX\u0026#34; static dev_t global_custom_major = CUSTOM_DEVICE_NUM; static int __init xxx_init(void) { dev_t custom_device_number= MKDEV(global_custom_major, 0);\t//\tcustom device number  /* device number register*/ if (global_custom_major) { ret = register_chrdev_region(custom_device_number, DEVICE_NUM, DEVICE_NAME); } else { ret = alloc_chrdev_region(\u0026amp;custom_device_number, 0, DEVICE_NUM, DEVICE_NAME); global_custom_major = MAJOR(custom_device_number); } } static void __exit xxx_exit(void) { unregister_chrdev_region(MKDEV(global_mem_major, 0), DEVICE_NUM); } module_init(xxx_init); module_exit(xxx_exit); 该函数实现了设备号的分配，如果主设备号为0，则采用动态配分的方式，否则采用静态分配的方式。\n  更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  4.3 字符设备的管理 #  了解完设备号的管理之后，我们来看下字符设备是如何管理的。\n4.3.1、字符设备初始化 #  void cdev_init(struct cdev *cdev, const struct file_operations *fops);  cdev：一个字符设备对象，也就是我们创建好的字符设备 fops：该字符设备的文件处理接口  函数作用：初始化一个字符设备，并且将所对应的文件处理指针与字符设备绑定起来。\n 4.3.2、字符设备注册 #  int cdev_add(struct cdev *p, dev_t dev, unsigned count);  p：一个字符设备指针，只想待添加的字符设备对象 dev：该字符设备所负责的第一个设备编号 count：该类型设备的个数  函数作用：添加一个字符设备驱动到Linux系统中。\n 4.3.3、字符设备注销 #  void cdev_del(struct cdev *p);  p：指向字符设备对象的指针  函数作用：从系统中移除该字符设备驱动\n 4.4 文件操作接口的实现 #  因为在Linux中，一切皆文件的思想，所以每一个字符设备，也都有一个文件节点来对应。\n我们在初始化字符设备的时候，会将struct file_operations的对象与字符设备进行绑定，其作用是来处理该字符设备的open、read、write等操作。\n我们要做的就是去实现我们需要的函数接口，如：\nstatic const struct file_operations global_mem_fops = { .owner = THIS_MODULE, .llseek = global_mem_llseek, .read = global_mem_read, .write = global_mem_write, .unlocked_ioctl = global_mem_ioctl, .open = global_mem_open, .release = global_mem_release, }; 至此，我们一个基本的字符设备驱动程序的框架就基本了然于胸了\n 5、总结 #  本篇文章，旨在通俗易懂的讲解：\n 字符设备驱动相关数据结构 数据结构关系图 核心API接口 字符设备驱动整体框架  希望对大家有所帮助。\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":1,"href":"/docs/linux/linux_mmc_subsystem/mmc%E5%AD%90%E7%B3%BB%E7%BB%9F%E4%B8%80mmc_sd_sdio%E4%BB%8B%E7%BB%8D/","title":"【MMC子系统】一、MMC_SD_SDIO介绍","section":"Linux MMC 子系统","content":"【MMC子系统】 一、MMC/SD/SDIO介绍 #  1、前言 #  该节学习Linux Kernel的MMC子系统，也称为块设备驱动，正如其名，与字符驱动相比，MMC子系统以块为单位进行操作。\n同时，由于MMC Card、SD Card、SDIO Card等设备协议基本大同小异，所以在Linux Kernel中使用MMC子系统来统一管理！\n 2、MMC/SD/SDIO介绍 #  上面我们了解到，Linux Kernel使用统一的子系统模型来管理MMC、SD、SDIO等设备，那么为什么要这样设计呢？\n 答案当然是：三者协议有一定的共通性。\n  MMC（MultiMediaCard）多媒体卡设备，从本质上看，它是一种用于固态非易失性存储的内存卡（memory card）规范，定义了诸如卡的形态、尺寸、容量、电气信号、和主机之间的通信协议等方方面面的内容。\n1997年，MMC规范正式发布，至今已经进化出了SD、MicroSD、SDIO、EMMC等多种不同的规范，虽然眼花缭乱，但是追其根源，都源于MMC规范，所以Linux Kernel可以将其统一管理！\n MMC：强调的是多媒体存储（MM：MultiMedia）\nSD：强调的是安全数据（SD：Secure Digital）\nSDIO：强调的是IO接口(IO：Input/Output)\n   更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  3、总线接口 #  MMC、SD、SDIO其物理接口也十分相似，我们以MMC为例进行分析。\n我们的MMC卡如上图所示，内部我们不展开分析，直接将其作为一个完整的设备来分析。\n其通过CLK、CMD、DATA等管脚与我们的SOC通信，两者之间当然少不了Controller了。\n把通信总线部分，拿出来看：\n CLK：提供SOC和设备之间的通信时钟，常用的通信频率为400KHz（识卡）、25MHz，50MHz\nCMD：提供SOC和设备之间的通信命令，标识不同的命令编号，类型多达50多种。\nDATA：提供SOC和设备之间的数据通信，其通信总线有8根，可自定义设置，一般默认的是1-bit (默认)模式、4-bit模式和8-bit模式。当然数据线越多，传输越快嘛，但是处理起来也稍微繁琐。\n除了上面的一些管脚，当然还少不了VCC、GND等管脚喽，与其它外设不同的是，MMC类的设备，还会有一个检测引脚DET，用于检测是否存在卡设备（热插拔）。\n  好啦，上面我们对MMC、SD、SDIO进行简单了解，也知道了通信的常用方式与物理接口，当然其最核心在于通信的协议啦！由于协议过于复杂，我们放到后面了解。\n 4、参考文章 #  [1]：http://www.wowotech.net/basic_tech/mmc_sd_sdio_intro.html\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":2,"href":"/docs/linux/bluetooth/bluetooth%E8%93%9D%E7%89%99%E5%BC%80%E5%8F%91%E4%B8%80%E6%89%93%E9%80%A0%E5%85%A8%E7%BD%91%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84bluetooth%E5%BC%80%E5%8F%91%E6%95%99%E7%A8%8B/","title":"【Bluetooth蓝牙开发】一、打造全网最详细的Bluetooth开发教程","section":"Bluetooth蓝牙开发","content":"【Bluetooth|蓝牙开发】一、开篇词 | 打造全网最详细的Bluetooth开发教程 #  1、前言 #  大家好，我是董哥！\n随着物联网技术的快速发展，WiFi、蓝牙成了物联网通信主力军，但是不得不说，这两个技术的门槛还是比较高的，尤其是蓝牙，单单其官方文档，就有将近3000Page，让人望而生畏！\n纵观全网，蓝牙技术的学习有三大难点：\n 其一：学习资料之杂 其二：极少有系统学习蓝牙的文章 其三：蓝牙协议晦涩难懂  这样就导致了蓝牙学习成本之高，劝退人数之多。\n因此，为了让初学者快速上手并且掌握蓝牙开发相关技术，我也根据自己的开发经验，精心打造了这一期专栏，主要目的是可以帮助大家零基础入门蓝牙开发，并且可以创建一个交流平台，以供大家交流！\n本专栏从四个大方面来学习蓝牙技术：蓝牙基础知识，蓝牙协议栈，蓝牙调试方法，蓝牙应用。我也一定会尽最大努力，帮助大家快速敲开蓝牙开发的大门。\n2、蓝牙综合介绍 #  下面我们看一下零基础入门蓝牙开发的学习步骤，希望能够帮助到大家！！！\n3、精华文章汇总 #  ==为了方便大家快速找到文章，这里按照学习流程进行汇总，点击即可访问！==\n   章节 内容     1、开篇词 1. 文章总览   2、蓝牙开发入门 2.1 蓝牙基本概念    2.2 蓝牙发展历程    2.3 常见蓝牙架构   3. 蓝牙协议栈总览 2.1 从两个视角，了解蓝牙协议栈   4. 蓝牙协议栈——物理层 3.1 物理层的划分   5. 蓝牙协议栈——链路层 4.1 链路层状态、角色定义    4.2 空中接口数据包格式，字段分析   6. 蓝牙协议栈——传输层 5.1 HCI接口功能介绍    5.2 HCI层包的格式，字段分析   7. 蓝牙协议栈——L2CAP协议 6.1 L2CAP协议作用    6.2 L2CAP协议包的格式，字段分析   8. 蓝牙协议栈——ATT协议 7.1 ATT协议作用及由来    7.2 ATT数据结构    7.3 ATT协议的数据包格式，字段分析   9. 蓝牙协议栈——GATT协议 8.1 GATT的作用    8.2 GATT协议框架   10. 蓝牙通信流程分析 9.1 蓝牙初始化流程    9.2 蓝牙广播流程    9.3 蓝牙扫描流程    9.4 蓝牙建立连接流程    9.5 蓝牙数据交互流程    9.6 蓝牙断开流程   11. 蓝牙协议栈——Bluez交叉编译 10.1 Glib、Zlib、D-Bus、ncurses、readline、bluez   12. 蓝牙调试工具汇总 11.1 hciconfig    11.2 hcitool    11.3 hcidump    11.4 hciattach    11.5 btmon    11.6 bluetoothd    11.7 bluetoothctl    4、结语 #  以上，为目前Bluetooth所汇总的文章，每一篇都是精心打磨的文章，并且后续会继续补充蓝牙开发相关内容，期待大家关注！\n 同时，为了更好的交流与成长，后续可以建立蓝牙开发交流群，以供大家探讨相关技术！ 附：蓝牙协议下载地址   欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":3,"href":"/docs/linux/linux_led_subsystem/led%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90%E4%B8%80linux%E9%A9%B1%E5%8A%A8%E5%BC%80%E5%8F%91%E6%96%B0%E6%89%8B%E5%BF%85%E8%AF%BB/","title":"【LED子系统深度剖析】一、开篇词|Linux驱动开发新手必读","section":"Linux LED子系统","content":"【LED子系统深度剖析】一、开篇词|Linux驱动开发新手必读 #  1、前言 #  大家好，我是董哥！\n俗话说：“万丈高楼平地起”，对于我们刚学习Linux驱动开发的小伙伴，Linux驱动开发的基础至关重要，无论我们是学习51单片机、STM32还是ARM，点灯的地位还是毋庸置疑的。\n在Linux驱动开发的学习过程中，点灯对于大多数人来说，对着教程照葫芦画瓢，还是能快速点亮一颗LED灯的，但是你真的明白，一颗小小LED灯的背后，到底执行了哪些动作吗，Linux内核是如何管理的呢？\n今天，作为在芯片原厂工作的我，有义务带着大家，深入扒一扒LED子系统的工作原理！\n 总结系列文章，花费时间较长，希望大家尊重原创！\n 2、LED子系统开发详细介绍 #  该系列文章整体预览如下：\n3、LED子系统开发文章汇总 #  为了方便大家快速找到文章，这里按照学习流程进行汇总，点击即可访问！\n   章节 内容     1、开篇词 1. 文章总览   2、LED子系统框架分析 2.1 裸机处理    2.2 LED子系统框架    2.3 目录结构及核心文件   3、硬件驱动层详解 3.1 gpio_led_probe分析    3.2 gpio_leds_create分析    3.3 create_gpio_led分析    3.4 数据结构之间的关系，以及实现流程   4、核心层——led-class.c详解 4.1 leds_init分析    4.2 leds_class_dev_pm_ops分析    4.3 led_groups分析    4.4 led class的注册注销分析   5、核心层——led-core.c详解 5.1 led_init_core分析    5.2 led_timer_function分析    5.3 set_brightness_delayed分析    5.4 代码实现流程分析   6、核心层——led-triggers.c详解 6.1 触发器设置相关函数分析    6.2 触发器注册注销函数分析    6.3 闪烁功能相关函数分析    6.4 调用流程分析   7、触发器的实现 7.1 触发器介绍    7.2 heartbeat触发器的注册注销流程    7.3 heartbeat触发器相关定义和实现   8、LED子系统——小试牛刀 8.1 硬件管脚确定    8.2 设备树配置    8.3 子系统配置    8.4 编译烧录    8.5 验证   9、数据结构详解（番外篇） 9.1 核心数据结构图   10、详细实现流程汇总（番外篇） 10.1 LED驱动匹配    10.2 读写流程详解    4、结语 #  以上，为LED子系统深入探究的所有文章，每一篇都是精心打磨的文章，以此奉给那些刚开始学习Linux驱动开发的入门者，同时也期待大家多多关注，支持！\n当然，如果读者有更好的建议，也可以向我反馈，期待大家的支持！\n最后，我把我所有创作的付费系列文章，全部打包放到我的星球【嵌入式艺术】里面了，里面主要提供以下几个服务：\n 超有深度的技术好文 优质的嵌入式领域开发者基地 超详细的入门指南 读者问答系统  翻开嵌入式领域的神秘面纱，探索更深层次的技术奥秘，您是否梦寐以求？如果您想深入了解嵌入式领域，我的星球可能是一个非常有价值的资源平台。\n我们会邀请重磅嘉宾为大家提供更好的服务，并定期举办一些活动，能力出众的人还有机会免费加入。\n对于内容创作者，我的星球也是一个展示作品的好平台。希望我的星球能够一直为嵌入式爱好者提供更多更好的资源和服务，携手我们，各展所长，共创嵌入式领域的辉煌未来！\n 最后，前50名加入的人，享有最大力度优惠！巨轮已经起航，快来加入我们吧！——【嵌入式艺术】\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":4,"href":"/docs/linux/linux_kernel_lock/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3linux%E9%94%81%E6%9C%BA%E5%88%B6%E4%B8%80%E5%86%85%E6%A0%B8%E9%94%81%E7%9A%84%E7%94%B1%E6%9D%A5/","title":"【深入理解Linux锁机制】一、内核锁的由来","section":"Linux 内核锁详解","content":"【深入理解Linux锁机制】一、内核锁的由来 #  在Linux设备驱动中，我们必须要解决的一个问题是：多个进程对共享资源的并发访问，并发的访问会导致竞态。\n1、并发和竞态 #  并发（Concurrency）：指的是多个执行单元同时、并行的被执行。\n竞态（RaceConditions）：并发执行的单元对共享资源的访问，容易导致竞态。\n共享资源：硬件资源和软件上的全局变量、静态变量等。\n解决竞态的途径是：保证对共享资源的互斥访问。\n互斥访问：一个执行单元在访问共享资源的时候，其他执行单元被禁止访问。\n临界区（Critical Sections）：访问共享资源的代码区域成为临界区。临界区需要以某种互斥机制加以保护。\n常见的互斥机制包括：中断屏蔽，原子操作，自旋锁，信号量，互斥体等。\n 2、竞态发生的场合 #  2.1 多对称处理器（SMP）的多个CPU之间 #  多个CPU使用共同的系统总线，可以访问共同的外设和存储器。在SMP的情况下，多核（CPU0、CPU1）的竞态可能发生于：\n CPU0的进程和CPU1的进程之间 CPU0的进程和CPU1的中断之间 CPU0的中断和CPU1的中断之间   2.2 单CPU内，该进程与抢占它的进程之间 #  在单CPU内，多个进程并发执行，当一个进程执行的时间片耗尽，也有可能被另一个高优先级进程打断，会发生竞态，即所谓的调度引发竞态。\n 2.3 中断（软中断、硬中断、Tasklet、底半部）与进程之间 #  当一个进程正在执行，一个外部/内部中断（软中断、硬中断、Tasklet等）将其打断，会导致竞态发生。\n 3、编译乱序和执行乱序 #  除了并发访问导致的竞态外，还需要了解编译器和处理器的一些特点所引发的一些问题。\n3.1 编译乱序 #   现代的高性能编译器，为了提高Cache命中率以及CPU的Load/Store工作效率，会对目标代码进行乱序优化，减少逻辑上不必要的访存！\n因此，在打开编译器优化后，生成的汇编码并没有严格按照代码的逻辑顺序执行，这是正常的。\n 为了解决编译乱序的问题，可以加入barrier()编译屏障。\n顾名思义，编译屏障，也就是为了阻挡编译器的编译优化，加入barrier()编译屏障，即可保证正确的执行顺序。\n编译屏障代码实现如下：\n#define barrier() __asm__ __volatile__(\u0026#34;\u0026#34;: : :\u0026#34;memory\u0026#34;) 这里详细解释一下barrier的汇编实现：\n __asm__：向编译器说明在此插入汇编代码 __volatile__：用于告诉编译器，严禁将此处的汇编语句与其它的语句重组合优化。 (\u0026quot;\u0026quot;: : :\u0026quot;memory\u0026quot;)：一条汇编语句，第一个:前为汇编指令，这里是空操作；第二个:前表示输出操作数，为空；第三个冒号前为输入操作数，也是要修改的寄存器；最后memory表示该指令对内存进行访问，该指令确保了命令之前的内存操作需要完全执行，不被优化。   使用案例：\nint main(int argc,char *argv[]) { int a = 0,b,c,d[4096],e; e = d[4095]; barrier(); b = a; c = a; return 0; }  3.2 执行乱序 #  编译乱序是编译器的行为，而执行乱序就是处理器运行时的行为。\n**高级的CPU往往会根据自身的缓存特性，将访存指令重新排序执行！**这样就导致了多个顺序的指令，后发的指令仍有可能先执行完毕。\n 这种执行乱序，在多个CPU之间，以及单个CPU内部，都是非常常见的。\n  3.2.1 多CPU之间 #  处理器为了解决多核之间执行乱序的问题，一个CPU的行为对另一个CPU可见的情况，ARM处理器引入了内存屏障指令：\n DMB（数据内存屏障），保证在该指令前的所有指令，内存访问完成，再去访问该指令之后的访存动作 DSB（数据同步屏障），保证在该指令前的所有访存指令执行完毕（访存，缓存，跳转预测，TLB维护等）完成 ISB（指令同步屏障），Flush流水线，保证所有在ISB之后执行的指令都是从缓存或者内存中获得。   3.2.2 单CPU内部 #   在单CPU中，我们常遇到访问外设寄存器时，某些外设寄存器就对读写顺序有很高的要求，为了避免执行乱序的发生，这时候就需要CPU的一些内存屏障指令了。\n CPU内部，为了解决这种问题，CPU提供了一些内存屏障指令：\n 可以参考Documentation/memory-devices.txt和Documentation/io_ordering.txt\n  读写屏障：mb() 读屏障：rmb() 写屏障：wmb() 寄存器读屏障__iormb()__ 寄存器写屏障__iowmb()__  #define writeb_relaxed(v,c)\t__raw_writeb(v,c) #define writew_relaxed(v,c)\t__raw_writew((__force u16) cpu_to_le16(v),c) #define writel_relaxed(v,c)\t__raw_writel((__force u32) cpu_to_le32(v),c)  #define readb(c)\t({ u8 __v = readb_relaxed(c); __iormb(); __v; }) #define readw(c)\t({ u16 __v = readw_relaxed(c); __iormb(); __v; }) #define readl(c)\t({ u32 __v = readl_relaxed(c); __iormb(); __v; })  #define writeb(v,c)\t({ __iowmb(); writeb_relaxed(v,c); }) #define writew(v,c)\t({ __iowmb(); writew_relaxed(v,c); }) #define writel(v,c)\t({ __iowmb(); writel_relaxed(v,c); })  writel与writel_relaxed的区别就在于有无屏障。\n  4、总结 #  由上文可知，发生竞态的场合，主要发生在\n 多对称处理器的多CPU之间 单CPU的进程调度、抢占引发的竞态 单CPU的中断与进程之间引发的竞态 高性能的编译器编译乱序问题 高性能的CPU带来的执行乱序问题  为了解决竞态的发生，CPU和ARM处理器提供的内存屏障指令等，同时也提供了中断屏蔽、原子操作、自旋锁、互斥锁、信号量等机制，下面我们来深入了解这些机制吧。\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":5,"href":"/docs/linux/linux_nvmem_subsystem/nvmem%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E4%B8%80efuse%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%AE%89%E5%85%A8%E5%90%AF%E5%8A%A8%E6%B5%85%E6%9E%90/","title":"【NVMEM子系统深入剖析】一、Efuse介绍及安全启动浅析","section":"Linux NVMEM 子系统","content":"【NVMEM子系统深入剖析】一、Efuse介绍及安全启动浅析 #  1、Efuse是什么 #  eFuse(electronic fuse)：电子保险丝，熔丝性的一种器件，属于一次性可编程存储器。\n之所以成为eFuse，因为其原理像电子保险丝一样，CPU出厂后，这片eFuse空间内所有比特全为1，如果向一位比特写入0，那么就彻底烧死这个比特了，再也无法改变它的值，也就是再也回不去 1 了。\n 一般OEM从CPU厂商购买芯片后，一般都要烧写eFuse，用于标识自己公司的版本信息，运行模式等相关信息。\n同时，由于其一次性编程的特性，我们又将其用在Secure Boot安全启动中。\n  2、OTP是什么 #   了解完eFuse后，我们就顺便了解一下OTP\n OTP(One Time Programmable)是反熔丝的一种器件，就是说，当OTP存储单元未击穿时，它的逻辑状态为0；当击穿时，它的逻辑状态为1，也属于一次性可编程存储器。\n它的物理状态和逻辑状态正好和eFuse相反！\n两者区别如下：\n  从成本上讲，eFuse器件基本上是各个Foundry厂自己提供，因此通常意味着免费或者很少的费用，而OTP器件则通常是第三方IP厂家提供，这就要收费。\n  从器件面积上讲，eFuse的cell的面积更大，所以仅仅有小容量的器件可以考虑。当然如果需要大容量的，也可以多个eFuse Macro拼接，但是这意味着芯片面积的增加，成本也会增加；OTP的cell面积很小，所有相对来讲，可以提供更大容量的Macro可供使用。\n  OTP 比 eFuse 安全性更好，eFuse的编程位可以通过电子显微镜看到，因此其存储的内容可以被轻易破解，但OTP在显微镜下无法区分编程位和未编程位，因此无法读取数据。\n  eFuse默认导通，存储的是\u0026quot;1\u0026quot;，而OTP默认是断开，存储的是\u0026quot;0\u0026quot;，因此OTP的功耗也较eFuse小，面积也较eFuse小。\n   3、什么是Secure Boot #   上面我们也了解过了，efuse主要用于记录一些OEM的产品信息，并且也会用于安全启动，那么安全启动是什么，为什么要做安全启动？\n 安全启动Secure Boot，其主要目的是：以限制消费者能力，防止消费者从软硬件层面，对产品的部分关键系统进行读写，调试等高级权限，达到对产品的商业保密，知识产权的保护。\n安全启动的安全模型是建立在消费者是攻击者的假设之上，一般常见的操作有：\n 刷机安装自定义的操作系统 绕过厂家封闭的支付平台 绕过系统保护，复制厂家保护的数字产品。  除此之外呢，有的比较专业的消费者，还可以：\n 使用数字示波器监听 CPU 和 RAM 、eMMC 之间的数据传输来读取非常底层的数据传输 而且像 eMMC 这种芯片通常都是业界标准化的，攻击者甚至可以把芯片拆下来，然后用市面上现成的通用 eMMC 编程工具来读写上面的内容。  安全启动等级也有一个上限：这个上限通常是认为攻击者不至于能够剥离芯片的封装，然后用电子显微镜等纳米级别精度的显像设备来逆向芯片的内部结构。\n简单来说：能成功攻破芯片安全机制的一次性投资成本至少需要在十万美元以上才可以认为是安全的。\n 4、CPU内部安全机制 #  4.1 bootROM #  BootROM是集成在CPU芯片的一个ROM空间，其主要用于存放一小段可执行程序，出厂的时候被烧录进去写死，不可修改。\nCPU在通电之后，执行的第一条程序就在BootROM，用于初始化Secure Boot安全机制，加载Secure Boot Key密钥，从 存储介质中加载并验证 First Stage Bootloader（FSBL）；最后跳转进 FSBL 中。\n 4.2 iRAM #  为了避免使用外部的RAM，支持Secure Boot的CPU都会内置一块很小的RAM，通常只有 16KB 到 64KB ，我们称之为 iRAM。\n这块 iRAM 上的空间非常宝贵，bootROM 一般会用 4KB 的 iRAM 作为它的堆栈。FSBL 也会被直接加载到 iRAM 上执行。\n 4.3 eFUSE #  如上面所述，在Secure Boot中存放的是根密钥，用于安全启动的验证。\n  一般有两种根密钥：一个是加密解密用的对称密钥 Secure Boot Key，一般是 AES 128 的，每台设备都是随机生成不一样的；\n  另一个是一个 Secure Boot Signing Key 公钥，一般用的 RSA 或 ECC，这个是每个 OEM 自己生成的，每台设备用的都一样，有些芯片会存公钥的 Hash 来减少 eFUSE 的空间使用。\n   4.5 Security Engine #  有些 CPU 中还会有一个专门负责加密解密的模块，我们称为 Security Engine。这个模块通常会有若干个密钥槽（Keyslots），可以通过寄存器将密钥加载到任意一个 Keyslot 当中，通过寄存器操作 DMA 读写，可以使用 Keyslot 中的密钥对数据进行加密、解密、签名、HMAC、随机数生成等操作.\n 4.6 First Stage Bootloader（FSBL） #  FSBL 的作用是初始化 PCB 板上的其他硬件设备，给外部 RAM 映射内存空间，从 外部存储介质中加载验证并执行接下来的启动程序。\n 4.7 根信任建立 #    CPU上电后执行Boot ROM的程序，其这一小段程序用于初始化RAM，并加载Efuse上的内容，判断其所处的运行模式是不是生产模式。\n  如果在生产模式，开启Secure Boot功能，把Efuse上保存的Secure Boot Key加载到Security Engine加密模块中处理。\n  从外部存储介质中加载FSBL，FSBL里面会有一个数字签名和公钥证书，bootROM 会验证这个签名的合法性，以及根证书的 Hash 是否和 eFUSE 中的 Signing Key 的 Hash 相同。\n  如果验证通过，说明 FSBL 的的确确是 OEM 正式发布的，没有受到过篡改。\n  然后bootROM 就会跳转到 FSBL 执行接下来的启动程序。\n   5、参考文章 #  [1]：https://zhuanlan.zhihu.com/p/540171344\n[2]：https://blog.csdn.net/phenixyf/article/details/125675637\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":6,"href":"/docs/linux/linux_memory_manage/%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E7%94%B1%E6%9D%A5%E5%8F%8A%E6%80%9D%E6%83%B3/","title":"一、内存管理的由来及思想","section":"Linux 内存管理","content":"Linux内存管理 | 一、内存管理的由来及思想 #  1、前言 #  《中庸》有：“九层之台，起于垒土” 之说，那么对于我们搞技术的人，同样如此！\n对于Linux内存管理，你可以说没有留意过，但是它存在于我们日常开发的方方面面，你所打开的文件，你所创建的变量，你所运行的程序，无不以此为基础，它可以说是操作系统的基石；只是底层被封装的太好了，以至于我们在做开发的过程中，不需要关心的太多，哪有什么岁月静好，只是有人在负重前行罢了。\n 虽然日常开发中涉及的比较少，但是作为一个合格的Linux开发者，搞懂内存管理，又显得至关重要，同时也会对嵌入式开发大有脾益，今天我们就来详细聊聊内存管理的那点事。\n 该方面的文章，网上也有很多写的非常不错，但是100个人有100种理解方式，并且不同的人，基础不同，理解能力也不同，所以我写这系列的文章，也更有了意义。\n 2、内存管理的由来 #   为什么要有这个概念呢？\n  首先，内存管理，管理的是个什么东西？  管理的其实是我们的物理内存，也就是我们的RAM空间，在电脑上，表现为我们安装的内存条，有的人装个4G的、8G的、甚至64G的，这些就是实打实的物理空间大小，也就是我们的实际的硬件资源。\n 为什么要进行管理？  做嵌入式的都知道，像我们刚开始玩的C51单片机、STM32单片机，我们将程序烧录到Flash中后，开机启动后，然后CPU会将Flash程序加载到RAM中，也就是我们的物理内存，随后我们的所有操作都是基于这一个物理内存所进行的。\n那么此时：\n 我们想再次运行一个一模一样的程序怎么办？ 即使运行了，那两个程序同时操作了同一个变量，值被错误修改了怎么办？  这些就是Linux内存管理要做的事情。\n  顺便介绍一下 我的圈子：高级工程师聚集地，期待大家的加入。\n 3、Linux内存管理思想 #  为了解决上面的一些问题，Linux采用虚拟内存管理技术。\n Linux操作系统抽象出来一个虚拟地址空间的概念，供上层用户使用，这么做的目的是为了让多个用户进程，都以为自己独享了内存空间。 而虚拟地址空间与物理地址空间的对应关系，就交给了一个MMU(Memory Managerment Unit)的家伙来管理，其主要负责将虚拟内存空间映射到真实的物理地址空间。  这么做的主要目的在于：\n 让每个进程都拥有相同大小的虚拟地址空间 避免用户直接访问物理内存，导致系统崩溃  这样，我们同时执行多个进程，虽然看起来虚拟地址操作都是相同的，但是通过MMU之后，就被映射到了不同的物理地址空间，这样就解决了以上的问题。\n 4、总结 #  熟悉了内存管理由来以及其思想，我们可以看出，操作系统的内存管理，主要分为以下几个方面：\n 虚拟内存空间管理：我们抽象出来的虚拟地址空间，该怎么使用，该怎么管理？ 物理内存空间管理：虚拟地址映射到物理内存空间后，该如何管理，如何分配？ 如何映射：虚拟内存如何映射到物理内存，是怎么操作的，映射方法有哪些？  下面我们来一一详细探究。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":7,"href":"/docs/linux/linux_debug/%E4%B8%80%E6%96%87%E7%A7%92%E6%87%82ftrace%E7%B3%BB%E7%BB%9F%E8%B0%83%E8%AF%95%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E7%BB%88%E6%9E%81%E6%8C%87%E5%8D%97/","title":"【一文秒懂】Ftrace系统调试工具使用终极指南","section":"Linux 调试工具","content":"【一文秒懂】Ftrace系统调试工具使用终极指南 #  1、Ftrace是什么 #  Ftrace是Function Trace的简写，由 Steven Rostedt 开发的，从 2008 年发布的内核 2.6.27 中开始就内置了。\nFtrace是一个系统内部提供的追踪工具，旨在帮助内核设计和开发人员去追踪系统内部的函数调用流程。\n随着Ftrace的不断完善，除了追踪函数调用流程外，还可以用来调试和分析系统的延迟和性能问题，并发展成为一个追踪类调试工具的框架。\n除了Ftrace外，追踪类调试工具还包括：\n2、Ftrace的实现原理 #  为了帮助我们更好的使用Ftrace，我们有必要简单了解Ftrace的实现原理。\n2.1 Ftrace框架图 #  Ftrace的框架图如下：\n由框架图我们可以知道：\n ftrace包括多种类型的tracers，每个tracer完成不同的功能 将这些不同类型的tracers注册进入ftrace framework 各类tracers收集不同的信息，并放入到Ring buffer缓冲区以供调用。   2.2 Ftrace是如何记录信息的 #  Ftrace采用了静态插桩和动态插桩两种方式来实现。\n静态插桩：\n我们在Kernel中打开了CONFIG_FUNCTION_TRACER功能后，会增加一个-pg的一个编译选项，这个编译选项的作用就是为每个函数入口处，都会插入bl mcount跳转指令，使得每个函数运行时都会进入mcount函数。\n Ftrace一旦使能，对kernel中所有的函数插桩，这带来的性能开销是惊人的，有可能导致人们弃用Ftrace功能。\n 为了解决这个问题，开发者推出了Dynamic ftrace，以此来优化整体的性能。\n动态插桩：\n 这里的动态，是指的动态修改函数指令。\n  编译时，记录所有被添加跳转指令的函数，这里表示所有支持追踪的函数。 内核将所有跳转指令替换为nop指令，以实现非调试状态性能零损失。 根据 function tracer 设置，动态将被调试函数的nop指令，替换为跳转指令，以实现追踪。   总而言之，Ftrace记录数据可以总结为以下几个步骤：\n 打开编译选项-pg，为每个函数都增加跳转指令 记录这些可追踪的函数，并为了减少性能消耗，将跳转函数替换为nop指令 通过flag标志位来动态管理，将需要追踪的函数预留的nop指令替换回追踪指令，记录调试信息。   3、如何使用Ftrace #  3.1 配置详解 #  CONFIG_FTRACE=y # 启用了 Ftrace CONFIG_FUNCTION_TRACER=y\t# 启用函数级别的追踪器 CONFIG_HAVE_FUNCTION_GRAPH_TRACER=y\t# 表示内核支持图形显示 CONFIG_FUNCTION_GRAPH_TRACER=y\t# 以图形的方式显示函数追踪过程 CONFIG_STACK_TRACER=y\t# 启用堆栈追踪器，用于跟踪内核函数调用的堆栈信息。 CONFIG_DYNAMIC_FTRACE=y\t# 启用动态 Ftrace，允许在运行时启用和禁用 Ftrace 功能。 CONFIG_HAVE_FTRACE_NMI_ENTER=y\t# 表示内核支持非屏蔽中断（NMI）时进入 Ftrace 的功能 CONFIG_HAVE_FTRACE_MCOUNT_RECORD=y\t# 表示内核支持通过 mcount 记录函数调用关系。 CONFIG_FTRACE_NMI_ENTER=y # 表示内核支持通过 mcount 记录函数调用关系。  CONFIG_FTRACE_SYSCALLS=y\t# 系统调用的追踪 CONFIG_FTRACE_MCOUNT_RECORD=y\t# 启用 mcount 记录函数调用关系。 CONFIG_SCHED_TRACER=y\t# 支持调度追踪 CONFIG_FUNCTION_PROFILER=y\t# 启用函数分析器，主要用于记录函数的执行时间和调用次数 CONFIG_DEBUG_FS=y\t# 启用 Debug 文件系统支持  上面只是介绍了部分配置，更多详细配置可自行了解。\n并且上述配置不一定全部打开，勾选自己需要的即可，通常我们选择CONFIG_FUNCTION_TRACER和CONFIG_HAVE_FUNCTION_GRAPH_TRACER即可，然后编译烧录到开发板。\n  3.2 挂载debugfs文件系统 #  Ftrace是基于debugfs调试文件系统的，所以我们的第一步就是先挂载debugfs。\nmount -t debugfs none /sys/kernel/debug 此时我们能够在/sys/kernel/debug下看到内核支持的所有的调试信息了。\n# cd /sys/kernel/debug/ # ls asoc gpio regmap bdi ieee80211 sched_debug block memblock sched_features clk mmc0 sleep_time device_component mmc1 suspend_stats devices_deferred mtd tracing dma_buf opp ubi extfrag pinctrl ubifs fault_around_bytes pm_qos wakeup_sources  3.3 traceing目录介绍 #  在/sys/kernel/debug目录下，包含的是kernel所有的调试信息，本章只关注与tracing目录，下面挑选一些比较重要的属性文件来分析。\n  万变不离其宗，如此复杂的框架，设计人员已经提供了README文件，里面详解了各个属性文件的含义，我建议抛弃本文，看README吧:)\n 3.3.1 trace #  trace ：包含当前追踪的内容，以人类可读的格式展现，通过echo \u0026gt; trace来清除。\n 3.3.2 trace_pipe #  trace_pipe 和 trace 一样，都是记录当前的追踪内容，但它和 trace 不一样的是：\n 对 trace_pipe 的读操作将会阻塞，直到有新的追踪数据进来为止； 当前从trace_pipe 读取的内容将被消耗掉，再次读 trace_pipe 又会阻塞到新数据进来为止。   简单的来说，cat trace_pipe是堵塞读取，有数据就读，没数据就等待；而cat trace有没有数据都是直接返回的\n  3.3.3 tracing_on #  tracing_on：向 tracing_on 写入 1，启用追踪；向 tracing_on 写入 0，停止追踪。\n 追踪使用 ring buffer 记录追踪数据。修改 tracing_on 不会影响 ring buffer 当前记录的内容。\n  3.3.4 current_tracer #  current_tracer 表示当前启用的 tracer ，默认为 nop ，即不做任何追踪工作：\n# cat current_tracer nop  3.3.5 available_filter_functions #  available_filter_functions：可以被追踪的函数列表，即可以写到 set_ftrace_filter，set_ftrace_notrace，set_graph_function，set_graph_notrace 文件的函数列表。\n 3.3.6 available_tracers #  available_tracers 文件中包含的是当前编译到内核的 tracer 列表，也表示当前内核支持的tracer列表。\n该列表的内容，就是可以写到 current_tracer 的 tracer 名。\n# cat available_tracers function_graph function nop  nop：表示为空，不追踪 function：追踪函数调用 function_graph：以图形形式追踪函数调用   3.3.7 buffer_size_kb #  buffer_size_kb 记录 CPU buffer 的大小，单位为 KB 。\nper_cpu/cpuX/buffer_size_kb 记录 每个CPU buffer 大小，单位为 KB 。可通过写 buffer_size_kb 来改变 CPU buffer 的大小。\n 3.3.8 buffer_total_size_kb #  buffer_total_size_kb 记录所有 CPU buffer 的总大小，即所有 CPU buffer 大小总和。\n 如有 128 个 CPU buffer ，每个大小 7KB，则 buffer_total_size_kb 记录的总大小为 128 * 7KB = 896。\n buffer_total_size_kb 文件是只读的。\n 3.3.9 set_ftrace_filter #  set_ftrace_filter ：过滤函数追踪，仅仅追踪写入该文件的函数名。\n可填入的参数，可以通过available_filter_functions文件查看当前支持的函数名。\n该过滤功能，也有很多其他变体，如追踪某个模块的函数调用等。\n 官方给的示例：\n Format: :mod:\u0026lt;module-name\u0026gt; example: echo :mod:ext3 \u0026gt; set_ftrace_filter\t# 该模块必须是已经加载进去的模块  3.3.10 set_ftrace_notrace #  set_ftrace_notrace：和 set_ftrace_filter 刚好相反，系统禁用对其中列举函数的追踪。\n 3.3.11 set_ftrace_pid #  系统对 set_ftrace_pid 文件中指定的 PID进程进行追踪。\n如果开启了 options/function-fork 选项，fork 的子进程的 PID 也会自动加入文件，同时该选项也会引起系统自动将退出进程的 PID 从文件中移除。\n 3.3.12 set_graph_function #  此文件中列出的函数将导致函数图跟踪器仅跟踪这些函数以及它们调用的函数。\n但是该跟踪的记录，仍然受set_ftrace_filter 和 set_ftrace_notrace 的影响。\n 3.3.12 set_graph_notrace #  与 set_graph_function 类似，但当函数被命中时，将禁用函数图跟踪，直到退出函数。\n 3.4 简单使用示例 #   一般我们挂载上debugfs后，tracing_on是处于打开状态的。\n 3.4.1 函数追踪 #   3.4.2 追踪图形显示 #   3.4.3 动态过滤追踪 #   3.4.4 重置追踪 #  echo 0 \u0026gt; tracing_on\t# 关闭trace echo \u0026gt; trace\t# 清空当前trace记录 cat available_tracers # 查看当前支持的追踪类型 echo function_graph \u0026gt; current_tracer # 设置当前的追踪类型 echo 1 \u0026gt; tracing_on\t# 开启追踪 cat trace\t# 查看追踪结果  4、进阶用法 #  上述章节，只是介绍了Ftrace最基本的命令，下面来看一下Ftrace在具体问题中的用法！\n4.1 追踪任意命令 #   如何追踪我们执行的命令呢？\n Ftrace支持追踪特定进程，通过set_ftrace_pid属性来设置指定进程。然后在该进程中，执行特定的命令。\n首先我们需要设置好我们的追踪器\nmount -t debugfs none /sys/kernel/debug cd /sys/kernel/debug/tracing echo 0 \u0026gt; tracing_on\t# 关闭追踪器 echo function \u0026gt; current_tracer\t# 设置当前追踪类别 在我们设置好追踪器后，使用如下命令，即可追踪我们执行的命令your_command\necho \u0026gt; trace; echo $$ \u0026gt; set_ftrace_pid; echo 1 \u0026gt; tracing_on; your_command; echo 0 \u0026gt; tracing_on  4.2 追踪指定函数的调用流程 #  跟踪函数的时候，设置 echo 1 \u0026gt; options/func_stack_trace 即可在 trace 结果中获取追踪函数的调用栈。\nmount -t debugfs none /sys/kernel/debug cd /sys/kernel/debug/tracing echo 0 \u0026gt; tracing_on\t# 关闭追踪器 cat available_filter_functions | grep \u0026#34;xxxxxx\u0026#34;\t# 搜索函数是否存在 echo xxxxxx \u0026gt; set_ftrace_filter\t# 设定追踪的函数 echo function \u0026gt; current_tracer\t# 设置当前追踪类别 echo 1 \u0026gt; options/func_stack_trace\t# 记录堆栈信息 echo \u0026gt; trace\t# 清空缓存 echo 1 \u0026gt; tracing_on\t# 开始追踪 效果如下：\n# cat trace # tracer: function # # entries-in-buffer/entries-written: 2/2 #P:3 # # _-----=\u0026gt; irqs-off # / _----=\u0026gt; need-resched # | / _---=\u0026gt; hardirq/softirq # || / _--=\u0026gt; preempt-depth # ||| / delay # TASK-PID CPU# |||| TIMESTAMP FUNCTION # | | | |||| | | kworker/1:1-59 [001] .... 168.954199: mmc_rescan \u0026lt;-process_one_work kworker/1:1-59 [001] .... 168.954248: \u0026lt;stack trace\u0026gt; =\u0026gt; mmc_rescan =\u0026gt; process_one_work =\u0026gt; worker_thread =\u0026gt; kthread =\u0026gt; ret_from_fork =\u0026gt; 0  4.3 追踪指定模块的所有函数 #  要想我们的ko文件能够被Ftrace记录到，我们需要在编译模块的时候，加上编译参数-pg，这点很重要，否则你在available_filter_functions列表中，查找不到你想要的函数。\n然后，需要我们设置过滤器，设置方法有以下几种：\n 按模块直接过滤：  # 示例 Format: :mod:\u0026lt;module-name\u0026gt; example: echo :mod:ext3 \u0026gt; set_ftrace_filter  追踪ext3模块内的所有函数\n   按函数直接过滤   如果该模块内的函数，命名都有一定的规则，可以按照正则表达式来过滤\n # 示例 echo \u0026#34;mmc*\u0026#34; \u0026gt; set_ftrace_filter  过滤包含mmc字符的所有函数\n   按照函数差异来过滤  如果函数命名没有规律，又想过滤该模块所有函数，该怎么办？\n按照加载模块前后的函数差异，写入到文件中来过滤\ncat available_filter_functions \u0026gt; /tmp/1.txt cat available_filter_functions \u0026gt; /tmp/2.txt diff /tmp/1.txt /tmp/2.txt \u0026gt; /tmp/3.txt cat /tmp/3.txt | sed \u0026#39;s/^+//\u0026#39; | awk \u0026#39;{print $1}\u0026#39;\t# 如果diff出来格式前带有+-号，需要手动去掉 cat /tmp/3.txt \u0026gt; set_ftrace_filter  5、自动化管理 #  Ftrace功能很强大，在内核层面我们通过echo和cat即可获取我们想要的所有信息，但是通过一次一次敲命令显得有些繁琐，自己也对常用的功能整合了一个自动化脚本，能够通过命令行，直接追踪特定模块、函数、命令，极大提高了调试效率。\n自动化脚本获取路径：common_trace.sh\n# /root/common_trace.sh  Usage: /root/common_trace.sh {module|funcs|funcs_stack|command|clear} /root/common_trace.sh module ext4 /root/common_trace.sh funcs sysfs /root/common_trace.sh funcs_stack sysfs /root/common_trace.sh command sysfs [functions] /root/common_trace.sh clear 脚本主要实现的功能有：\n 追踪指定模块，查看所有调用流程 追踪指定函数，查看该函数的调用链 追踪指定函数，获取堆栈信息 追踪用户命令，查看所有调用流程，并可选择指定函数来查看调用流程。   脚本除了command功能外，其他功能都需要手动调用common_trace.sh clear来停止追踪。\n  6、总结 #  以上，介绍了Ftrace的由来，实现原理，以及如何使用Ftrace，并最终提供了自动化测试脚本，希望对大家有所帮助。\n 欢迎关注【嵌入式艺术】，董哥原创！\r\r"},{"id":8,"href":"/docs/linux/linux_api/linux-api-%E6%8F%AD%E7%A7%98module_init%E4%B8%8Emodule_exit/","title":"【Linux API 揭秘】module_init与module_exit","section":"Linux API 揭秘","content":"【Linux API 揭秘】module_init与module_exit #   Linux Version：6.6\nAuthor：Donge\nGithub：linux-api-insides\n  1、函数作用 #  module_init和module_exit是驱动中最常用的两个接口，主要用来注册、注销设备驱动程序。\n并且这两个接口的实现机制是一样的，我们先以module_init为切入点分析。\n 2、module_init函数解析 #  2.1 module_init #  #ifndef MODULE /** * module_init() - driver initialization entry point * @x: function to be run at kernel boot time or module insertion * * module_init() will either be called during do_initcalls() (if * builtin) or at module insertion time (if a module). There can only * be one per module. */ #define module_init(x)\t__initcall(x);  ...... #else /* MODULE */ ...... /* Each module must use one module_init(). */ #define module_init(initfn)\t\\ static inline initcall_t __maybe_unused __inittest(void)\t\\ { return initfn; }\t\\ int init_module(void) __copy(initfn)\t\\ __attribute__((alias(#initfn)));\t\\ ___ADDRESSABLE(init_module, __initdata);  ...... #endif 函数名称：module_init\n文件位置：include/linux/module.h\n函数解析：\n 在Linux内核中，驱动程序可以以两种方式存在：内建(Builtin)和模块(Module)。内建驱动就是在编译时，直接编译进内核镜像中；而模块驱动则是在内核运行过程中动态加载卸载的。\n module_init函数的定义位置有两处，使用MODULE宏作为判断依据。MODULE是一个预处理器宏，仅当该驱动作为模块驱动时，编译的时候会加入MODULE的定义。\n 这里难免会有疑问：为什么会有两套实现呢？\n 其实，当模块被编译进内核时，代码是存放在内存的.init字段，该字段在内核代码初始化后，就会被释放掉了，所以当可动态加载模块需要加载时，就需要重新定义了。\n 2.1.1 模块方式 #  当驱动作为可加载模块时，MODULE宏被定义，我们简单分析一下相关代码\n#define module_init(initfn)\t\\ static inline initcall_t __maybe_unused __inittest(void)\t\\ { return initfn; }\t\\ int init_module(void) __copy(initfn)\t\\ __attribute__((alias(#initfn)));\t\\ ___ADDRESSABLE(init_module, __initdata);  static inline initcall_t __maybe_unused __inittest(void) { return initfn; }：一个内联函数，返回传入的initfn函数。  __maybe_unused ：编译器指令，用于告诉编译器，该函数可能不会使用，以避免编译器产生警告信息。   int init_module(void) __copy(initfn) __attribute__((alias(#initfn)));：init_module函数的声明  __copy(initfn)：编译器指令，也就是将我们的initfn函数代码复制到init_module中， __attribute__((alias(#initfn)))：编译器指令，将init_module函数符号的别名设置为initfn。   ___ADDRESSABLE(init_module, __initdata);：一个宏定义，主要用于将init_module函数的地址放入__initdata段，这样，当模块被加载时，init_module函数的地址就可以被找到并调用。  总的来说，如果是可加载的ko模块，module_init宏主要定义了init_module函数，并且将该函数与initfn函数关联起来，使得当模块被加载时，初始化函数可以被正确地调用。\n 2.1.2 内建方式 #  当模块编译进内核时，MODULE宏未被定义，所以走下面流程\n#define module_init(x)\t__initcall(x);  2.2 __initcall #  #define __initcall(fn) device_initcall(fn)  #define device_initcall(fn)\t__define_initcall(fn, 6)  #define __define_initcall(fn, id) ___define_initcall(fn, id, .initcall##id)  #define ___define_initcall(fn, id, __sec)\t\\ __unique_initcall(fn, id, __sec, __initcall_id(fn))  #define __unique_initcall(fn, id, __sec, __iid)\t\\ ____define_initcall(fn,\t\\ __initcall_stub(fn, __iid, id),\t\\ __initcall_name(initcall, __iid, id),\t\\ __initcall_section(__sec, __iid))  #define ____define_initcall(fn, __unused, __name, __sec)\t\\ static initcall_t __name __used \\ __attribute__((__section__(__sec))) = fn;  #define __initcall_stub(fn, __iid, id)\tfn  /* Format: \u0026lt;modname\u0026gt;__\u0026lt;counter\u0026gt;_\u0026lt;line\u0026gt;_\u0026lt;fn\u0026gt; */ #define __initcall_id(fn)\t\\ __PASTE(__KBUILD_MODNAME,\t\\ __PASTE(__,\t\\ __PASTE(__COUNTER__,\t\\ __PASTE(_,\t\\ __PASTE(__LINE__,\t\\ __PASTE(_, fn))))))  /* Format: __\u0026lt;prefix\u0026gt;__\u0026lt;iid\u0026gt;\u0026lt;id\u0026gt; */ #define __initcall_name(prefix, __iid, id)\t\\ __PASTE(__,\t\\ __PASTE(prefix,\t\\ __PASTE(__,\t\\ __PASTE(__iid, id))))  #define __initcall_section(__sec, __iid)\t\\ #__sec \u0026#34;.init\u0026#34;  /* Indirect macros required for expanded argument pasting, eg. __LINE__. */ #define ___PASTE(a,b) a##b #define __PASTE(a,b) ___PASTE(a,b) 函数名称：__initcall\n文件位置：include/linux/init.h\n函数解析：设备驱动初始化函数\n 2.2.1 代码调用流程 #  module_init(fn) |--\u0026gt; __initcall(fn) |--\u0026gt; device_initcall(fn) |--\u0026gt; __define_initcall(fn, 6) |--\u0026gt; ___define_initcall(fn, id, __sec) |--\u0026gt; __initcall_id(fn) |--\u0026gt; __unique_initcall(fn, id, __sec, __iid) |--\u0026gt; ____define_initcall(fn, __unused, __name, __sec) |--\u0026gt; __initcall_stub(fn, __iid, id) |--\u0026gt; __initcall_name(prefix, __iid, id) |--\u0026gt; __initcall_section(__sec, __iid) |--\u0026gt; ____define_initcall(fn, __unused, __name, __sec)   进行函数分析前，我们先要明白#和##的概念\n 2.2.2 #和##的作用 #     符号 作用 举例     ## ##符号 可以是连接的意思 例如 __initcall_##fn##id 为__initcall_fnid那么，fn = test_init，id = 6时，__initcall##fn##id 为 __initcall_test_init6   # #符号 可以是字符串化的意思 例如 #id 为 \u0026quot;id\u0026quot;，id=6 时，#id 为\u0026quot;6\u0026quot;      更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  2.2.3 函数解析 #   下面分析理解比较有难度的函数\n #define device_initcall(fn)\t__define_initcall(fn, 6) #define __define_initcall(fn, id) ___define_initcall(fn, id, .initcall##id)  .initcall##id：通过##来拼接两个字符串：.initcall6  #define ___define_initcall(fn, id, __sec)\t\\ __unique_initcall(fn, id, __sec, __initcall_id(fn))  /* Format: \u0026lt;modname\u0026gt;__\u0026lt;counter\u0026gt;_\u0026lt;line\u0026gt;_\u0026lt;fn\u0026gt; */ #define __initcall_id(fn)\t\\ __PASTE(__KBUILD_MODNAME,\t\\ __PASTE(__,\t\\ __PASTE(__COUNTER__,\t\\ __PASTE(_,\t\\ __PASTE(__LINE__,\t\\ __PASTE(_, fn))))))  /* Indirect macros required for expanded argument pasting, eg. __LINE__. */ #define ___PASTE(a,b) a##b #define __PASTE(a,b) ___PASTE(a,b)  ___PASTE：拼接两个字符串 __initcall_id：它用于生成一个唯一的标识符，这个标识符用于标记初始化函数。  __KBUILD_MODNAME：当前正在编译的模块的名称 __COUNTER__：一个每次使用都会递增计数器，用于确保生成名称的唯一性 __LINE__：当前代码的行号     #define __unique_initcall(fn, id, __sec, __iid)\t\\ ____define_initcall(fn,\t\\ __initcall_stub(fn, __iid, id),\t\\ __initcall_name(initcall, __iid, id),\t\\ __initcall_section(__sec, __iid))  #define ____define_initcall(fn, __unused, __name, __sec)\t\\ static initcall_t __name __used \\ __attribute__((__section__(__sec))) = fn;  #define __initcall_stub(fn, __iid, id)\tfn  /* Format: __\u0026lt;prefix\u0026gt;__\u0026lt;iid\u0026gt;\u0026lt;id\u0026gt; */ #define __initcall_name(prefix, __iid, id)\t\\ __PASTE(__,\t\\ __PASTE(prefix,\t\\ __PASTE(__,\t\\ __PASTE(__iid, id))))  #define __initcall_section(__sec, __iid)\t\\ #__sec \u0026#34;.init\u0026#34; __unique_initcall：调用____define_initcall，关键实现部分\n____define_initcall：定义一个名为 __name 的 initcall_t 类型的静态变量，并将其初始化为 fn，并放入特定的__sec段中。\n __initcall_stub：表示唯一的函数名fn __initcall_name：表示一个唯一的变量名 __initcall_section： 生成一个唯一的段名。 #__sec \u0026quot;.init\u0026quot;：将两个字符串拼接起来，比如：__sec=.initcall6，拼接后的段为：.initcall6.init，该段为最终存储的段。   字段通过链接器链接起来，形成一个列表进行统一管理。\n 这些字段我们可以在arch/arm/kernel/vmlinux.lds中查看。\n ...... __initcall6_start = .; KEEP(*(.initcall6.init)) KEEP(*(.initcall6s.init)) ......  3、module_exit函数解析 #   module_exit和module_init的实现机制几乎没有差别，下面就简单介绍一下。\n 3.1 module_exit #  #ifndef MODULE  /** * module_exit() - driver exit entry point * @x: function to be run when driver is removed * * module_exit() will wrap the driver clean-up code * with cleanup_module() when used with rmmod when * the driver is a module. If the driver is statically * compiled into the kernel, module_exit() has no effect. * There can only be one per module. */ #define module_exit(x)\t__exitcall(x);  ...... #else /* MODULE */ ...... /* This is only required if you want to be unloadable. */ #define module_exit(exitfn)\t\\ static inline exitcall_t __maybe_unused __exittest(void)\t\\ { return exitfn; }\t\\ void cleanup_module(void) __copy(exitfn)\t\\ __attribute__((alias(#exitfn)));\t\\ ___ADDRESSABLE(cleanup_module, __exitdata);  ...... #endif 函数名称：module_exit\n文件位置：include/linux/module.h\n3.1.1 模块方式 #  作为模块方式，与module_init的实现方式一样，定义cleanup_module与exitfn函数相关联，存放在__exitdata段内。\n 3.1.2 内建方式 #  当模块编译进内核时，MODULE宏未被定义，所以走下面流程\n#define module_exit(x)\t__exitcall(x);  3.2 __exitcall #  #define __exitcall(fn)\t\\ static exitcall_t __exitcall_##fn __exit_call = fn  #define __exit_call\t__used __section(\u0026#34;.exitcall.exit\u0026#34;) 函数名称：__initcall\n文件位置：include/linux/init.h\n函数解析：设备驱动卸载函数\n__exitcall_##fn：定义一个新的 exitcall_t 类型的静态变量，并赋值为fn\n__exit_call：__used __section(\u0026quot;.exitcall.exit\u0026quot;)，定义该函数存储的段\n 4、扩展 #   还记得__define_initcall的定义吗？\n #define pure_initcall(fn) __define_initcall(fn, 0)  #define core_initcall(fn) __define_initcall(fn, 1) #define core_initcall_sync(fn) __define_initcall(fn, 1s) #define postcore_initcall(fn) __define_initcall(fn, 2) #define postcore_initcall_sync(fn) __define_initcall(fn, 2s) #define arch_initcall(fn) __define_initcall(fn, 3) #define arch_initcall_sync(fn) __define_initcall(fn, 3s) #define subsys_initcall(fn) __define_initcall(fn, 4) #define subsys_initcall_sync(fn) __define_initcall(fn, 4s) #define fs_initcall(fn) __define_initcall(fn, 5) #define fs_initcall_sync(fn) __define_initcall(fn, 5s) #define rootfs_initcall(fn) __define_initcall(fn, rootfs) #define device_initcall(fn) __define_initcall(fn, 6) #define device_initcall_sync(fn) __define_initcall(fn, 6s) #define late_initcall(fn) __define_initcall(fn, 7) #define late_initcall_sync(fn) __define_initcall(fn, 7s)  #define __initcall(fn) device_initcall(fn) 不同的宏定义，被赋予了不同的调用等级，最后将不同的驱动初始化函数统一汇总到__initcallx_start字段统一管理，形成一个有序的列表。\n这样，我们在内核中，按照顺序遍历这个列表，最后执行对应的模块初始化函数fn即可实现驱动的初始化。\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":9,"href":"/docs/uboot/%E4%B8%80uboot%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3/","title":"一、uboot基础了解","section":"Uboot开发","content":"一、uboot基础了解 #  1. U-boot是什么 #  U-Boot，全称 Universal Boot Loader，是遵循GPL条款的从FADSROM、8xxROM、PPCBOOT逐步发展演化而来的 开放源码项目。\nU-boot，是一个主要用于嵌入式系统的引导加载程序，可以支持多种不同的计算机系统结构，其主要作用为：==引导系统的启动！==目前，U-Boot不仅支持Linux系统的引导，还支持NetBSD, VxWorks, QNX, RTEMS, ARTOS, LynxOS, android等多种嵌入式操作系统。\n2. U-boot主要特性及功能 #   开放：开放的源代码 多平台：支持多种嵌入式操作系统，如Linux、NetBSD、android等 生态：有丰富的设备驱动源码，如以太网、SDRAM、LCD等，同时也具有丰富的开发文档。  3. U-boot下载地址 #  Uboot开发源码：\n  https://source.denx.de/u-boot/u-boot\n  https://ftp.denx.de/pub/u-boot/\n  其他厂商定制的uboot源码：\n 野火  4. U-boot目录结构 #     目录 含义     arch 各个厂商的硬件信息，目录下包括支持的处理器类型   arch/arm/cpu/xxx **每一个子文件夹，包含一种cpu系列。**每个子文件夹下包含cpu.c（CPU初始化），interrupts.c（设置中断和异常），start.S（U-boot的启动文件，早期的初始化）。   board 与开发板有关，每一个子文件夹代表一个芯片厂家，芯片厂家下，每一个子文件夹，表示一个开发板   common 存放与处理器体系无关的通用代码，可以说为通用核心代码！   cmd 存放uboot的相关命令实现部分   drivers 存放外围芯片驱动，网卡，USB等   disk 存放驱动磁盘的分区处理代码   fs 本目录下存放文件系统相关代码，每一个子文件夹表示文件系统   net 网络协议相关代码   doc uboot说明文档   include 各种头文件   post 上电自检代码   api 外部扩展程序的API和示例   tools 编译S-Record或者U-boot镜像的相关工具    5. 如何编译Uboot #  make ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- distclean make ARCH=arm CORSS_COMPILE=arm-linux-gnueabihf- colibri-imx6ull_defconfig make V=1 ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- -j8 ARCH=arm：arm架构\nCROSS_COMPILE：使用的交叉编译器\n 如果编译出错，your compile older 6.0，可以参考【1】\n colibri-imx6ull_defconfig：指定一个config文件，作为相关版型的配置信息\nV=1：这个选项能显示出编译过程中的详细信息，即是verbose编译模式\n-j8：多核并行编译，可以提高编译速度，受硬件限制\n6. U-boot工作模式 #   U-boot的工作模式有：启动加载模式和下载模式\n  启动加载模式：  启动加载模式，为Bootloader正常工作模式，一款开发板，正常上电后，Bootloader将嵌入式操作系统==从FLASH中加载到SDRAM中==运行。\n 下载模式：  下载模式，就是Bootloader通过通信，将内核镜像、根文件系统镜像从PC机直接下载到目标板的FLASH中。\n7. U-boot的存放位置 #  嵌入式系统，一般使用Flash来作为启动设备，Flash上存储着U-boot、环境变量、内核映像、文件系统等。U-boot存放于Flash的起始地址，所在扇区由Soc规定。\n8. U-boot系列文章汇总 #   下面是进行U-boot开发期间，感觉比较不错的资料，总结分享一下！\n [1] : Uboot官网、Uboot官方指南、官方指南2\n[2] : https://blog.51cto.com/u_9291927/category5\n[3] : https://blog.csdn.net/ooonebook/category_6484145.html\n[4]：https://blog.csdn.net/qq_36310253/category_9332618.html\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":10,"href":"/docs/embeded_tech/embeded_interview/soc%E7%9A%84bringup%E6%B5%81%E7%A8%8B/","title":"Soc的Bring Up流程","section":"嵌入式面经","content":"1、Bring Up流程 #  SOC (System on a Chip) bring-up是一个复杂的过程，涉及到硬件、固件和软件的集成和验证，以下是一个基于BROM，SPL，UBOOT和Linux的启动流程的概述：\n BROM (Boot Read-Only Memory)启动：启动的最初阶段，在这个阶段，系统会执行芯片ROM里面的代码，这部分代码主要用来检查启动模式，包括NOR、Nand、Emmc等，然后从对应的存储介质中加载SPL(Secondary Program Loader)代码。 SPL (Secondary Program Loader)启动：SPL属于Uboot的一部分，它的主要作用就是：初始化硬件并加载完整的U-boot，主要体现在初始化时钟、看门狗、DDR、GPIO以及存储外设，最后将U-boot代码加载到DDR中执行。 U-Boot启动：U-boot的主要作用是：引导加载Kernel和DTS。U-boot在启动之后，同样初始化Soc硬件资源，然后会计时等待，并执行默认的启动命令，将Kernel和DTS信息从存储介质中读取出来并加载到内存中执行。 Kernel启动：在U-Boot加载了内核映像和设备树之后，系统会启动Linux。在这个阶段，系统会初始化各种硬件设备，加载驱动程序并启动用户空间应用程序。   更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  2、常见问题 #  Q：为什么上一个阶段已经初始化了硬件资源，下一个阶段为何重复初始化？\nA：\n  每个阶段的硬件初始化，其目标和需求都不同，硬件配置也会不一样，因此在不同阶段进行不同的初始化。\n  硬件状态可能会改变，在SOC启动过程中，硬件状态可能会因为电源管理、时钟管理等原因而改变，这可能需要在每个阶段都重新初始化以确保其正确工作\n  为了保证硬件资源的可靠性，最好每个阶段都重新初始化一次\n   Q：U-boot加载内核时，会进行重定位的操作，这一操作有何意义？\nA：\n U-boot的重定位，主要作用是为了 给内核提供一个连续的、大的内存空间，供内核和其他应用程序使用 U-boot的加载过程分两个阶段，即：SPL和U-boot，   在SPL阶段，主要将U-boot代码从Flash中加载到RAM指定位置 在U-boot阶段，U-boot会将自身从RAM的开始部分移动到RAM的末尾，占用高地址空间，从而让低地址空间可以作为一个连续的，大的内存空间供内核和其他应用程序使用。   Q：在Bring Up中，为了保证启动时间，如何裁剪？\nA：\n 启动时间的裁剪是一个重要的步骤，其主要目标是缩短从电源打开到操作系统完全启动的时间。\n  优化Bootloader：减小Bootloader的代码大小，减少硬件初始化（只初始化必要硬件设备）等 优化Kernel：减少启动服务数量，优化服务的启动顺序，使用预加载技术等方法来实现。 使用快速启动模式：一些SOC支持快速启动模式，这种模式下，SOC会跳过一些不必要的硬件初始化和自检过程，从而更快地启动。 使用休眠和唤醒技术：一些SOC还支持休眠和唤醒技术，这种技术可以将系统的状态保存到非易失性存储器中，然后关闭系统。当系统再次启动时，可以直接从非易失性存储器中恢复系统的状态，从而更快地启动。  \r\u0026nbsp;\r"},{"id":11,"href":"/docs/linux/linux_driver_develop_basic/%E4%B8%80%E6%96%87%E7%A7%92%E6%87%82linux%E8%AE%BE%E5%A4%87%E6%A0%91%E8%AF%A6%E8%A7%A3/","title":"【一文秒懂】Linux设备树详解","section":"Linux 驱动开发基础","content":"【一文秒懂】Linux设备树详解 #  1、Linux设备树概念 #  Linux内核是从V2.6开始引入设备树的概念，其起源于OF:OpenFirmware， 用于描述一个硬件平台的硬件资源信息，这些信息包括：CPU的数量和类别、内存基地址和大小、总线和桥、外设连接、中断控制器和中断使用情况、GPIO控制器和GPIO使用情况、Clock控制器和Clock使用情况等等。\n官方说明：\n The \u0026ldquo;Open Firmware Device Tree\u0026rdquo;, or simply Device Tree (DT), is a data structure and language for describing hardware.\n设备树是一种数据结构和一种用于描述硬件信息的语言。\n 设备树的特点：\n 实现驱动代码与设备硬件信息相分离。 通过被bootloader(uboot)和Linux传递到内核， 内核可以从设备树中获取对应的硬件信息。 对于同一SOC的不同主板，只需更换设备树文件即可实现不同主板的无差异支持，而无需更换内核文件，实现了内核和不同板级硬件数据的拆分。   2、设备树的由来 #  明白了设备树的概念，不妨思考一下：为什么要引入设备树？\n在Linux内核v2.6版本以前，ARM架构用于描述不同的硬件信息的文件都存放在arch/arm/plat-xxx和arch/arm/mach-xxx文件夹下，如下：\n在这些文件内，都是通过手动定义不同的硬件设备，步骤非常繁琐\n这样就导致了Linux内核代码中充斥着大量的垃圾代码，因为不同的板级他们的硬件信息都不相同，这些都是硬件特有的信息，对内核而言没有任何的意义，但是往往这部分代码特别的多，造成内核的冗余。\n设备树的引入就是为了解决这个问题，通过引入设备树，我们可以直接通过它来传递给Linux，而不再需要内核中大量的垃圾代码。\n 3、设备树组成 #   整个设备树牵涉面比较广，即增加了新的用于描述设备硬件信息的文本格式，又增加了编译这个文本的工具，同时还得支持Bootloader解析设备树，并将信息传递给内核。\n 整个设备树包含DTC（device tree compiler），DTS（device tree source）和DTB（device tree blob）。\n DTS（device tree source）  DTS是一种ASCII文本格式的设备树描述，在ARM Linux中，一个dts文件对应一个ARM的设备，该文件一般放在arch/arm/boot/dts/目录中。\n 当然，我们还会看到一些dtsi文件，这些文件有什么用呢？\nDtsi：由于一个SoC可能对应多个设备（一个SoC可以对应多个产品和电路板），这些.dts文件势必须包含许多共同的部分，Linux内核为了简化，把SoC公用的部分或者多个设备共同的部分一般提炼为.dtsi，类似于C语言的头文件。其他的设备对应的.dts就包括这个.dtsi 。\n  DTC（device tree compiler）  DTC是将.dts编译为.dtb的工具，相当于gcc。\nDTC的源代码位于内核的scripts/dtc目录中， 在Linux内核使能了设备树的情况下， 编译内核的时候，工具DTC会被编译出来， 对应于scripts/dtc/Makefile中hostprogs-y:=dtc这一编译目标。\n该工具一般在编译内核的时候，默认会自动执行编译操作，如果我们想单独编译设备树，该怎么办呢？\n 两条编译命令：\n 将dts文件编译为dtb\ndtc -I dts -O dtb xxx.dtb xxx.dts 将dtb文件反编译为dts\ndtc -I dtb -O dts xxx.dts xxx.dtb  DTB（device tree blob）  dtb文件是.dts 被 DTC 编译后的二进制格式的设备树文件，它由Linux内核解析，也可以被bootloader进行解析。\n 通常在我们为电路板制作NAND、SD启动映像时，会为.dtb文件单独留下一个很小的区域以存放之，之后bootloader在引导内核的过程中，会先读取该.dtb到内存。\n 总之，三者关系如下：\n 4、设备树语法 #  dts文件是一种ASCII文本格式的设备树描述，它有以下几种特性：\n  每个设备树文件都有一个根节点，每个设备都是一个节点。\n  节点间可以嵌套，形成父子关系，这样就可以方便的描述设备间的关系。\n  每个设备的属性都用一组key-value对(键值对)来描述。\n  每个属性的描述用;结束\n   记住上面的几个核心特性，往下看！\n 4.1 数据格式 #  /dts-v1/; / { node1 { a-string-property = \u0026#34;A string\u0026#34;; a-string-list-property = \u0026#34;first string\u0026#34;, \u0026#34;second string\u0026#34;; // hex is implied in byte arrays. no \u0026#39;0x\u0026#39; prefix is required a-byte-data-property = [01 23 34 56]; child-node1 { first-child-property; second-child-property = \u0026lt;1\u0026gt;; a-string-property = \u0026#34;Hello, world\u0026#34;; }; child-node2 { }; }; node2 { an-empty-property; a-cell-property = \u0026lt;1 2 3 4\u0026gt;; /* each number (cell) is a uint32 */ child-node1 { }; }; };  /：表示根节点 node1、node2：表示根节点下的两个子节点 child-node1、child-node2：表示子节点node1下的两个子节点 a-string-property = \u0026quot;A string\u0026quot;;：字符串属性，用双引号表示 cell-property = \u0026lt;0xbeef 123 0xabcd1234\u0026gt;;：32bit的无符号整数，用尖括号表示 binary-property = [0x01 0x23 0x45 0x67];：二进制数据用方括号表示 a-string-list-property = \u0026quot;first string\u0026quot;, \u0026quot;second string\u0026quot;;：用逗号表示字符串列表   4.2 数据结构 #  DeviceTree的结构非常简单，由两种元素组成：Node(节点)和Property(属性)。\n[label:] node-name[@unit-address] { [properties definitions] [child nodes] }  想象一下，一棵大树，每一个树干都认为是一个节点，每一片树叶，想作一个属性！\n  label：节点的一个标签，可以作为别名 node-name：节点的名称 unit-address：单元地址，也就是控制器的地址 properties：属性名称 definitions：属性的值   4.3 属性介绍 #  /dts-v1/; / { compatible = \u0026#34;acme,coyotes-revenge\u0026#34;; #address-cells = \u0026lt;1\u0026gt;; #size-cells = \u0026lt;0\u0026gt;; cpus { cpu@0 { compatible = \u0026#34;arm,cortex-a9\u0026#34;; reg = \u0026lt;0\u0026gt;; }; cpu@1 { compatible = \u0026#34;arm,cortex-a9\u0026#34;; reg = \u0026lt;1\u0026gt;; }; }; serial@101f0000 { #address-cells = \u0026lt;1\u0026gt;; #size-cells = \u0026lt;1\u0026gt;; compatible = \u0026#34;arm,pl011\u0026#34;; reg = \u0026lt;0x101f0000 0x1000 \u0026gt;; }; }; 4.3.1 基本属性之compatible、name、unit-address #   下面几个属性是基本属性\n  /dts-v1/;：表示一个dts设备树文件 /：表示根节点 compatible = \u0026quot;acme,coyotes-revenge\u0026quot;;  compatible： “兼容性” 属性，这是非常重要的一个属性兼容属性，由该属性值来匹配对应的驱动代码。 \u0026quot;acme,coyotes-revenge\u0026quot;：该值遵循\u0026quot;manufacturer,model\u0026quot;格式manufacturer表示芯片厂商，model表示驱动名称     compatible是一个字符串列表。列表中的第一个字符串指定节点在表单中表示的确切设备\u0026quot;\u0026lt;manufacturer\u0026gt;,\u0026lt;model\u0026gt;\u0026quot;。\n例如，飞思卡尔 MPC8349 片上系统 (SoC) 有一个串行设备，可实现 National Semiconductor ns16550 寄存器接口。因此，MPC8349 串行设备的 compatible 属性应为：compatible = \u0026quot;fsl,mpc8349-uart\u0026quot;, \u0026quot;ns16550\u0026quot;. 在这种情况下，fsl,mpc8349-uart指定确切的设备，并ns16550声明它与 National Semiconductor 16550 UART 的寄存器级兼容。\n  cpus：表示一个子节点，该子节点下又有两个子节点，分别为cpu0和cpu1。 cpu@0：遵循\u0026lt;name\u0026gt;[@\u0026lt;unit-address\u0026gt;]格式  \u0026lt;name\u0026gt;：ascii字符串，表示节点名称 \u0026lt;unit-address\u0026gt;：单元地址，设备的私有地址，在节点reg属性中描述。    4.3.2 寻址属性之address-cells、size-cells、reg、range #   下面几个属性与寻址相关的\n   #address-cells ：表示reg属性中表示地址字段的单元个数，每个单元32bit，即用多少个32bit单元表示地址信息。\n  #size-cells：表示reg属性中表示长度字段的单元个数，每个单元32bit，即用多少个32bit单元表示长度信息。\n  reg：该属性一般用于描述设备地址空间资源信息，一般都是某个外设的寄存器地址范围信息。其式为reg = \u0026lt;address1 length1 [address2 length2] [address3 length3] ... \u0026gt;。每个地址值都是一个或多个 32 位整数的列表，称为单元格。同样，长度值可以是单元格列表，也可以是空的。\n    以cpu节点为例：\n cpu@0 { compatible = \u0026#34;arm,cortex-a9\u0026#34;; reg = \u0026lt;0\u0026gt;; }; 其#address-cells=1表示reg属性中描述地址字段，所需32bit的单元个数为1，#size-cells=0表示reg属性中没有表示长度的单元，即reg=\u0026lt;0\u0026gt;\n  再以serial节点为例：\n serial@101f0000 { #address-cells = \u0026lt;1\u0026gt;; #size-cells = \u0026lt;1\u0026gt;; compatible = \u0026#34;arm,pl011\u0026#34;; reg = \u0026lt;0x101f0000 0x1000 \u0026gt;; };  该设备都被分配一个基址，以及被分配区域的大小\n 其#address-cells=1表示reg属性中描述地址字段需要1个32bit单元，#size-cells=1表示reg属性中描述长度字段需要2个单元，即reg=\u0026lt;0x101f0000 0x1000\u0026gt;\n 0x101f0000：表示serial的控制器起始地址 0x1000：表示serial控制器所占用的大小   地址映射部分还要了解一个属性\u0026lt;range\u0026gt;，为什么要引入这个属性呢？\n根节点与根节点的直接子节点，都使用了CPU的地址分配空间，但是根节点的非直接子节点，并不会自动实用CPU的地址空间，因此需要手动用\u0026lt;range\u0026gt;属性分配。\n如上述的serial节点，属于根节点下的直接子节点，无需手动再次分配地址空间，而下面所述的 external-bus节点，其内部的子节点就需要再次分配！\n/dts-v1/; / { compatible = \u0026#34;acme,coyotes-revenge\u0026#34;; #address-cells = \u0026lt;1\u0026gt;; #size-cells = \u0026lt;1\u0026gt;; ... external-bus { #address-cells = \u0026lt;2\u0026gt;; #size-cells = \u0026lt;1\u0026gt;; ranges = \u0026lt;0 0 0x10100000 0x10000 // Chipselect 1, Ethernet 1 0 0x10160000 0x10000 // Chipselect 2, i2c controller 2 0 0x30000000 0x1000000\u0026gt;; // Chipselect 3, NOR Flash ethernet@0,0 { compatible = \u0026#34;smc,smc91c111\u0026#34;; reg = \u0026lt;0 0 0x1000\u0026gt;; }; i2c@1,0 { compatible = \u0026#34;acme,a1234-i2c-bus\u0026#34;; #address-cells = \u0026lt;1\u0026gt;; #size-cells = \u0026lt;0\u0026gt;; reg = \u0026lt;1 0 0x1000\u0026gt;; rtc@58 { compatible = \u0026#34;maxim,ds1338\u0026#34;; reg = \u0026lt;58\u0026gt;; }; }; flash@2,0 { compatible = \u0026#34;samsung,k8f1315ebm\u0026#34;, \u0026#34;cfi-flash\u0026#34;; reg = \u0026lt;2 0 0x4000000\u0026gt;; }; }; };  该总线使用了不同的寻址方式，分析一下external-bus节点：\n  #address-cells = \u0026lt;2\u0026gt;：用两个单元表示地址 #size-cells = \u0026lt;1\u0026gt;：用一个单元表示长度 reg = \u0026lt;0 0 0x1000\u0026gt;：第一个0表示片选号，第二个0表示基于片选的偏移，第三个表示偏移的大小   这种抽象的表示，如何映射到CPU地址区域呢？```属性来帮助！\nranges = \u0026lt;0 0 0x10100000 0x10000 // Chipselect 1, Ethernet 1 0 0x10160000 0x10000 // Chipselect 2, i2c controller 2 0 0x30000000 0x1000000\u0026gt;; // Chipselect 3, NOR Flash range：表示了不同设备的地址空间范围，表中的每一项都是一个元组，包含子地址、父地址以及子地址空间中区域的大小，这三个字段。\n 子地址字段：由子节点的#address-cells决定，如前面的0 0、0 1 父地址字段：由父节点的#address-cells决定，如0x10100000、0x10160000 子地址空间字段：描述子节点的空间大小，由父节点的#size-cells决定，如0x10000、0x10000  经过映射后，总线的地址映射如下：\n  Offset 0 from chip select 0 is mapped to address range 0x10100000..0x1010ffff Offset 0 from chip select 1 is mapped to address range 0x10160000..0x1016ffff Offset 0 from chip select 2 is mapped to address range 0x30000000..0x30ffffff   4.3.3 中断属性之interrupt-controller、interrupt-cells、interrupt-parent、interrupts #  /dts-v1/; / { compatible = \u0026#34;acme,coyotes-revenge\u0026#34;; #address-cells = \u0026lt;1\u0026gt;; #size-cells = \u0026lt;1\u0026gt;; interrupt-parent = \u0026lt;\u0026amp;intc\u0026gt;; cpus { #address-cells = \u0026lt;1\u0026gt;; #size-cells = \u0026lt;0\u0026gt;; cpu@0 { compatible = \u0026#34;arm,cortex-a9\u0026#34;; reg = \u0026lt;0\u0026gt;; }; cpu@1 { compatible = \u0026#34;arm,cortex-a9\u0026#34;; reg = \u0026lt;1\u0026gt;; }; }; serial@101f0000 { compatible = \u0026#34;arm,pl011\u0026#34;; reg = \u0026lt;0x101f0000 0x1000 \u0026gt;; interrupts = \u0026lt; 1 0 \u0026gt;; }; intc: interrupt-controller@10140000 { compatible = \u0026#34;arm,pl190\u0026#34;; reg = \u0026lt;0x10140000 0x1000 \u0026gt;; interrupt-controller; #interrupt-cells = \u0026lt;2\u0026gt;; }; }; 如上\n interrupt-controller：声明一个节点是接收中断信号的设备，也就是中断控制器 #interrupt-cells：interrupt-controller节点下的一个属性，表明中断标识符用多少个单元表示 interrupt-parent：设备节点中的一个属性，选择哪个中断控制器 interrupts：设备节点的一个属性，中断标识符列表，其单元个数取决于#interrupt-cells  根据设备树，我们了解到：\n 该机器有一个中断控制器interrupt-controller@10140000 intc标签，为中断控制器的别名，方便引用 #interrupt-cells = \u0026lt;2\u0026gt;;：中断标识符用两个单元格表示 interrupt-parent = \u0026lt;\u0026amp;intc\u0026gt;;：选择中断控制器 interrupts = \u0026lt; 1 0 \u0026gt;;：表示一个中断，第一个值用于表明中断线编号，第二个值表明中断类型，如高电平，低电平，跳变沿等  4.3.4 其他属性之aliases、chosen #  aliases { ethernet0 = \u0026amp;eth0; serial0 = \u0026amp;serial0; }; aliases：正如其名，别名属性，使用方式：property = \u0026amp;label;\n  chosen { bootargs = \u0026quot;root=/dev/nfs rw nfsroot=192.168.1.1 console=ttyS0,115200\u0026quot;; }; chosen：该属性并不表示一个真实的设备，但是提供一个空间，用于传输固件和Linux之间的数据，像启动参数，\n 5、设备树的加载流程 #   我们知道，dts文件经过dtc工具编译为dtb，内核加载并解析dtb文件，最终获得设备树的信息。\n 那么Linux如何加载``dtb文件，并生成对应节点的呢？\n 5.1 设备树地址设置 #  我们一般通过Bootloader引导启动Kernel，在启动Kernel之前，Bootloader必须将dtb文件的首地址传输给Kernel，以供使用。\n Bootloader将dtb二进制文件的起始地址写入r2寄存器中 Kernel在第一个启动文件head.S/head-common.S中，读取r2寄存器中的值，获取dtb文件起始地址 跳转入口函数start_kernel执行C语言代码   5.2 获取设备树中的平台信息——machine_desc #  在dts文件中，在根节点中有一个compatible属性，该属性的值是一系列的字符串，比如compatible = “samsung，smdk2440”“samsung,smdk2410,samsung，smdk24xx”;，该属性就是告诉内核要选择什么样的machine_desc，因为machine_desc结构体中有一个dt_compat成员，该成员表示machine_desc支持哪些单板，所以内核会把compatible中的字符串与dt_compat进行依次比较。\nstart_kernel // init/main.c  setup_arch(\u0026amp;command_line); // arch/arm/kernel/setup.c  mdesc = setup_machine_fdt(__atags_pointer); // arch/arm/kernel/devtree.c  early_init_dt_verify(phys_to_virt(dt_phys) // 判断是否有效的dtb, drivers/of/ftd.c  initial_boot_params = params; mdesc = of_flat_dt_match_machine(mdesc_best, arch_get_next_mach); // 找到最匹配的machine_desc, drivers/of/ftd.c  while ((data = get_next_compat(\u0026amp;compat))) { score = of_flat_dt_match(dt_root, compat); if (score \u0026gt; 0 \u0026amp;\u0026amp; score \u0026lt; best_score) { best_data = data; best_score = score; } } machine_desc = mdesc;  5.3 获取设备树的配置信息 #   在前面，我们也知道设备树中的chosen属性，用于传输固件和Linux之间的数据，包含一些启动参数，那么我们该如何解析出来呢？\n  /chosen节点中bootargs属性的值, 存入全局变量： boot_command_line 确定根节点的这2个属性的值: #address-cells, #size-cells 存入全局变量: dt_root_addr_cells, dt_root_size_cells 解析/memory中的reg属性, 提取出\u0026quot;base, size\u0026quot;, 最终调用memblock_add(base, size);   5.4 设备树节点解析 #  dtb文件会在内存中一直存在着，不会被内核或者应用程序占用，我们需要使用的时候可以直接使用dtb文件。dtb文件的内容会被解析生成多个device_node，然后这些device_node构成一棵树, 根节点为: of_root\n 每一个节点都以TAG(FDT_BEGIN_NODE, 0x00000001)开始, 节点内部可以嵌套其他节点,\n每一个属性都以TAG(FDT_PROP, 0x00000003)开始\n   设备树中的每一个节点，都会被转换为device_node结构体  struct device_node { const char *name; // 来自节点中的name属性, 如果没有该属性, 则设为\u0026#34;NULL\u0026#34;  const char *type; // 来自节点中的device_type属性, 如果没有该属性, 则设为\u0026#34;NULL\u0026#34;  phandle phandle; const char *full_name; // 节点的名字, node-name[@unit-address]  struct fwnode_handle fwnode; struct property *properties; // 节点的属性  struct property *deadprops; /* removed properties */ struct device_node *parent; // 节点的父亲  struct device_node *child; // 节点的孩子(子节点)  struct device_node *sibling; // 节点的兄弟(同级节点)  #if defined(CONFIG_OF_KOBJ)  struct kobject kobj; #endif  unsigned long _flags; void *data; #if defined(CONFIG_SPARC)  const char *path_component_name; unsigned int unique_id; struct of_irq_controller *irq_trans; #endif  };   将device_node转换为platform_device   那么多的device_node，哪些会被转化为platform_device呢？\n 根节点下的子节点，且该子节点必须包含compatible属性； 如果一个节点的compatile属性含有这些特殊的值(“simple-bus”,“simple-mfd”,“isa”,“arm,amba-bus”)之一，那么它的子结点(需含compatile属性)也可以转换为platform_device。   struct platform_device { const char\t*name; int\tid; bool\tid_auto; struct device\tdev; u32\tnum_resources; struct resource\t*resource; const struct platform_device_id\t*id_entry; char *driver_override; /* Driver name to force a match */ /* MFD cell pointer */ struct mfd_cell *mfd_cell; /* arch specific additions */ struct pdev_archdata\tarchdata; }; 转换完成之后，\n 设备树中的reg/irq等属性，都存放在了platform_device-\u0026gt;resource结构体中 设备树中的其他属性，都存在在了platform_device.dev-\u0026gt;of_node结构体中    C代码获取设备树属性  转换完成之后，内核提供了一些API来直接获取设备树中对应的属性。如：\n of_property_read_u32_index：获取设备树中某个属性的值 of_property_read_string：获取设备树中某个属性的字符串的值 of_get_address：获取设备树中的某个节点的地址信息  整体总结下来，有几个类别：\na. 处理DTB of_fdt.h // dtb文件的相关操作函数, 我们一般用不到, 因为dtb文件在内核中已经被转换为device_node树(它更易于使用)  b. 处理device_node of.h // 提供设备树的一般处理函数, 比如 of_property_read_u32(读取某个属性的u32值), of_get_child_count(获取某个device_node的子节点数) of_address.h // 地址相关的函数, 比如 of_get_address(获得reg属性中的addr, size值) of_match_device(从matches数组中取出与当前设备最匹配的一项) of_dma.h // 设备树中DMA相关属性的函数 of_gpio.h // GPIO相关的函数 of_graph.h // GPU相关驱动中用到的函数, 从设备树中获得GPU信息 of_iommu.h // 很少用到 of_irq.h // 中断相关的函数 of_mdio.h // MDIO (Ethernet PHY) API of_net.h // OF helpers for network devices. of_pci.h // PCI相关函数 of_pdt.h // 很少用到 of_reserved_mem.h // reserved_mem的相关函数  c. 处理 platform_device of_platform.h // 把device_node转换为platform_device时用到的函数, // 比如of_device_alloc(根据device_node分配设置platform_device), // of_find_device_by_node (根据device_node查找到platform_device), // of_platform_bus_probe (处理device_node及它的子节点) of_device.h // 设备相关的函数, 比如 of_match_device   上述总结下来，流程为dts-\u0026gt;dtb-\u0026gt;device_node-\u0026gt;platform_device\n 6、设备树调试 #   查看原始的dtb文件  ls /sys/firmware/fdt hexdump -C /sys/firmware/fdt   查看设备树信息  ls /sys/firmware/devicetree ls /proc/device-tree  以目录结构程现的dtb文件, 根节点对应base目录, 每一个节点对应一个目录, 每一个属性对应一个文件\n/proc/device-tree 是链接文件, 指向 /sys/firmware/devicetree/base\n   查看所有硬件信息  ls /sys/devices/platform  系统中所有的platform_device, 有来自设备树的, 也有来有.c文件中注册的。\n  7. 参考地址 #  [1]：https://elinux.org/Device_Tree_Usage\n[2]：https://www.kernel.org/doc/Documentation/devicetree/usage-model.txt\n[3]：https://blog.csdn.net/zj82448191/article/details/109195364\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":12,"href":"/docs/linux/linux_mmc_subsystem/mmc%E5%AD%90%E7%B3%BB%E7%BB%9F%E4%BA%8Cemmc%E5%8D%8F%E8%AE%AE/","title":"【MMC子系统】二、EMMC协议","section":"Linux MMC 子系统","content":"【MMC子系统】 二、EMMC协议 #  1、前言 #  在上一节，我们知道EMMC、SD、SDIO三种规范都是在MMC规范之上发展而来，协议相差不大，所以Linux Kernel才能使用MMC子系统来统一管理！\n下面，我们以MMC协议为例，来了解一下相关协议!\n 2、EMMC基本了解 #  2.1 物理线路 #     物理接口 接口含义     CLK 时钟线，此信号的每一周期控制命令线上的 1 bit 传输，以及所有数据线上 1 bit（1x） 或 2 bit（2x）传输。   CMD 命令线，此信号是双向命令通道，用于设备初始化和命令传输。CMD信号有两种工 作模式：用于初始化模式开漏模式和快速命令传输推拉模式。   DAT0-7 这些是双向的数据通道。DAT 信号以推拉模式工作。缺省状态，只有DAT0处于推拉模式，DAT1-7处于上拉（内含上拉），进入4bit后，DAT0-3处于推拉     2.2 EMMC相关寄存器了解 #   2.3 其他特性了解 #    读写模式：单块读写，多块读写\n  寻址方式：字节寻址和扇区寻址，字节寻址允许最大2GB，容量超过2GB的，使用扇区（512B）寻址\n  电压模式：支持高电压和双电压模式\n  支持增强分区模式等\n   3、总线协议 #  3.1 基础了解 #   命令：启动一种操作的Token，命令从主机发往设备，在CMD线路上串行传输。 应答：从设备发往主机作为对上一命令的回答的Token，在CMD线路上串行传输。 数据：在主从机之间双向传输，总线宽度可以是1-bit（缺省）、4-bit 和 8-bit   3.2 命令格式 #  每一个Token，都是由一个起始位（’0’）前导，以一个停止位（’1’）终止。总长度是 48 比特。每一个 Token 都用CRC保护，因此可以检测到传输错误，可重复操作。\n命令索引：也就是前面CMDX的0，1，2，3等命令编号。\n命令参数：有些命令需要参数，例如地址信息等。\n 3.3 命令格式 #  ① 无应答广播命令（bc）\n② 有应答广播命令（bcr）\n③ 点对点寻址命令（ac），无DAT数据\n④ 寻址数据传输命令（adtc），有DAT数据\n 3.4 应答格式 #  所有的应答均通过命令线CMD发送，编码的长度取决于应答类型，应答Token类型有有 5 种编码方案，分别为R1、R2、R3、R4、R5。Token 长度是 48 或 136 比特。\n① R1（正常应答类型） #  编码长度48bit，bits 45:40 表示应答相对的命令索引，bit 8:39表示欲发送设备的状态信息。\n ② R2（CID CSD寄存器） #  编码长度136bit，CID寄存器的内容，作为对CMD2和CMD10的应答发送。CSD寄存器内容作为对CMD9应答发送。并且CID和CSD寄存器只有bit 127:1被发送。\n ③ R3（OCR寄存器） #  编码长度48bit，OCR寄存器作为对CMD1的应答发送。\n ④ R4（快速I/O） #  编码长度48bit，参数域包含了被寻址设备的RCA、要读写的寄存器地址和内容。\n ⑤ R5（中断请求） #  编码长度48bit，如果应答是主机生成的，参数的RCA应为0。\n 4、工作模式 #   主机和设备之间的通信，都由主机控制发起，主机发送命令，引起设备的应答。\n EMMC工作模式也定义了5种：\n 引导模式：使设备处于引导状态 设备识别模式：引导模式结束，设备再次模式下，接受SET_RCA命令，进行识别设备。 数据传输模式：分配RCA后，设备进入数据传输模式，准备数据通信 中断模式：主机与设备同时进入，无数据传输，只允许消息来自主机或从机的中断请求 非活动模式：如果设备工作电压范围和访问模式无效，则进入非活动模式。   每一种模式，都有其各自的特点，我们主要来了解一下设备识别过程和数据传输过程。\n   更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  4.1 设备识别模式 #   乍一看图，肯定大家都一脸懵逼，不仔细分析协议，单看图还是有一定理解难度的。\n 总体来说，设备识别模式下，主机要想识别到卡，主要步骤有如下几步：\n  复位设备\n  验证工作电压及访问模式\n  识别设备并分配相对设备地址RCA\n  使设备进入数据传输模式\n   4.1.1 复位 #  EMMC控制器通过发送CMD0，参数为0x00000000，使设备进入Idle状态。\n同时，为了向后兼容，在除Inactive的任何状态，接收 非0XFFFFFFFA或0XF0F0F0F0的参数，都作为CMD0。\n4.1.2 验证工作电压及访问模式 #  EMMC控制器通过发送CMD1，参数为OCR寄存器，该寄存器种包含了2bit的存储器访问模式。\n如上，bit[30：29]表示访问模式，通过CMD1发送该数据目的是向存储器同步寻址类型。\nEMMC设备同时也应以固定模式0x00FF8080 或 0x40FF8080（如果设备忙）、0x80FF8080（容量小于等于 2GB）或 0xC0FF8080（容量大于 2GB）应答。\n同时，bit31用来判忙，如果为1，说明EMMC设备仍然处于复位过程中，主机也同时重复发送CMD1来确保该忙位清除。\n4.1.3 识别设备分配RCA #  通过CMD1进行检查后，不符合的设备就进入了Inactive状态。而符合的设备就进入了Ready状态。\n进而，EMMC控制器发送CMD2，请求符合要求的设备发送唯一设备标识CID号。CID号对于每一张卡，都是唯一的。\n发送CID成功的设备，就进入到了Identification状态。\n进而，EMMC控制器发送CMD3，赋予设备一个相对设备地址RCA，从设备一旦接收到RCA，设备就变为Stand-by状态，即空闲态。\n 4.2 数据传输过程 #  分配完RCA后，从设备接收到RCA，立即处于stand-by状态时，CMD和DAT线路，均变为推拉模式。\n 4.2.1 获取CSD寄存器信息 #  CMD9：主机发送该命令，以获取设备专用寄存器CSD的数据，如块长度，存储容量，最大时钟速率等。\n 4.2.2 获取CID寄存器信息 #  CMD10：主机发送该命令，以获取设备专用寄存器CID的数据，获取设备识别号。\n 4.2.3 切换为Transfer状态 #  CMD7：主机发送该命令，选定该设备，使其切换到发送数据状态。\n 4.2.4 查看EXT_CSD扩展寄存器 #  CMD8：主机发送该命令，设备作为数据块发送其 EXT_CSD寄存器的数据，设备将数据作为一个512字节的数据块发送。\n 4.2.5 修改EXT_CSD扩展寄存器的值 #  CMD6：主机发送该命令，用于切换工作模式，或者修改EXT_CSD寄存器。\nCMD6，这个命令，参数的设置有很大讲究呢！\n [31:26]：正如手册所写，直接设置为0\n[25:24]：访问模式选择，那么访问模式有哪几种呢？\n如果 SWITCH 命令用于更换命令集（[25:24]为00），Index 和 Value 域被忽略（[23:16]、[15:8]忽略），且 EXT_CSD 不会被写。\n如果 SWITCH命令用于写 EXT_CSD寄存器，Cmd Set 域被忽略[2:0] 忽略，命令集保持不变。\n[23:16]：索引，指的是EXT_CSD寄存器中，所要修改字节的索引。\n[15:8]：要写入的值\n[2:0]：命令集选择，命令集有如下几种类别，相关手册可以查阅。\n 举个栗子：\n如果我们想要操作总线长度，我们该怎么修改呢？\nCMD6命令，发送args=03B70200，即可修改。\n03：代表访问模式为写字节\nB7：转换为十进制183，对应EXT_CSD总线宽度模式的字节。\n02：设置该字节的值为02，即8位数据总线\n00：写字节访问模式下，该位无效。\n 4.2.6 读数据 #   单块读  CMD17：直接发送读命令，参数为要写入的数据地址信息，只读一个块。\n 多块读  CMD18：直接发送读命令，参数为要写入的数据地址信息，并且一直读下去。\nCMD12：停止命令，停止传输。\n4.2.7 写数据 #   确保设备处于发送状态，即主机发送CMD7命令\n  单块写  CMD24：直接发送写命令，参数为要写入的数据地址信息，只写一个块。\n 多块写  多块写的模式有两种：\n① 一种是：设置要传输的数据块的个数，达到个数后，自动停止\nCMD16：设置要传输的块长度\nCMD25：开始发送CMD16指定长度的数据块，直到达到设置的数据块写入完成。\n②另一种是：一直传输数据，直到接收停止数据的命令\nCMD25：开始发送数据块，一直等待数据发送完全\nCMD12：停止命令，停止传输。\n好啦，到这里我们基本了解了MMC的协议，这也有助于我们去分析EMMC的框架。\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":13,"href":"/docs/linux/bluetooth/bluetooth%E8%93%9D%E7%89%99%E5%BC%80%E5%8F%91%E4%BA%8C%E8%93%9D%E7%89%99%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8/","title":"【Bluetooth蓝牙开发】二、蓝牙开发入门","section":"Bluetooth蓝牙开发","content":"【Bluetooth|蓝牙开发】二、蓝牙开发入门 #  1、蓝牙基础概念 #  蓝牙，是一种利用低功率无线电，支持设备短距离通信的无线电技术，能在包括移动电话、PDA、无线耳机、笔记本电脑、相关外设等众多设备之间进行无线信息交换，蓝牙工作在全球通用的2.4GHz ISM（即工业、科学、医学）频段，使用IEEE802.11协议。\n 2、蓝牙发展历程 #  自1994年由爱立信推出至今，蓝牙技术已经走过了20个岁月。从最初的Bluetooth V1.0，到Bluetooth V5.2，经历了近9个版本的修订后，发展为当前的状况。\n “蓝牙”的形成背景是这样的：\n 1998 年 5 月，爱立信、诺基亚、东芝、 IBM和英特尔公司等五家著名厂商， 在联合开展短程无线通信技术的标准化活动时提出了蓝牙技术，其宗旨是提供一种短距离、 低成本的无线传输应用技术。\n芯片霸主 Intel 公司负责半导体芯片和传输软件的开发，爱立信负责无线射频和移动电话软件的开发， IBM 和东芝负责笔记本电脑接口规格的开发。\n1999 年下半年，著名的业界巨头微软、摩托罗拉、三星、朗讯与蓝牙特别小组的五家公司共同发起成立了蓝牙技术推广组织，从而在全球范围内掀起了一股“蓝牙”热潮。\n全球业界即将开发一大批蓝牙技术的应用产品， 使蓝牙技术呈现出极其广阔的市场前景，并预示着 21 世纪初将迎来波澜壮阔的全球无线通信浪潮。\n  第一代蓝牙：关于短距离通讯早期的探索，使用的是BR技术，此时蓝牙的理论传输速率，只能达到721.2Kbps。 第二代蓝牙：新增的 EDR（Enhanced Data Rate）技术，使得蓝牙设备的传输率可达 3Mbps。 第三代蓝牙：核心是 AMP（Generic Alternate MAC/PHY），这是一种全新的交替射频技术，支持动态地选择正确射频，传输速率高达 24Mbps 第四代蓝牙：主推” Low Energy”低功耗， BLE（Bluetooth Low Energy）低功耗功能 第五代蓝牙：开启「物联网」时代大门，在低功耗模式下具备更快更远的传输能力   3、蓝牙技术概述 #  蓝牙协议包括两种技术：BR：Basic Rate和LE：Low Energy。这两种技术都包括搜索（discovery）管理、连接（connection）管理等机制，但它们是相互独立的，不能互通的技术！\n 厂商如果只实现了一种，那么只能与同样实现该技术的设备互通。\n如果厂商要确保能和所有的蓝牙设备互通，那么就只能同时实现两种技术，而不去管是否真的需要。\n  3.1 Basic Rate(BR) #  BR：Basic Rate是正宗的蓝牙技术，可以包括**可选（optional）的EDR（Enhanced Data Rate）技术，以及交替使用的（Alternate）**的MAC（Media Access Control）层和PHY层扩展（简称AMP（Alternate MAC and PHY layer extension））。\n BR：最早期的蓝牙技术，速度只能达到721.2Kbps，在那个年代，已为高大上了。 EDR：随着技术的提升，使用EDR技术的蓝牙，理论速率可以达到2.1Mbps。 AMP：使用AMP技术的蓝牙，理论速率可以达到54Mbps。   AMP的Alternate交替使用体现在：由于蓝牙自身的物理层和AMP技术差异太明显，BR/EDR和AMP是不能同时使用的。\n简单的说，就是：BR和EDR是可以同时存在的，但BR/EDR和AMP只能二选一\n  3.2 Low Energy（LE） #   上面所讲的BR技术的进化路线，就是传输速率的加快、加快、再加快。\n但能量是守恒的，你想传的更快，代价就是消耗更多的能量。而有很多的应用场景，并不关心传输速率，反而非常关心功耗。\n这就是Bluetooth LE（称作蓝牙低功耗）产生的背景。\n 从它的英文名字上就可以看出它是一种低功耗蓝牙技术，是蓝牙技术联盟设计和销售的一种个人局域网技术，旨在用于医疗保健、运动健身、信标、安防、家庭娱乐等领域的新兴应用。\n低功耗蓝牙与经典蓝牙使用相同的2.4GHz无线电频率，因此双模设备可以共享同一个天线。低功耗蓝牙使用的调制系统更简单。\nLE技术相比BR技术，差异非常大，或者说就是两种不同的技术，凑巧都加一个“蓝牙”的前缀而已。\n目前BLE主要广泛应用于IoT产品领域。\n 4、常见蓝牙架构 #   市面上，大致有几种蓝牙架构：\n  4.1 SOC蓝牙单芯片方案 #  一般是半导体厂商半开源协议栈，把开发的蓝牙协议栈直接烧写到蓝牙芯片中，（比如CSR BC4/5,CSR8670,CSR8675,TI CC2540，NRF51xxx,NRF52xxx，乐鑫ESP32等等），架构如下：\n此类芯片一般可以直接做为MCU用，这类产品一般用于消费类电子，集成度很高，调调部参数可以直接使用，常见的有蓝牙耳机等产品。\n 4.2 SOC蓝牙+MCU方案 #  在集成好的蓝牙芯片基础上，通过特定的接口（UART居多），发送自定义的command来达到想要的功能。比如发送0x01代表搜索周围设备\u0026hellip;\u0026hellip;\n此部分的应用，将蓝牙作为一个外设使用，用于远程通信。\n例如网上卖的一些蓝牙串口。\n 4.3 蓝牙host + controller分开方案 #  这种应用算是蓝牙最复杂的应用，客户需要使用蓝牙的场景有很多，牵涉到的蓝牙协议也有很多，需要将Host与Controller分开，集成更多的蓝牙协议，比如蓝牙电话（HFP），蓝牙音频（A2DP），蓝牙音乐控制（AVRCP），蓝牙电话本（PBAP），蓝牙短信（MAP）等。\n其中Transport是一个协议，H2就是在USB的基础上的协议，H4,H5,BCSP是UART基础上的协议，当然还有SDIO。\n此部分应用，将定制蓝牙的各种服务，实现蓝牙多功能需求，\n 4.4 使用场景 #  大概列举了以下几种，帮助理解：\n1）手机 -\u0026gt; 手机的蓝牙复杂应用，注定要用第3种方案，也就是蓝牙协议栈（host）在主芯片中，蓝牙芯片为HCI架构的\n2）蓝牙音响，蓝牙耳机 -\u0026gt; 此种应用一般用单芯片方案就能hold住，比如CSR8670/8675/杰理蓝牙等，好处在于开发便捷\n3）蓝牙手表 -\u0026gt; 手表要看功能复杂性，如果仅仅有时间显示，传感器交互，蓝牙，那么可以选择单芯片方案（也就是方案1），如果有网络等比较复杂的功能就要使用MCU+蓝牙芯片方案（也就是方案3）了\n4）蓝牙手环，蓝牙心率带等 -\u0026gt; 基本上是单芯片方案\n 5、参考文档 #  [1]：蓝牙官网：https://www.bluetooth.com/\n[2]：https://blog.csdn.net/XiaoXiaoPengBo/article/details/107466841\n[3]：https://zhuanlan.zhihu.com/p/43516534\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":14,"href":"/docs/linux/linux_led_subsystem/led%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90%E4%BA%8Cled%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%A1%86%E6%9E%B6%E5%88%86%E6%9E%90/","title":"【LED子系统深度剖析】二、LED子系统框架分析","section":"Linux LED子系统","content":"【LED子系统深度剖析】二、LED子系统框架分析 #  1、前言 #  我们学习嵌入式，无论是C51、STM32或者是ARM，都是从点灯开始的，点灯在嵌入式中的地位等同于Hello World在各大语言中的地位！\n虽然LED功能简单，但是其麻雀虽小，五脏俱全，在学习Linux驱动开发的过程中，学习LED子系统，往往也能够起到牵一发而动全身的作用，也更有益于大家熟悉驱动开发的框架！\n2、LED裸机处理 #  我们在学习Linux驱动框架的时候，第一步要做的就是去掉子系统的面纱，先弄明白裸机处理的流程！\n有嵌入式经验的朋友，对LED的裸机在清楚不过了，上面是LED的硬件电路，通常一端接到VCC，一端接到GPIO，当GPIO拉低时，LED亮；当GPIO拉高时，LED灭。\n在这里裸机我们不过多了解了，目的在于窥探LED子系统。\n3、LED子系统框架 #  框架是什么？\n框架是一个规范，为我们开发者增加限制的同时，也是为了更好的开发新的程序，新的功能，其目的主要是：将不变的成分剥离开来，固化进框架，让开发者做最少的事情!\n框架所处的位置，正如上图所示，由下往上看：\n Hardware：我们的硬件设备，指的是LED 硬件驱动层：是直接操作硬件的实现，用于驱动硬件，实现相应的功能，并且将硬件设备注册进框架之中。 核心层：将LED进行统一管理，提供注册，注销，管理LED等相关接口，起到呈上启下的作用，方便上层调用。 用户层：用户通过sysfs文件系统中对应的文件节点，能够直接控制LED的亮灭。  4、LED子系统目录结构及核心文件 #  了解完LED子系统框架之后，我们来分析一下其相关的目录结构！\nketnel │ └── driver │ │ └── leds │ │ │ ├──\tMakefile │ │ │ ├──\tled-core.c │ │ │ ├──\tled-gpio.c │ │ │ ├──\tled-class.c │ │ │ ├──\tled-triggers.c │ │ │ ├──\t...... │ │ │ └── trigger │ │ │ │ ├── ledtrig-cpu.c │ │ │ │ ├── ledtrig-heartbeat.c │ │ │ │ ├── ....... 上面即为LED子系统的目录结构，其主要核心文件有：\n led-gpio.c：硬件驱动层实现，直接控制硬件设备，并且将其操作硬件设备的接口，注册进入LED驱动框架 led-core.c：核心层实现，抽象软件实现的相关功能，如闪烁，亮度设置等等，并管理LED设备 led-class.c：完成逻辑设备的注册以及相应属性文件的添加 led-triggers.c：LED触发功能的抽象，提供注册接口以及闪烁功能控制 ledtrig-cpu.c：将LED作为CPU灯 ledtrig-heartbeat.c：将LED作为心跳灯  5、sysfs目录结构 #  在上面，我们了解了Linux下LED子系统相关的几个重要文件，那么该子系统在嵌入式设备的sysfs中的呈现方式又是如何的呢？\n5.1 确保LED子系统打开 #  我们如果在内核中打开了LED子系统目录下的Makefile文件，也就是kernel/drivers/leds/Makefile，我们看到\n# SPDX-License-Identifier: GPL-2.0  # LED Core obj-$(CONFIG_NEW_LEDS)\t+= led-core.o obj-$(CONFIG_LEDS_CLASS)\t+= led-class.o obj-$(CONFIG_LEDS_CLASS_FLASH)\t+= led-class-flash.o obj-$(CONFIG_LEDS_TRIGGERS)\t+= led-triggers.o ...... 我们必须在内核的配置中，通过 make menuconfig打开LED的相关配置，才支持LED相关功能。\n5.2 查看sysfs文件结构 #  我们在开发板中输入ls /sys/class/leds/，可以查看LED子系统生成的文件信息。\n   /sys/class/leds下的目录 对应的LED灯设备     cpu 开发板的心跳灯   red Pro开发板RGB灯的红色   green Pro开发板RGB灯的绿色   blue Pro开发板RGB灯的蓝色   mmc0: SD卡指示灯（出厂镜像默认没有启用）    根据打开配置的不同，生成不同的文件节点\n查看一下CPU节点的信息\n#在开发板上有cpu目录，可在开发板上执行如下命令查看 ls /sys/class/leds/cpu 相关属性文件有：brightness、max_brightness、trigger等\n max_brightness文件：表示LED灯的最大亮度值。 brightness文件：表示当前LED灯的亮度值，它的可取 值范围为[0~max_brightness]，一些LED设备不支持多级亮度，直接以非0值来 表示LED为点亮状态，0值表示灭状态。 trigger文件：则指示了LED灯的触发方式，查看该文件的内容时，该文件会 列出它的所有可用触方式，而当前使用的触发方式会以“[]”符号括起。  常见的触 发方式如下表所示：\n   触发方式 说明     none 无触发方式   disk-activity 硬盘活动   nand-disk nand flash活动   mtd mtd设备活动   timer 定时器   heartbeat 系统心跳    6、总结 #  好啦，本篇主要介绍了LED子系统框架，下面我们来回顾一下：\n 明白裸机中LED底层硬件的操作方法 了解Linux下LED子系统驱动框架以及每层含义 了解Linux和sysfs的文件分布   欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":15,"href":"/docs/linux/linux_kernel_lock/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3linux%E9%94%81%E6%9C%BA%E5%88%B6%E4%BA%8C%E4%B8%AD%E6%96%AD%E5%B1%8F%E8%94%BD/","title":"【深入理解Linux锁机制】二、中断屏蔽","section":"Linux 内核锁详解","content":"【深入理解Linux内核锁】二、中断屏蔽 #   上一篇了解了内核锁的由来，本篇文章主要来讲一下中断屏蔽的底层实现以及原理。\n  1、中断屏蔽思想 #  中断屏蔽，正如其名，屏蔽掉CPU的中断响应功能，解决并发引起的竞态问题。\n 在进入临界区前屏蔽中断，这么做有什么好处，以及有什么弊端？\n 好处在于：\n 解决了进程与中断之间的并发：保证在执行临界区代码时，不被中断所打断。 解决了进程与进程之间调度的并发：系统的进程调度与中断息息相关，同时也限制了系统进程的并发，解决了系统进程并发带来的竞态问题。  弊端在于：\n 各类中断类型较多，一棒子打死影响大：Linux内核中，除了系统进程调度依赖中断，还有一些异步I/O等众多操作都依赖中断，因此长时间屏蔽中断是很危险的，会对系统造成严重影响，因此也要求临界区代码要简短。 解决的不够完善：关闭中断能够解决进程调度、中断引发的竞态，但是这些都是单CPU内部的，对于SMP对称多处理器，仍然不可避免的会收到其他CPU的中断。因此，并不能解决SMP多CPU引发的竞态。  因此，单独使用中断屏蔽通常不是一种值得推荐的避免竞态的方法\n 2、Linux内核中断屏蔽的实现 #  2.1 Linux内核提供的API接口 #   关于中断屏蔽，Linux内核所提供的接口如下：\n local_irq_enable()\t//\t使能本CPU的中断 local_irq_disable()\t//\t禁止本CPU的中断 local_irq_save(flags)\t//\t禁止本CPU的中断，并保存CPU中断位的信息 local_irq_restore(flags)\t//\t使能本CPU的中断，并恢复CPU中断位的信息 local_bh_disable(void)\t//\t禁止本CPU底半部中断 local_bh_enable(void) //\t使能本CPU底半部中断 文件位置：kernel/include/linux/irqflags.h\n local_irq_enable与local_irq_disable：直接打开/关闭本CPU内的中断，包括了顶半部和底半部中断的打开和关闭。 local_irq_save与local_irq_restore：直接打开/关闭本CPU中断，并且保存中断屏蔽前的状态，便于后续恢复 local_bh_enable与local_bh_disable：直接打开/关闭本CPU内的底半部中断   2.2 API接口实现分析 #   因为中断屏蔽与底层芯片架构有关，不同架构处理方式不同，我们以ARM为例\n 2.2.1 local_irq_enable #  #define local_irq_enable()\tdo { raw_local_irq_enable(); } while (0)  #define raw_local_irq_enable()\tarch_local_irq_enable()  #define arch_local_irq_enable arch_local_irq_enable static inline void arch_local_irq_enable(void) { asm volatile( \u0026#34;\tcpsie i\t@ arch_local_irq_enable\u0026#34; : : : \u0026#34;memory\u0026#34;, \u0026#34;cc\u0026#34;); } 函数介绍：local_irq_enable函数用于将CPSR寄存器中的中断使能位设为1，从而使得CPU能够响应中断。\n文件位置：kernel/arch/arm/include/asm/irqflags.h\n相关实现：\nasm：声明一个内联汇编表达式\ncpsie i：全称Change Processor State, Interrupts Enabled，主要用来设置CPSR寄存器的I位，来允许本CPU响应中断。\nmemory：向汇编说明，此处内存发生了更改，类似于内存屏障的作用\ncc：表示可能会修改条件码的标志\n 汇编语言的格式，大家可以自行简单了解\n  2.2.2 local_irq_disable #  #define local_irq_disable() \\ do { raw_local_irq_disable(); trace_hardirqs_off(); } while (0)  #define raw_local_irq_disable()\tarch_local_irq_disable()  #define arch_local_irq_disable arch_local_irq_disable static inline void arch_local_irq_disable(void) { asm volatile( \u0026#34;\tcpsid i\t@ arch_local_irq_disable\u0026#34; : : : \u0026#34;memory\u0026#34;, \u0026#34;cc\u0026#34;); } 函数介绍：local_irq_disable函数用于将CPSR寄存器中的中断使能位设为0，从而禁止CPU响应中断。\n文件位置：kernel/arch/arm/include/asm/irqflags.h\n相关实现：同上\n cpsid：全称Change Processor State, Interrupts Disabled，用于清除CPSR寄存器的中断标志，以禁止中断！   这里顺便提及一下，CPSR寄存器为Current Program Status Register，用于存储当前程序的状态信息，包括中断使能状态、处理器模式、条件标志等。\n大多数的ARM处理器，都采用CPSR寄存器来管理装填信息，所以ARM处理器可以直接进行操作。\n 2.2.3 local_irq_save #  #define IRQMASK_REG_NAME_R \u0026#34;primask\u0026#34;  #define local_irq_save(flags)\t\\ do {\t\\ raw_local_irq_save(flags);\t\\ trace_hardirqs_off();\t\\ } while (0)  #define raw_local_irq_save(flags)\t\\ do {\t\\ typecheck(unsigned long, flags);\t\\ flags = arch_local_irq_save();\t\\ } while (0)  static inline unsigned long arch_local_irq_save(void) { unsigned long flags; asm volatile( \u0026#34;\tmrs\t%0, \u0026#34; IRQMASK_REG_NAME_R \u0026#34;\t@ arch_local_irq_save\\n\u0026#34; \u0026#34;\tcpsid\ti\u0026#34; : \u0026#34;=r\u0026#34; (flags) : : \u0026#34;memory\u0026#34;, \u0026#34;cc\u0026#34;); return flags; } 函数介绍：arch_local_irq_save函数，用于保存当前中断状态并禁用中断。\n文件位置：kernel/arch/arm/include/asm/irqflags.h\n相关实现：\nmrs %0 IRQMASK_REG_NAME_R：这行代码使用mrs指令将中断屏蔽寄存器的值读取到通用寄存器%0中。IRQMASK_REG_NAME_R是一个占位符，表示要读取的中断屏蔽寄存器的名称，实际的中断屏蔽寄存器为primask。通过这行代码，中断屏蔽寄存器的值被保存到了%0寄存器中。\n: \u0026quot;=r\u0026quot; (flags) : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;: 这是一个约束部分，用于指定寄存器和内存的使用约束。\u0026quot;=r\u0026quot; (flags)表示将%0寄存器的值保存到flags变量中。\u0026quot;memory\u0026quot;和\u0026quot;cc\u0026quot;表示这段代码可能会修改内存和条件码寄存器。\n总的来说，这段代码主要实现了：\n 保存中断屏蔽寄存器的值到flags变量中，并返回 关闭本CPU的中断   2.2.4 local_irq_restore #  #define IRQMASK_REG_NAME_W \u0026#34;primask\u0026#34;  #define local_irq_restore(flags)\t\\ do {\t\\ if (raw_irqs_disabled_flags(flags)) {\t\\ raw_local_irq_restore(flags);\t\\ trace_hardirqs_off();\t\\ } else {\t\\ trace_hardirqs_on();\t\\ raw_local_irq_restore(flags);\t\\ }\t\\ } while (0)  #define raw_local_irq_restore(flags)\t\\ do {\t\\ typecheck(unsigned long, flags);\t\\ arch_local_irq_restore(flags);\t\\ } while (0)  /* * restore saved IRQ \u0026amp; FIQ state */ static inline void arch_local_irq_restore(unsigned long flags) { asm volatile( \u0026#34;\tmsr\t\u0026#34; IRQMASK_REG_NAME_W \u0026#34;, %0\t@ local_irq_restore\u0026#34; : : \u0026#34;r\u0026#34; (flags) : \u0026#34;memory\u0026#34;, \u0026#34;cc\u0026#34;); } 函数介绍：arch_local_irq_restore函数，用于恢复当前中断状态并打开中断。\n相关实现：同上\n 2.2.5 local_bh_enable #  static inline void local_bh_enable(void) { __local_bh_enable_ip(_THIS_IP_, SOFTIRQ_DISABLE_OFFSET); } void __local_bh_enable_ip(unsigned long ip, unsigned int cnt) { WARN_ON_ONCE(in_irq()); lockdep_assert_irqs_enabled(); #ifdef CONFIG_TRACE_IRQFLAGS  local_irq_disable(); #endif  /* * Are softirqs going to be turned on now: */ if (softirq_count() == SOFTIRQ_DISABLE_OFFSET) trace_softirqs_on(ip); /* * Keep preemption disabled until we are done with * softirq processing: */ preempt_count_sub(cnt - 1); if (unlikely(!in_interrupt() \u0026amp;\u0026amp; local_softirq_pending())) { /* * Run softirq if any pending. And do it in its own stack * as we may be calling this deep in a task call stack already. */ do_softirq(); } preempt_count_dec(); #ifdef CONFIG_TRACE_IRQFLAGS  local_irq_enable(); #endif  preempt_check_resched(); } EXPORT_SYMBOL(__local_bh_enable_ip); asmlinkage __visible void do_softirq(void) { __u32 pending; unsigned long flags; if (in_interrupt()) return; local_irq_save(flags); pending = local_softirq_pending(); if (pending \u0026amp;\u0026amp; !ksoftirqd_running(pending)) do_softirq_own_stack(); local_irq_restore(flags); } 函数介绍：local_bh_enable函数，启用本地的底半部bottom half处理，当中断来临时，底半部处理可以被执行。\n文件位置：kernel/include/linux/bottom_half.h\n相关实现：\n 调用__local_bh_enable_ip传入两个参数，这两个参数的作用是：  _THIS_IP_：是一个宏定义，用于获取当前的指令地址，也就是调用 local_bh_enable 函数的地方。 SOFTIRQ_DISABLE_OFFSET：是一个常量，用于指定软中断禁用的偏移量。   __local_bh_enable_ip其主要作用是：处理完软中断softirq后重新启用本地底半部bottom half处理，并检查是否需要进行进程调度。 WARN_ON_ONCE(in_irq())：判断是否处于硬件中断上下文中，如果是，则打印警告信息 lockdep_assert_irqs_enabled()：这是一个锁依赖性检查宏，用于确保在调用此函数时中断是被启用的。 if (softirq_count() == SOFTIRQ_DISABLE_OFFSET) lockdep_softirqs_on(ip)：如果当前软中断计数等于SOFTIRQ_DISABLE_OFFSET，则启用软中断。 __preempt_count_sub(cnt - 1)：减少抢占计数，这是为了防止在处理软中断时发生抢占。 do_softirq：这里表示如果有待处理的软中断，那么就调用do_softirq()函数来处理这些软中断。  in_interrupt()：判断是否处于硬中断中，如果是，则直接返回 local_irq_save：它保存并关闭本地中断，以防止在处理软中断时被其他硬中断打断。 local_softirq_pending：它获取当前待处理的软中断。 如果存在待处理的软中断，并且软中断处理线程（ksoftirqd）没有在运行，那么就在当前的栈上处理软中断。 local_irq_restore：恢复本地中断。      这里有一个疑问，大家不妨思考一下：\n中断上半部和下半部的机制，就是为了让那些紧急处理的事情放在下半部，不那么紧急或者时间较长的任务放到下半部处理，来保证系统的实时性。\n那么在这里，使能中断底半部之后，仍然执行了local_irq_save和local_irq_restore，来关闭本地硬中断，这么做是为了什么？\n 我的猜想：local_bh_disable和local_bh_enable是成对出现的，当我们关闭掉了底半部中断时，也有可能硬中断引发了多个软中断触发，在此打开的时候，此时已经就已经挂起了其他的软中断处理程序；\n如果不关闭硬中断，这时候就有可能发生嵌套，导致堆栈溢出。\n 大家不妨可以一起讨论下。\n  2.2.6 local_bh_disable #  static inline void local_bh_disable(void) { __local_bh_disable_ip(_THIS_IP_, SOFTIRQ_DISABLE_OFFSET); } static __always_inline void __local_bh_disable_ip(unsigned long ip, unsigned int cnt) { preempt_count_add(cnt); barrier(); } 函数介绍：local_bh_disable函数，增加当前进程的抢占计数，从而阻止内核抢占当前进程。\n文件位置：kernel/include/linux/bottom_half.h\n相关实现：\n preempt_count_add：增加当前进程的抢占计数 barrier：执行内存屏障，以确保抢占计数的增加在所有其他内存操作之前完成。   2.2.7 抢占计数机制 #  在local_bh_eable和local_bh_disable中，有一些抢占计数的操作，如：preempt_count_add、preempt_count_dec、preempt_count_dec等，这些作用是什么呢？\n抢占计数（preempt_count）在Linux内核中起着非常重要的作用。它主要用于防止内核抢占。\n 在Linux内核中，当一个进程正在执行内核代码时，如果发生了中断或者有更高优先级的进程需要运行，那么当前进程可能会被抢占，即暂停当前进程，转而去执行中断处理程序或者更高优先级的进程。这种机制被称为内核抢占。\n然而，有些情况下，我们不希望当前进程被抢占。例如，当一个进程正在修改一些全局数据结构时，如果这个进程被抢占，那么其他进程可能会看到这些数据结构处于不一致的状态。为了防止这种情况发生，我们可以通过增加抢占计数来禁止内核抢占。\n 当抢占计数大于0时，内核抢占被禁止。\n当抢占计数等于0时，内核抢占被允许。\n因此，我们可以通过调用preempt_count_add函数来增加抢占计数，从而禁止内核抢占，通过调用preempt_count_dec和preempt_count_dec函数来减少抢占计数，从而允许内核抢占。\n总的来说，抢占计数的作用就是用于控制内核抢占的开启和关闭，以保证内核代码的正确执行。\n  关于中断底半部机制，内容较为复杂，放在后面单独拆解！\n  3、总结 #  该篇文章，主要了解以下几点：\n 中断屏蔽的思想 中断屏蔽的好处与不足 Linux内核提供的中断屏蔽接口 中断屏蔽的底层操作的实现方式    欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":16,"href":"/docs/linux/linux_nvmem_subsystem/nvmem%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E4%BA%8Cnvmem%E9%A9%B1%E5%8A%A8%E6%A1%86%E6%9E%B6/","title":"【NVMEM子系统深入剖析】二、NVMEM驱动框架","section":"Linux NVMEM 子系统","content":"【NVMEM子系统深入剖析】二、NVMEM驱动框架 #  1、前言 #   NVMEM SUBSYSTEM，该子系统整体架构不算太大，还是比较容易去理解的，下面我们一起去一探究竟！\n NVMEM（Non Volatile Memory），该子系统主要用于实现EEPROM、Efuse等非易失存储器的统一管理。\n在早期，像EEPROM驱动是存放于/drivers/misc目录下，由于没有做到好的抽象，每次需要去访问相应内存空间，都需要去复制几乎一样的代码，去注册sysfs，这是一个相当大的抽象泄露。\nNVMEM子系统就是为了解决以往的抽象泄露问题。\n 2、驱动框架 #   该驱动框架较为简单，也适合初学者去熟悉基本的驱动框架。\n 应用层：可以通过用户空间所提供的文件节点，来读取或者修改nvmem存储器的数据。\nNVMEM 核心层：统一管理NVMEM设备，向上实现文件系统接口数据的传递，向下提供统一的注册，注销nvmem设备接口。\nNVMEM 总线驱动：注册NVMEM总线，实现NVMEM控制器的底层代码实现。\nTIP：\nnvmem子系统提供读写存储器的接口有两种，一种是通过文件系统读写，一种是在内核驱动直接读写。\n对于EEPROM，其可以进行读写操作，而对于efuse，更多用于读取密钥信息，进而判断镜像是否被篡改，在用户空间是不允许被更改的。\n这种是通过驱动提供的开放接口，直接获取指定位置的数据，详细的后面展开来说。\n 3、源码目录结构 #  ketnel │ └── driver │ │ └── nvmem │ │ │ ├──\tcore.c\t# NVMEM核心层 │ │ │ ├──\trockchip-efuse.c\t# NVMEM总线驱动  4、用户空间下的目录结构 #  我们可以在用户空间去读取/写入数据，其所在的目录：/sys/bus/nvmem/devices/dev-name/nvmem\nhexdump /sys/bus/nvmem/devices/qfprom0/nvmem 0000000 0000 0000 0000 0000 0000 0000 0000 0000 * 00000a0 db10 2240 0000 e000 0c00 0c00 0000 0c00 0000000 0000 0000 0000 0000 0000 0000 0000 0000 ... * 0001000  5、参考文章 #  [1]：https://blog.csdn.net/qq_33160790/article/details/87836614\n[2]：https://blog.csdn.net/tiantao2012/article/details/72284862\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":17,"href":"/docs/embeded_tech/embeded_interview/cpu%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/","title":"CPU体系架构","section":"嵌入式面经","content":"CPU体系架构 #  2.1 CPU体系架构有哪些？ #   我们常见的CPU架构有哪些呢？\n 如果我们熟悉Linux，那么这个问题肯定不难回答！\n我们查看内核目录下的arch子目录，就可以看到Linux所支持的处理器架构，基本属于我们常见的类型了。\n# ls ./arch alpha arc arm arm64 c6x h8300 hexagon ia64 Kconfig m68k microblaze mips nds32 nios2 openrisc parisc powerpc riscv s390 sh sparc um unicore32 x86 xtensa  准确来说，CPU处理器架构主要有以下几种类型：\n CISC（复杂指令集计算机）：CISC架构的CPU设计理念是尽可能减少程序指令的数量，以降低CPU和内存之间的通信频率。这种架构的一个显著特点是拥有大量的寄存器和复杂的指令集。Intel的x86架构就是一个典型的CISC架构 RISC（精简指令集计算机）：RISC架构的CPU设计理念是通过简化指令集来提高CPU的运行效率。这种架构的一个显著特点是拥有较少的寄存器和简单的指令集。ARM架构就是一个典型的RISC架构 MISC（中间指令集计算机）：MISC架构的CPU设计理念是在CISC和RISC之间寻找一个平衡点，既不过于复杂也不过于简单。这种架构的一个显著特点是指令集的复杂度介于CISC和RISC之间 VLIW（超长指令字计算机）：VLIW架构的CPU设计理念是通过增大指令长度来提高并行执行的可能性。这种架构的一个显著特点是指令长度远大于其他架构的CPU EPIC（显式并行指令计算）：EPIC架构的CPU设计理念是通过显式标记并行指令来提高CPU的运行效率。这种架构的一个显著特点是指令集中包含了并行执行的信息。Intel的Itanium架构就是一个典型的EPIC架构 超标量架构：超标量架构的CPU设计理念是通过在一个时钟周期内执行多条指令来提高CPU的运行效率。这种架构的一个显著特点是CPU内部包含了多个执行单元，可以同时执行多条指令 超线程技术：超线程技术是Intel公司为其部分CPU所采用的一种使单一处理器像多个逻辑处理器那样并行处理多个线程的技术 多核心架构：多核心架构的CPU设计理念是在一个CPU芯片内集成多个处理器核心，以提高并行处理能力。这种架构的一个显著特点是CPU内部包含了多个独立的处理器核心，每个核心可以独立执行指令   这里就有一个疑问，我们什么时候说RISC架构，什么时候说ARM架构，这两个有什么区别呢？\n 以ARM和RISC为例：\n ARM架构和RISC架构的主要区别在于ARM实际上是RISC的一个具体实现，而RISC则是一个更广泛的处理器设计理念。换句话说，ARM是RISC的一个子集。\n同理，X86架构是CISC的一个子集。\n 2.2 常见的问题 #  Q1：你所熟知的处理器架构有哪些？\n我们常见的处理器架构有ARM、X86、mips架构等；\n Q2：STM32属于什么架构的？\nSTM32是ST公司开发的32位微控制器集成电路，基于 ARM 的 Cortex-M 系列内核。因此，STM32 属于 ARM 架构的微控制器。\n Q3：RISC和CISC的区别是什么？\n RISC：精简指令集架构，通过简化指令集，使得大多数的操作都能够在一个指令周期内完成，提高CPU运行效率 CISC：复杂指令集架构，指令集丰富，能够完成一些较为复杂的任务，并且可以降低CPU和内存之间的通信频率，提高性能。    欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":18,"href":"/docs/linux/linux_debug/%E4%B8%80%E6%96%87%E7%A7%92%E6%87%82top%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/","title":"【一文秒懂】TOP命令详解","section":"Linux 调试工具","content":"【一文秒懂】TOP命令详解 #  1、Top命令介绍 #  Linux系统中，Top命令主要用于实时运行系统的监控，包括Linux内核管理的进程或者线程的资源占用情况。\n这个命令对所有正在运行的进程和系统负荷提供不断更新的概览信息，包括系统负载、CPU利用分布情况、内存使用、每个进程的内容使用情况等信息。\n 2、Top命令使用 #  Top的命令介绍如下：\ntop -hv|-bcHiOSs -d secs -n max -u|U user -p pid -o fld -w [cols] 常用的Top指令有：\ntop：启动top命令 top -c：显示完整的命令行 top -b：以批处理模式显示程序信息 top -S：以累积模式显示程序信息 top -n 2：表示更新两次后终止更新显示 top -d 3：设置信息更新周期为3秒 top -p 139：显示进程号为139的进程信息，CPU、内存占用率等 top -n 10：显示更新十次后退出 除此之外，在top进程运行过程中，两个最重要的功能是查看帮助（h 或 ？）和退出（q 或 Ctrl+C）。\n 3、Top信息详解 #  top展示界面由从上到下3部分组成\n 概览区域 表头 任务区域 还有一个输入/消息行，位于概览区域和表头之间。  3.1 概览区详解 #  top - 14:46:08 up 5:46, 1 user, load average: 0.00, 0.00, 0.00  程序或者窗口的名称：top 当前时间和系统的启动时间：14:46:08 up 5:46 总共的用户数量：1 user 过去1、5和15分钟的系统平均负载：load average: 0.00, 0.00, 0.00  Tasks: 290 total, 1 running, 212 sleeping, 0 stopped, 0 zombie %Cpu(s): 0.0 us, 0.1 sy, 0.0 ni, 99.9 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st 这两行显示了任务数量和CPU状态\n 第一行该信息对Task进行分类，包括running、sleeping、stopped、zombie四类，显示了系统中正在运行的任务的状态统计信息。具体来说，这里有291个任务总数，其中有1个任务正在运行，212个任务正在睡眠，0个任务已停止，0个任务为僵尸进程。 第二行显示CPU的状态百分比  %Cpu(s): CPU使用率的统计信息。 us (user): 用户空间进程占用CPU的时间百分比。 sy (system): 内核空间进程占用CPU的时间百分比。 ni (nice): 用户进程以优先级调整过的占用CPU的时间百分比（通常不会有这个值）。 id (idle): CPU空闲的时间百分比。 wa (IO-wait): CPU等待I/O操作的时间百分比。 hi (hardware interrupt): CPU处理硬件中断的时间百分比。 si (software interrupt): CPU处理软件中断的时间百分比。 st: 被虚拟化环境偷取的时间百分比（通常不会有这个值）。    KiB Mem : 3994720 total, 525876 free, 595492 used, 2873352 buff/cache KiB Swap: 2097148 total, 2096624 free, 524 used. 3114400 avail Mem 这两行表示内存的使用情况\n 第一行表示物理内存，分为total、 free、 used 、 buff/cache 第二行表示虚拟内存，分为total、free、used、avail   默认单位是KiB，使用按键E可以切换为MiB、GiB、TiB、PiB、EiB\n KiB = kibibyte = 1024 bytes MiB = mebibyte = 1024 KiB = 1,048,576 bytes GiB = gibibyte = 1024 MiB = 1,073,741,824 bytes TiB = tebibyte = 1024 GiB = 1,099,511,627,776 bytes PiB = pebibyte = 1024 TiB = 1,125,899,906,842,624 bytes EiB = exbibyte = 1024 PiB = 1,152,921,504,606,846,976 bytes   更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  3.2 任务区 #  任务区是按照列的形式来显示的，并且有多个字段可以用来查看进程的状态信息。\n3.2.1 任务字段介绍 #    %CPU： CPU Usage，自上次屏幕更新以来任务占用的CPU时间份额，表示为总CPU时间的百分比。\n  %MEM： Memory Usage，进程使用的物理内存百分比\n  CODE：Code Size，可执行代码占用的物理内存量\n  COMMAND：Command Name or Command Line，用于显示输入的命令行或者程序名称\n  PID：Process Id，任务独立的ID，即进程ID\n  PPID：Parent Process Id，父进程ID\n  UID：User Id，任务所有者的用户ID\n  USER：User Name，用户名\n  RUSER：Real User Name，实际的用户名\n  TTY：Controlling Tty，控制终端名称\n  TIME：CPU TIME，该任务CPU总共运行的时间\n  TIME+：同TIME，其粒度更细\n  OOMa：Out of Memory Adjustment Factor，内存溢出调整机制，这个字段会被增加到当前内存溢出分数中，来决定什么任务会被杀掉，范围是-1000到+1000。\n  OOMs：Out of Memory Score，内存溢出分数，这个字段是用来选择当内存耗尽时杀掉的任务，范围是0到+1000。0的意思是绝不杀掉，1000的意思是总是杀掉。\n  S：Process Status，表示进程状态信息\n D： 不可中断休眠 I：空闲 R：运行中 S：休眠 T：被任务控制信号停止 t：在跟踪期间被调试器停止 Z：僵尸     相关属性有很多，可以使用man top查看，这里先列举这些。\n  3.2.2 字段管理 #  我们输入top后，默认只显示一部分属性信息，我们可以自行管理想要的属性信息。\n我们输入F或者f，进入字段管理功能，用于选择想要的字段信息\n   按键 功能     ↑、↓ 光标上下移动选择   空格、d 切换   s 设置为排序依据字段   a、w 在4种窗口中切换：1.默认，2.任务，3.内存，4.用户   Esc键、q 退出当前窗口     4、交互命令详解 #  top的功能很多，基本能够查看进程的各种状态信息，其中还有一些交互式的命令，方便我们更好的查看系统状态。\n 在top主界面中，我们输入下面的命令\n    命令 功能     h、? 帮助信息查看，涵盖所有的快捷键   空格、回车按键 手动刷新界面信息   q、ESC按键 退出   B 粗体显示功能   d、s 改变间隔时间   E、e 切换内存显示的单位，从KiB到EiB   g 然后输入1-4其中一个数字，选择哪种窗口（1.默认，2.任务，3.内存，4.用户）   H 进程、线程显示切换   k 输入PID信息，杀掉一个任务   Z 改变配色     上面介绍了一些比较常见的交互式命令，还有更多需要你去探索哦！\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":19,"href":"/docs/linux/linux_memory_manage/%E4%BA%8C%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E5%B8%83%E5%B1%80/","title":"二、虚拟地址空间布局","section":"Linux 内存管理","content":"Linux内存管理 | 二、虚拟地址空间布局 #  上一章，我们了解了内存管理的由来以及核心思想，下面我们按照顺序，先来介绍一下Linux虚拟内存空间的管理。\n 同样，我们知道Linux内核抽象出来虚拟内存空间，主要是为了让每个进程都独享该空间，那虚拟内存空间是如何布局的呢？\n 前提：针对于不同位数的CPU，寻址能力不同，抽象出来的虚拟内存空间大小也不同，我们以常见的32位的CPU为例。\n  1、虚拟内存空间布局 #  对于32位的CPU，寻址范围为0~2^32，也就是0x00000000-0xFFFFFFFF，即最多抽象出来4G的虚拟内存空间。\n这4GB的内存空间，在Linux中，又分为用户空间和内核空间，其中0x0000000-0xBFFFFFFF，共3G为用户空间，0xC00000000-0xFFFFFFFF，共1G为内核空间，如下：\n无论内核空间还是用户空间，其仍然是在虚拟内存空间基础之上进行划分的，其直接访问的依旧都是虚拟地址，而非物理地址！\n我们编写代码后，所生成的可执行程序，运行之后就成为一个系统进程，我们在\u0026quot;虚\u0026quot;的角度来看，每个进程都是独享这4G虚拟地址空间的，\n 2、用户态空间布局 #  如上所述，用户空间在虚拟内存中分布在0x0000000-0xBFFFFFFF，大小为3G。\n每一个用户进程，按照访问属性一致的地址空间存放在一起的原则，划分成5个不同的内存区域（访问属性一致指的是：可读，可写，可执行）：\n 代码段：Text Segment，也就是我们的二进制程序，代码段需要防止在运行时被非法修改，所以该段为只读。 数据段：Data Segment，主要存放初始化了的变量，主要包括：静态变量和全局变量，该段为读写。 BSS段：BSS Segment，主要存放未初始化的全局变量，在内存中 bss 段全部置零，该段为读写。 堆段：Heap Segment，主要存放进程运行过程中动态分配的内存段，大小不固定，可动态扩张和缩减，通常使用malloc和free来分配释放，并且堆的增长方向是向上的。 文件映射和匿名映射段：Memory Mapping Segment，主要存放进程使用到的文件或者依赖的动态库，从低地址向上增长。 栈段：Stack Segment，主要存放进程临时创建的局部变量，函数调用上下文信息等，栈向下增长。  一个可执行程序，可以通过size命令，查看编译出来的可执行文件大小，其中包括了代码段，数据段等数据信息，如下:\ndonge@Donge:$ size Donge-Demo text data bss dec hex filename 12538 1916 43632 58086 e2e6 Donge-Demo  text：代码段大小 data：数据段大小 bss：bss段大小 dec：十进制表示的可执行文件大小 hex：十六进制表示的可执行文件大小   运行该程序后，可以通过cat /proc/PID/maps命令，或者pmap PID命令，来查看该进程在虚拟内存空间中的分配情况，其中PID为进程的PID号，如下:\ndonge@Donge:$ cat /proc/16508/maps 55976ff9e000-55976ffa0000 r--p 00000000 08:10 184922 /home/donge/WorkSpace/Program/Donge_Programs/Donge_Demo/build/Donge-Demo 55976ffa0000-55976ffa2000 r-xp 00002000 08:10 184922 /home/donge/WorkSpace/Program/Donge_Programs/Donge_Demo/build/Donge-Demo 55976ffa2000-55976ffa3000 r--p 00004000 08:10 184922 /home/donge/WorkSpace/Program/Donge_Programs/Donge_Demo/build/Donge-Demo 55976ffa3000-55976ffa4000 r--p 00004000 08:10 184922 /home/donge/WorkSpace/Program/Donge_Programs/Donge_Demo/build/Donge-Demo 55976ffa4000-55976ffa5000 rw-p 00005000 08:10 184922 /home/donge/WorkSpace/Program/Donge_Programs/Donge_Demo/build/Donge-Demo 55976ffa5000-55976ffaf000 rw-p 00000000 00:00 0 559771d91000-559771db2000 rw-p 00000000 00:00 0 [heap] 7fec1ad84000-7fec1ad87000 rw-p 00000000 00:00 0 7fec1ad87000-7fec1adaf000 r--p 00000000 08:10 22282 /usr/lib/x86_64-linux-gnu/libc.so.6 7fec1adaf000-7fec1af44000 r-xp 00028000 08:10 22282 /usr/lib/x86_64-linux-gnu/libc.so.6 7fec1af44000-7fec1af9c000 r--p 001bd000 08:10 22282 /usr/lib/x86_64-linux-gnu/libc.so.6 7fec1af9c000-7fec1afa0000 r--p 00214000 08:10 22282 /usr/lib/x86_64-linux-gnu/libc.so.6 7fec1afa0000-7fec1afa2000 rw-p 00218000 08:10 22282 /usr/lib/x86_64-linux-gnu/libc.so.6 7fec1afa2000-7fec1afaf000 rw-p 00000000 00:00 0 7fec1afb5000-7fec1afb7000 rw-p 00000000 00:00 0 7fec1afb7000-7fec1afb9000 r--p 00000000 08:10 22068 /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 7fec1afb9000-7fec1afe3000 r-xp 00002000 08:10 22068 /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 7fec1afe3000-7fec1afee000 r--p 0002c000 08:10 22068 /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 7fec1afef000-7fec1aff1000 r--p 00037000 08:10 22068 /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 7fec1aff1000-7fec1aff3000 rw-p 00039000 08:10 22068 /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 7ffce385d000-7ffce387e000 rw-p 00000000 00:00 0 [stack] 7ffce394e000-7ffce3952000 r--p 00000000 00:00 0 [vvar] 7ffce3952000-7ffce3953000 r-xp 00000000 00:00 0 [vdso] 上面能大致看出该进程的代码段、堆、文件映射段，栈的内存分布等情况，以上就是我们的可执行程序被加载进入内存之后，在用户态虚拟内存空间的布局情况。\n  顺便介绍一下 我的圈子：高级工程师聚集地，期待大家的加入。\n 3、内核态空间布局 #  下面我们来看一下内核态的虚拟空间布局，首先我们要知道：\n 在Linux系统中，用户进程通常只能访问用户空间的虚拟地址，只有在执行内陷操作或系统调用时才能访问内核空间。 所有的进程通过系统调用进入内核态之后，看到的虚拟地址空间都是一样的，他们是共享内核态虚拟内存空间的。   32位的内核态虚拟空间在虚拟内存中分布在0xC00000000-0xFFFFFFFF上，大小为1G，其要分为以下几个区：\n 直接映射区（Direct Memory Region）：顾名思义，直接映射区就是直接与物理内存建立一一映射关系。从内核空间起始地址开始，到896M的内核空间地址区间，为直接内存映射区，该区域线性地址和分配的物理地址都是连续的。   896M以上的内核地址空间，又称为高端内存区域。\n   安全保护区：也成为内存空洞，大小为8M，其主要目的是为了避免 非连续区的非法访问，\n  动态映射区：也就是vmalloc Region，该区域由Vmalloc函数分配，特点是：虚拟地址空间连续，但是物理地址空间不一定连续。\n  永久映射区（Persistent Kernel Mapping Region）：该区域主要用于访问高端内存，通过alloc_page (_GFP_HIGHMEM)接口分配高端内存页，可以使用kmap函数将分配到的高端内存映射到该区域。\n  固定映射区（Fixing kernel Mapping Region）：该区域虚拟内存地址可以自由映射到物理内存的高端地址上，“固定”表现在“虚拟内存空间地址是固定的”，被映射的物理地址是可变的。\n   为什么会有固定映射这个概念呢 ?\n比如：在内核的启动过程中，有些模块需要使用虚拟内存并映射到指定的物理地址上，而且这些模块也没有办法等待完整的内存管理模块初始化之后再进行地址映射。因此，内核固定分配了一些虚拟地址，这些地址有固定的用途，使用该地址的模块在初始化的时候，将这些固定分配的虚拟地址映射到指定的物理地址上去。\n 4、总结 #  以上就是整个虚拟地址空间的划分，总结如下：\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":20,"href":"/docs/linux/linux_api/linux-api-%E6%8F%AD%E7%A7%98container_of%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/","title":"【Linux API 揭秘】container_of函数详解","section":"Linux API 揭秘","content":"【Linux API 揭秘】container_of函数详解 #   Linux Version：6.6\nAuthor：Donge\nGithub：linux-api-insides\n  1、container_of函数介绍 #  container_of可以说是内核中使用最为频繁的一个函数了，简单来说，它的主要作用就是根据我们结构体中的已知的成员变量的地址，来寻求该结构体的首地址，直接看图，更容易理解。\n 下面我们看看linux是如何实现的吧\n 2、container_of函数实现 #  /** * container_of - cast a member of a structure out to the containing structure * @ptr:\tthe pointer to the member. * @type:\tthe type of the container struct this is embedded in. * @member:\tthe name of the member within the struct. * * WARNING: any const qualifier of @ptr is lost. */ #define container_of(ptr, type, member) ({\t\\ void *__mptr = (void *)(ptr);\t\\ static_assert(__same_type(*(ptr), ((type *)0)-\u0026gt;member) ||\t\\ __same_type(*(ptr), void),\t\\ \u0026#34;pointer type mismatch in container_of()\u0026#34;);\t\\ ((type *)(__mptr - offsetof(type, member))); })  函数名称：container_of\n文件位置：include/linux/container_of.h\n该函数里面包括了一些封装好的宏定义以及函数，比如：static_assert、__same_type、offsetof，以及一些指针的特殊用法，比如：(type *)0)，下面我们一一拆解来看。\n2.1 static_assert #  /** * static_assert - check integer constant expression at build time * * static_assert() is a wrapper for the C11 _Static_assert, with a * little macro magic to make the message optional (defaulting to the * stringification of the tested expression). * * Contrary to BUILD_BUG_ON(), static_assert() can be used at global * scope, but requires the expression to be an integer constant * expression (i.e., it is not enough that __builtin_constant_p() is * true for expr). * * Also note that BUILD_BUG_ON() fails the build if the condition is * true, while static_assert() fails the build if the expression is * false. */ #define static_assert(expr, ...) __static_assert(expr, ##__VA_ARGS__, #expr) #define __static_assert(expr, msg, ...) _Static_assert(expr, msg) 函数名称：static_assert\n文件位置：include/linux/build_bug.h\n函数解析：该宏定义主要用来 在编译时检查常量表达式，如果表达式为假，编译将失败，并打印传入的报错信息\n expr：该参数表示传入进来的常量表达式 ...：表示编译失败后，要打印的错误信息 _Static_assert：C11中引入的关键字，用于判断表达式expr并打印错误信息msg。  在container_of函数中，主要用来断言判断\nstatic_assert( __same_type(*(ptr), ((type *)0)-\u0026gt;member) || __same_type(*(ptr), void) , \u0026#34;pointer type mismatch in container_of()\u0026#34; );  2.2 __same_type #  /* Are two types/vars the same type (ignoring qualifiers)? */ #ifndef __same_type # define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b)) #endif 函数名称：__same_type\n文件位置：include/linux/compiler.h\n函数解析：该宏定义用于检查两个变量是否是同种类型\n __builtin_types_compatible_p：gcc的内建函数，判断两个参数的类型是否一致，如果是则返回1 typeof：gcc的关键字，用于获取变量的类型信息  了解完__same_type，想要理解__same_type(*(ptr), ((type *)0)-\u0026gt;member)，需要先弄明白(type *)0的含义。\n  更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  2.3 (type *)0 #  (type *)0，该如何理解这个表达式呢？\n 首先，type是我们传入进来的结构体类型，比如上面讲到的struct test，而这里所做的可以理解为强制类型转换：(struct test *)addr。 addr可以表示内存空间的任意的地址，我们在强制转换后，默认后面一片的内存空间存储的是该数据结构。   而(type *)0的作用，也就是默认将0地址处的内存空间，转换为该数据类型。   我们就把0，当作我们正常的addr地址变量来操作，((type *)0)-\u0026gt;member，就是获取我们结构体的成员对象。 ((type *)0)-\u0026gt;member：是一种常见的技巧，用于直接获取结构体type的成员member的类型，而不需要定义一个type类型的对象。   2.4 offsetof #  #ifndef offsetof #define offsetof(TYPE, MEMBER) ((size_t) \u0026amp;((TYPE *)0)-\u0026gt;MEMBER) #endif 函数名称：offsetof\n文件位置：include/linux/stddef.h\n函数解析：该宏定义用于获取结构体中指定的成员，距离该结构体偏移量。\n TYPE：表示结构体的类型 MEMBER：表示指定的结构体成员 __builtin_offsetof：gcc内置函数，直接返回偏移量。   在新的linux源码中，直接引用了gcc内置的函数，而在老的内核源码中，该偏移量的实现方式如下：\n#define offsetof(TYPE, MEMBER) ((size_t) \u0026amp;((TYPE *)0)-\u0026gt;MEMBER) 同样用到了((TYPE *)addr)，上面我们知道\n ((TYPE *)addr)-\u0026gt;MEMBER：表示获取该结构体的成员 \u0026amp;((TYPE *)addr)-\u0026gt;MEMBER)：加了一个\u0026amp;，表示地址，取该成员的内存地址。  比如我们addr=0x00000010，那么\u0026amp;((TYPE *)0x00000010)-\u0026gt;MEMBER)就相当于0x00000010+size 比如我们addr=0，那么\u0026amp;((TYPE *)0)-\u0026gt;MEMBER)就相当于size     到这里，我们对container_of函数内部涉及的相关知识了然于胸，下面我们再来看container_of，简直容易到起飞。\n 2.5 container_of #  #define container_of(ptr, type, member) ({\t\\ void *__mptr = (void *)(ptr);\t\\ static_assert(__same_type(*(ptr), ((type *)0)-\u0026gt;member) ||\t\\ __same_type(*(ptr), void),\t\\ \u0026#34;pointer type mismatch in container_of()\u0026#34;);\t\\ ((type *)(__mptr - offsetof(type, member))); })  static_assert：断言信息，避免我们传入的参数类型不对，而做的编译检查处理，直接忽略。  #define container_of(ptr, type, member) ({\t\\ void *__mptr = (void *)(ptr);\t\\ ((type *)(__mptr - offsetof(type, member))); })   offsetof(type, member)：计算的是结构体中的成员的偏移量，这里称为size\n  (__mptr - offsetof(type, member))：也就是根据我们已知的成员变量地址，计算出来结构体的首地址\n  ((type *)(__mptr - offsetof(type, member)))：最后强制转换为(type *)，结构体指针。\n   比如，我们已知的结构体成员的地址为0xffff0000，计算之后如下：\n 3、总结 #  linux内核中，小小的一个函数，内部包括的技巧如此之多：static_assert、__same_type、(type *)0、offsetof。\n了解完内部完整的实现手法之后，我们也可以手码一个container_of了 :)\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":21,"href":"/docs/uboot/%E4%BA%8Cuboot%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/","title":"二、uboot启动流程分析","section":"Uboot开发","content":"二、uboot启动流程分析 #   上一篇文章：（一）uboot基础了解 下一篇文章：（三）Uboot驱动模型\n 同大多数的Bootloader一样，uboot的启动过程也分为BL1、BL2两个阶段，分别对应着SPL和Uboot。\nSPL（BL1阶段）：负责开发板的基础配置和设备初始化，并且搬运Uboot到内存中，由汇编代码和少量的C语言实现\nUboot（BL2阶段）：主要负责初始化外部设备，引导Kernel启动，由纯C语言实现。\n 我们这篇文章，主要介绍Uboot（BL2阶段）的启动流程，BL1阶段启动流程的详细分析，可以见我的后续文章。想要深入了解的，可以好好研究下！\n 2.1、程序执行流程图 #  我们先总体来看一下Uboot的执行步骤，这里以EMMC作为启动介质，进行分析！\n无论是哪种启动介质，基本流程都相似，我们这就往下看！\n==打开图片，结合文档、图片、代码进行理解！==\n 2.2、u-boot.lds——Uboot的入口函数 #  u-boot.lds：是uboot工程的链接脚本文件，对于工程的编译和链接有非常重要的作用，决定了uboot的组装，并且u-boot.lds链接文件中的ENTRY(_start)指定了uboot程序的入口地址。\n 如果不知道u-boot.lds放到在哪里，可以通过find -name u-boot.lds查找，根目录要进入到uboot的源码的位置哦！\n如果查找结果有很多，结合自己的板子信息，确定自己使用的u-boot.lds。\n当然，准确的方法是查看Makefile文件，分析出来u-boot.lds所生成的位置。\n 在u-boot.lds的文件中，可以看到.text段，存放的就是执行的文本段。截取部分代码段如下：\nOUTPUT_FORMAT(\u0026#34;elf32-littlearm\u0026#34;, \u0026#34;elf32-littlearm\u0026#34;, \u0026#34;elf32-littlearm\u0026#34;) OUTPUT_ARCH(arm) ENTRY(_start) SECTIONS { . = 0x00000000;\t@起始地址  . = ALIGN(4);\t@四字节对齐  .text :\t{\t*(.__image_copy_start)\t@映像文件复制起始地址 *(.vectors)\t@异常向量表 arch/arm/cpu/armv7/start.o (.text*)\t@启动函数 } ...... }   ENTRY(_start)：程序的入口函数，_start在arch/arm/lib/vectors.S中定义.globl _start\n  SECTIONS定义了段，包括text文本段、data数据段、bss段等。\n  __image_copy_start在System.map和u-boot.map中均有定义\n  arch/arm/cpu/armv7/start.o对应文件arch/arm/cpu/armv7/start.S，该文件中定义了main函数的入口。\n   Tip：上面只进行大概分析，有汇编经验的朋友，可以详细进行分析！\n 2.3、board_init_f——板级前置初始化 #  跟随上文的程序执行流程图，我们看board_init_f这个函数。其位于common/board_f.c。\nvoid board_init_f(ulong boot_flags) { gd-\u0026gt;flags = boot_flags; gd-\u0026gt;have_console = 0; if (initcall_run_list(init_sequence_f)) hang(); } static const init_fnc_t init_sequence_f[] = { setup_mon_len, ... log_init, arch_cpu_init,\t/* basic arch cpu dependent setup */ env_init,\t/* initialize environment */ ... reloc_fdt, reloc_bootstage, reloc_bloblist, setup_reloc, ... } board_init_f()，其最核心的内容就是调用了init_sequence_f初始化序列，进行了一系列初始化的工作。\n主要包括：串口、定时器、设备树、DM驱动模型等，另外还包括global_data结构体相关对象的变量。\n 详细分析，可以看文末的参考文章[1]\n 我们需要注意的一点就是，在初始化队列末尾，执行了几个reloc_xxx的函数，这几个函数实现了Uboot的重定向功能。\n2.4、relocate_code重定向 #   重定向技术，可以说也算是Uboot的一个重点了，也就是将uboot自身镜像拷贝到ddr上的另外一个位置的动作。\n 2.4.1 为什么需要重定向呢？ #   一般需要重定向的条件如下：\n  uboot存储在只读存储器上，比如ROM、Nor flash，需要将代码拷贝到DDR上，才能完整运行Uboot。 为Kernel腾空间，Kernel一般会放在DDR的地段地址上，所以要把Uboot重定向到顶端地址，避免冲突。  2.4.2 Uboot是如何重定向的？ #  Uboot的重定向有如下几个步骤：\n 对relocate进行空间划分 计算uboot代码空间到relocate的位置的偏移 relocate旧的global_data到新的global_data空间上 relocate Uboot 修改relocate后的全局变量的label relocate中断向量表  运行大致流程：\narch/arm/lib/crt0.S文件内，主要实现了：\nENTRY(_main) bl board_init_f @@ 在board_init_f里面实现了 @@ （1）对relocate进行空间规划 @@ （2）计算uboot代码空间到relocation的位置的偏移 @@ （3）relocate旧的global_data到新的global_data的空间上 ldr sp, [r9, #GD_START_ADDR_SP] /* sp = gd-\u0026gt;start_addr_sp */  bic sp, sp, #7 /* 8-byte alignment for ABI compliance */  ldr r9, [r9, #GD_BD] /* r9 = gd-\u0026gt;bd */  sub r9, r9, #GD_SIZE /* new GD is below bd */ @@ 把新的global_data地址放在r9寄存器中 adr lr, here ldr r0, [r9, #GD_RELOC_OFF] /* r0 = gd-\u0026gt;reloc_off */  add lr, lr, r0 @@ 计算返回地址在新的uboot空间中的地址。b调用函数返回之后，就跳到了新的uboot代码空间中。 ldr r0, [r9, #GD_RELOCADDR] /* r0 = gd-\u0026gt;relocaddr */ @@ 把uboot的新的地址空间放到r0寄存器中，作为relocate_code的参数 b relocate_code @@ 跳转到relocate_code中，在这里面实现了 @@ （1）relocate旧的uboot代码空间到新的空间上去 @@ （2）修改relocate之后全局变量的label @@ 注意，由于上述已经把lr寄存器重定义到uboot新的代码空间中了，所以返回之后，就已经跳到了新的代码空间了！！！！！！ bl relocate_vectors @@ relocate中断向量表  setup_reloc——重定向地址查看（仿真有关）  在这里我们说明一下board_init_f里面的setup_reloc初始化函数\nstatic int setup_reloc(void) { if (gd-\u0026gt;flags \u0026amp; GD_FLG_SKIP_RELOC) { debug(\u0026#34;Skipping relocation due to flag\\n\u0026#34;); return 0; } #ifdef CONFIG_SYS_TEXT_BASE #ifdef ARM  gd-\u0026gt;reloc_off = gd-\u0026gt;relocaddr - (unsigned long)__image_copy_start; #elif defined(CONFIG_M68K)  /* * On all ColdFire arch cpu, monitor code starts always * just after the default vector table location, so at 0x400 */ gd-\u0026gt;reloc_off = gd-\u0026gt;relocaddr - (CONFIG_SYS_TEXT_BASE + 0x400); #elif !defined(CONFIG_SANDBOX)  gd-\u0026gt;reloc_off = gd-\u0026gt;relocaddr - CONFIG_SYS_TEXT_BASE; #endif #endif  memcpy(gd-\u0026gt;new_gd, (char *)gd, sizeof(gd_t)); debug(\u0026#34;Relocation Offset is: %08lx\\n\u0026#34;, gd-\u0026gt;reloc_off); if (is_debug_open()) { printf(\u0026#34;Relocating to %08lx, new gd at %08lx, sp at %08lx\\n\u0026#34;, gd-\u0026gt;relocaddr, (ulong)map_to_sysmem(gd-\u0026gt;new_gd), gd-\u0026gt;start_addr_sp); } return 0; } 由于，Uboot进行了重定向，所以按照常规的地址仿真的话，我们可能访问到错误的内存空间，通过setup_reloc的Relocating to %08lx打印，我们可以得到重定向后的地址，方便我们仿真。\nUboot的重定向也有相当大的一部分知识点，上面也仅仅是简单介绍了relocate的基本步骤和流程，后续看大家需要，如果大家想了解，我再补上这一部分。\n2.4.3 Uboot重定向作用 #  总之，Uboot重定向之后，把Uboot整体搬运到了高端内存区，为Kernel的加载提供空间，避免内存践踏。\n2.5、board_init_r——板级后置初始化 #   我们接着跟着流程图往下看，重定向之后，Uboot运行于新的地址空间，接着我们执行board_init_r，主要作为Uboot运行的最后初始化步骤。\n board_init_r这个函数，同样位于common/board_f.c，主要用于初始化各类外设信息\nvoid board_init_r(gd_t *new_gd, ulong dest_addr) {\tif (initcall_run_list(init_sequence_r)) hang(); /* NOTREACHED - run_main_loop() does not return */ hang(); } static init_fnc_t init_sequence_r[] = { initr_reloc, initr_reloc_global_data, board_init,\t/* Setup chipselects */ initr_dm, initr_mmc, ... run_main_loop } 与board_init_f相同，同样有一个init_sequence_r初始化列表，包括：initr_dmDM模型初始化，initr_mmcMMC驱动初始化，等等。\n最终，uboot就运行到了run_main_loop，进而执行main_loop这个函数。\n2.6、main_loop——Uboot主循环 #   该函数为Uboot的最终执行函数，无论是加载kernel还是uboot的命令行体系，均由此实现。\n void main_loop(void) { const char *s; bootstage_mark_name(BOOTSTAGE_ID_MAIN_LOOP, \u0026#34;main_loop\u0026#34;); if (IS_ENABLED(CONFIG_VERSION_VARIABLE)) env_set(\u0026#34;ver\u0026#34;, version_string); /* set version variable */ cli_init(); if (IS_ENABLED(CONFIG_USE_PREBOOT)) run_preboot_environment_command(); if (IS_ENABLED(CONFIG_UPDATE_TFTP)) update_tftp(0UL, NULL, NULL); s = bootdelay_process(); if (cli_process_fdt(\u0026amp;s)) cli_secure_boot_cmd(s); autoboot_command(s); cli_loop(); panic(\u0026#34;No CLI available\u0026#34;); } env_set：设置环境变量，两个参数分别为name和value\ncli_init：用于初始化hash shell的一些变量\nrun_preboot_environment_command：执行预定义的环境变量的命令\nbootdelay_process：加载延时处理，一般用于Uboot启动后，有几秒的倒计时，用于进入命令行模式。\ncli_loop：命令行模式，主要作用于Uboot的命令行交互。\n2.6.1 bootdelay_process #   记得对照文章开始的执行流程图哦！\n 详细解释标注于代码中\u0026hellip;\u0026hellip;\nconst char *bootdelay_process(void) { char *s; int bootdelay; bootcount_inc(); s = env_get(\u0026#34;bootdelay\u0026#34;);\t//先判断是否有bootdelay环境变量，如果没有，就使用menuconfig中配置的CONFIG_BOOTDELAY时间  bootdelay = s ? (int)simple_strtol(s, NULL, 10) : CONFIG_BOOTDELAY; if (IS_ENABLED(CONFIG_OF_CONTROL))\t//是否使用设备树进行配置  bootdelay = fdtdec_get_config_int(gd-\u0026gt;fdt_blob, \u0026#34;bootdelay\u0026#34;, bootdelay); debug(\u0026#34;### main_loop entered: bootdelay=%d\\n\\n\u0026#34;, bootdelay); if (IS_ENABLED(CONFIG_AUTOBOOT_MENU_SHOW)) bootdelay = menu_show(bootdelay); bootretry_init_cmd_timeout(); #ifdef CONFIG_POST  if (gd-\u0026gt;flags \u0026amp; GD_FLG_POSTFAIL) { s = env_get(\u0026#34;failbootcmd\u0026#34;); } else #endif /* CONFIG_POST */ if (bootcount_error()) s = env_get(\u0026#34;altbootcmd\u0026#34;); else s = env_get(\u0026#34;bootcmd\u0026#34;);\t//获取bootcmd环境变量，用于后续的命令执行  if (IS_ENABLED(CONFIG_OF_CONTROL)) process_fdt_options(gd-\u0026gt;fdt_blob); stored_bootdelay = bootdelay; return s; } 2.6.2 autoboot_command #  详细解释标注于代码中\u0026hellip;\u0026hellip;\nvoid autoboot_command(const char *s) { debug(\u0026#34;### main_loop: bootcmd=\\\u0026#34;%s\\\u0026#34;\\n\u0026#34;, s ? s : \u0026#34;\u0026lt;UNDEFINED\u0026gt;\u0026#34;); if (stored_bootdelay != -1 \u0026amp;\u0026amp; s \u0026amp;\u0026amp; !abortboot(stored_bootdelay)) { bool lock; int prev; lock = IS_ENABLED(CONFIG_AUTOBOOT_KEYED) \u0026amp;\u0026amp; !IS_ENABLED(CONFIG_AUTOBOOT_KEYED_CTRLC); if (lock) prev = disable_ctrlc(1); /* disable Ctrl-C checking */ run_command_list(s, -1, 0); if (lock) disable_ctrlc(prev);\t/* restore Ctrl-C checking */ } if (IS_ENABLED(CONFIG_USE_AUTOBOOT_MENUKEY) \u0026amp;\u0026amp; menukey == AUTOBOOT_MENUKEY) { s = env_get(\u0026#34;menucmd\u0026#34;); if (s) run_command_list(s, -1, 0); } } 我们看一下判断条件stored_bootdelay != -1 \u0026amp;\u0026amp; s \u0026amp;\u0026amp; !abortboot(stored_bootdelay\n stored_bootdelay：为环境变量的值，或者menuconfig设置的值 s：为环境变量bootcmd的值，为后续运行的指令 abortboot(stored_bootdelay)：主要用于判断是否有按键按下。如果按下，则不执行bootcmd命令，进入cli_loop 命令行模式；如果不按下，则执行bootcmd命令，跳转到加载Linux启动。  2.6.3 cli_loop #  void cli_loop(void) { bootstage_mark(BOOTSTAGE_ID_ENTER_CLI_LOOP); #ifdef CONFIG_HUSH_PARSER  parse_file_outer(); /* This point is never reached */ for (;;);\t//死循环 #elif defined(CONFIG_CMDLINE)  cli_simple_loop(); #else  printf(\u0026#34;## U-Boot command line is disabled. Please enable CONFIG_CMDLINE\\n\u0026#34;); #endif /*CONFIG_HUSH_PARSER*/} 如上代码，程序只执行parse_file_outer来处理用户的输入、输出信息。\n 好啦，基本到这里，我们已经对Uboot的启动流程了然于胸了吧！\n当然，更深层次的不建议去深入了解，有时间可以慢慢去研究。\n 大家有疑问，可以评论区交流\u0026hellip;\u0026hellip;\n参考文章：\n[1]：boadr_init_f介绍\n[2]：启动流程参考\n[3]：main_loop相关\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":22,"href":"/docs/linux/linux_driver_develop_basic/%E4%B8%80%E6%96%87%E7%A7%92%E6%87%82%E4%B8%BA%E4%BB%80%E4%B9%88linux%E5%86%85%E6%A0%B8%E4%B8%AD%E4%B8%8D%E7%BB%8F%E5%B8%B8%E4%BD%BF%E7%94%A8typedef/","title":"【一文秒懂】为什么Linux内核中不经常使用typedef","section":"Linux 驱动开发基础","content":"为什么 Linux 内核中不经常使用 typedef？ #   我们在进行Linux驱动开发过程中，有没有出现过这样的报错？\nWARNING: do not add new typedefs 不允许使用typedef！\n虽然只是一个警告，但是如果你想往开源仓库提交代码，这就是一个必优化项。\n那么，为什么Linux内核不建议使用typedef呢？\n 1、Linus Torvalds 的态度 #    \u0026gt; On Mon, 10 Jun 2002, Linus Torvalds wrote: \u0026gt; \u0026gt; \u0026ndash;snip/snip \u0026gt; \u0026gt; But in the end, maintainership matters. I personally don\u0026rsquo;t want the \u0026gt; \u0026gt; typedef culture to get the upper hand, but I don\u0026rsquo;t mind a few of them, and \u0026gt; \u0026gt; people who maintain their own code usually get the last word.\n\u0026gt; \u0026gt; to sum it up: \u0026gt; \u0026gt; using the \u0026ldquo;struct mystruct\u0026rdquo; is recommended, but not a must.\n Torvalds 本人不太想看到typedef文化占上风，但是维护自己代码的人通常有最后的发言权。\n Torvalds 还是比较推荐使用struct mystruct的结构 不易理解：使用typedef类型，不容易去理解变量的实际类型是什么样子的 不好维护：由于Linux内核架构的庞大，不同架构之间定义的typedef类型可能并不具有通用性。  Torvalds 原文详见：https://lkml.indiana.edu/hypermail/linux/kernel/0206.1/0402.html\n 2、内核编码规范 #   从内核编码规范的角度，来看typedef\n 内核编码规范给出了typedef使用的一些场合：\n 完全不透明的对象：隐藏内部对象 明确的整数类型：抽象有助于避免混淆是int型还是long型，如u8/u16/u32 在某些特殊情况下，与标准C99类型相同的新类型。 可在用户空间中使用的类型  内核编码规范详见：https://www.kernel.org/doc/html/v4.10/process/coding-style.html\n 3、个人看法 #  个人感觉，从大型项目的开发维护上来说，typedef不建议使用，避免造成类型泛滥，也更加不容易理解。\n对于个人开发的小项目，typedef可以完全看自己心情，毕竟typedef褒贬不一。\n 下面分享一些社区讨论帖子：\n 为什么我们要在C语言中频繁使用typedef：https://stackoverflow.com/questions/252780/why-should-we-typedef-a-struct-so-often-in-c 为什么Linux编码锋哥不建议使用typedef：https://www.reddit.com/r/C_Programming/comments/dan8vr/why_does_the_linux_kernel_coding_style_guide/?rdt=36702    欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":23,"href":"/docs/linux/linux_debug/%E4%B8%80%E6%96%87%E7%A7%92%E6%87%82linux%E5%86%85%E6%A0%B8%E8%B0%83%E8%AF%95%E5%B7%A5%E5%85%B7debugfs/","title":"【一文秒懂】Linux内核调试工具——Debugfs","section":"Linux 调试工具","content":"【一文秒懂】Linux内核调试工具——Debugfs #  1、介绍 #  Debugfs其存在的主要意义是为了内核开发者向用户空间传递更多有用的信息，与proc不同，proc只提供进程相关的信息；同时也与sysfs不同，sysfs对每个文件都要求一定的规则，而Debugfs没有任何的规则。\n简而言之，Debugfs是一种用于内核调试的虚拟文件系统。\n2、如何调试 #  2.1 配置Debugfs #  进去menuconfig选项中，按下/搜索CONFIG_DEBUG_FS关键词即可！\n 当然，可以看Location在内核中的位置。\n 2.2 挂载Debugfs #  mount -t debugfs none /sys/kernel/debug\t#挂载 mount\t#查看挂载情况 ___ none on /sys/kernel/debug type debugfs (rw,relatime) 2.3 GPIO调试 #  cat gpio gpio-43 ( |wakeup ) in lo IRQ gpio-64 ( |cd ) in lo IRQ  上述只是简单的调试GPIO的方法，而Debugfs功能远不止于此，其提供了一些API接口，方便我们在内核中Debug使用。\n而我们要做的，就是在我们想要进行Debug的地方，注册debugfs接口，然后查看我们要调试的信息。\n 2.4 GPIO的实现 #  文件kernel\\drivers\\gpio\\gpiolib.c中\n`static const struct file_operations gpiolib_operations = { .owner\t= THIS_MODULE, .open\t= gpiolib_open, .read\t= seq_read, .llseek\t= seq_lseek, .release\t= seq_release, }; static int __init gpiolib_debugfs_init(void) { /* /sys/kernel/debug/gpio */ (void) debugfs_create_file(\u0026#34;gpio\u0026#34;, S_IFREG | S_IRUGO, NULL, NULL, \u0026amp;gpiolib_operations); return 0; } subsys_initcall(gpiolib_debugfs_init);` 3、如何使用 #  3.1 使用步骤 #   想要使用Debugfs功能，首先要做的就是要包含 \u0026lt;linux/debugfs.h\u0026gt;头文件 使用debugfs_create_dir接口，创建一个文件夹，用于保存debugfs所操作的文件 使用debugfs_create_file接口，创建多个文件进行操作  3.2 接口介绍 #   debugfs_create_dir  struct dentry *debugfs_create_dir(const char *name, struct dentry *parent); name：文件夹名称\nparent：父目录，如果为NULL，则在root根目录下\n debugfs_create_file  struct dentry *debugfs_create_file(const char *name, umode_t mode, struct dentry *parent, void *data, const struct file_operations *fops); name：文件名\nmode ：文件访问权限\nparent：父目录，用于保存该文件\ndata：保存一些数据等\nfops：文件操作接口\n 一些类似的接口  void debugfs_create_u8(const char *name, umode_t mode,\tstruct dentry *parent, u8 *value);\t//创建一个文件，表示一个u8的值 void debugfs_create_u16(const char *name, umode_t mode, struct dentry *parent, u16 *value);\t//创建一个文件，表示一个u16的值 void debugfs_create_u32(const char *name, umode_t mode, struct dentry *parent, u32 *value); void debugfs_create_u64(const char *name, umode_t mode, struct dentry *parent, u64 *value); void debugfs_create_bool(const char *name, umode_t mode, struct dentry *parent, bool *value); 4、demo分享 #  #include \u0026lt;linux/module.h\u0026gt;#include \u0026lt;linux/kernel.h\u0026gt;#include \u0026lt;linux/debugfs.h\u0026gt; static struct dentry *dir = NULL; static unsigned int debugfs_hello; static u32 sum = 0; static int add_write(void *data, u64 value) { sum += value; return 0; } DEFINE_SIMPLE_ATTRIBUTE(add_ops, NULL, add_write, \u0026#34;%llu\\n\u0026#34;); static __init int hello_init(void) { struct dentry *tmp_dir = NULL; /* create /sys/kernel/debug/debugfs_hello/ directory */ dir = debugfs_create_dir(\u0026#34;debugfs_hello\u0026#34;, NULL); if (!dir) { printk(KERN_ALERT \u0026#34;debugfs_create_dir failed\\n\u0026#34;); return -1; } /* create /sys/kernel/debug/debugfs_hello/hello value, mode: rw*/ tmp_dir = debugfs_create_u32(\u0026#34;hello\u0026#34;, 00666, dir, \u0026amp;debugfs_hello); if (!tmp_dir) { printk(KERN_ALERT \u0026#34;debugfs_create_u32 failed\\n\u0026#34;); return -1; } /* create /sys/kernel/debug/debugfs_hello/add value, mode: w*/ tmp_dir = debugfs_create_file(\u0026#34;add\u0026#34;, 0222, dir, NULL, \u0026amp;add_ops); if (!tmp_dir) { printk(KERN_ALERT \u0026#34;debugfs_create_file failed\\n\u0026#34;); return -1; } /* create /sys/kernel/debug/debugfs_hello/sum value, mode: r*/ tmp_dir = debugfs_create_u32(\u0026#34;sum\u0026#34;, 0444, dir, \u0026amp;sum); if (!tmp_dir) { printk(KERN_ALERT \u0026#34;debugfs_create_u32 failed\\n\u0026#34;); return -1; } return 0; } static void __exit hello_exit(void) { printk(KERN_INFO \u0026#34;Exit debugfs_hello module\\n\u0026#34;); debugfs_remove_recursive(dir); dir = NULL; } module_init(hello_init); module_exit(hello_exit); MODULE_LICENSE(\u0026#34;Dual BSD/GPL\u0026#34;); MODULE_DESCRIPTION(\u0026#34;Debugfs hello examle\u0026#34;); 5、参考文章 #  [1]：https://www.kernel.org/doc/html/latest/filesystems/debugfs.html\n[2]：https://xuesong.blog.csdn.net/article/details/114383866\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":24,"href":"/docs/linux/linux_mmc_subsystem/mmc%E5%AD%90%E7%B3%BB%E7%BB%9F%E4%B8%89mmc%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%A1%86%E6%9E%B6/","title":"【MMC子系统】三、MMC子系统框架","section":"Linux MMC 子系统","content":"【MMC子系统】三、MMC子系统框架 #  上章，我们简单了解了EMMC协议，感兴趣的可以查阅一下SD和SDIO的协议，之所以Linux内核能够对SD、SDIO、EMMC进行统一管理，根本原因就是三者协议上的相似性，我们该系列文章均以EMMC为剑，一层层划开包裹着的盔甲。\n 本系列文章，均以Linux 4.19为参考\n  1、MMC子系统框架 #  如上图所示，MMC子系统的整体框架包括：MMC Host、MMC Core、MMC Block。我们从下网上看：\n MMC HOST：即MMC控制器驱动层，正如其名，该层主要是为了实现MMC控制器的初始化，以及MMC底层的数据收发操作，其直接控制的是底层寄存器，用以产生相应的通信时序。 MMC CORE：即MMC核心层，该层主要起到了承上启下的作用。对下，主要体现在注册MMC总线，实现对MMC device和MMC driver的统一管理；对上，体现在实现MMC通信协议，并向上提供相应的读写操作接口。 MMC BLOCK：即MMC块设备驱动层，其主要作用是屏蔽底层的实现逻辑，将底层抽象为卡设备，并且与虚拟文件系统关联，负责块设备请求的处理以及请求队列的管理，又称为card卡驱动。   哈哈，简单吧，我们刚开始对MMC子系统框架就先了解这么多，不着急，慢慢来。\n  2、MMC子系统文件结构 #  了解完MMC子系统后，我们看一下MMC驱动在Linux下的目录结构，我们进入到drivers/mmc目录\ndrivers/mmc/ ├── core ├── block.c ├── bus.c ├── core.c ├── mmc.c ├── mmc_ops.c ├── ...... ├── host ├── sunxi-mmc.c ├── ......  这里介绍一个方法\n 如果刚接触的朋友，不知道文件之间的关系是怎么样的，可以通过Makefile和Kconfig文件来大致看一下。\nobj-$(CONFIG_MMC)\t+= mmc_core.o mmc_core-y\t:= core.o bus.o host.o \\ \tmmc.o mmc_ops.o sd.o sd_ops.o \\ \tsdio.o sdio_ops.o sdio_bus.o \\ \tsdio_cis.o sdio_io.o sdio_irq.o \\ \tslot-gpio.o 由上面可知，MMC CORE核心层，包括的文件有：core.c、bus.c等等，\n  更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  了解大致的文件布局后，我们主要介绍一下文件的作用：\nMMC HOST控制器驱动层：\n sunxi-mmc.c：所有的控制器驱动文件，均位于host文件夹下，主要是不同芯片厂家对MMC控制器的底层实现，一般非原厂人员不必深入研究。  MMC CORE核心层：\n bus.c：主要作用是为了注册MMC总线，实现对MMC drivers和devices的统一管理 core.c：主要作用是实现MMC通信协议，封装MMC通信命令，完成MMC核心功能，并向上提供操作接口。 mmc_ops.c：主要作用是提供操作MMC卡的接口函数，如：发送，接收数据接口，该文件会用到core.c提供的命令接口。 mmc.c：主要作用是处理MMC卡的相关操作，包括识别卡，读写卡等等，该文件会用到mmc_ops.c和core.c提供的命令接口。  MMC BLOCK块设备驱动层：\n block.c：处理MMC卡的块设备接口，包括读写块，处理块设备的电源和时钟等。   3、MMC设备在Linux下的文件分布 #  介绍完源代码中的目录结构，我们看一下成功加载MMC驱动后，在Linux文件系统中的目录结构。\n在进程文件系统中：\n 我们的MMC Card被成功识别后，会在procfs中看到mmc的相关节点：如/dev/mmcblk0，其中看到/dev/mmcblk0p1表示磁盘中的一个分区  在虚拟文件系统中：\n 同时在sysfs中也会看到对应的节点\n  在/sys/bus/mmc/devices/下，我们能够看到识别到的设备信息 在/sys/class/mmc_host/mmc0/下也能够看到设备信息 同时在/sys/block/mmcblk0/下，能够看到该卡设备的信息，以及分区信息等。  # ls /sys/bus/mmc/devices/ mmc0:aaaa # ls /sys/class/mmc_host/mmc0/ device mmc0:aaaa power subsystem uevent # ls /sys/block/mmcblk0/ alignment_offset ext_range mq size bdi force_ro power slaves capability hidden queue stat dev holders range subsystem device inflight removable uevent discard_alignment mmcblk0p1 ro  4、总结 #  本章主要介绍了三个部分：\n MMC子系统的框架 MMC子系统的文件结构 MMC设备在Linux下的文件分布  下章，我们来详细了解MMC子系统的代码实现部分。\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":25,"href":"/docs/linux/bluetooth/bluetooth%E8%93%9D%E7%89%99%E5%BC%80%E5%8F%91%E4%B8%89%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0%E5%B8%A6%E4%BD%A0%E6%80%BB%E8%A7%88%E8%93%9D%E7%89%99%E5%8D%8F%E8%AE%AE/","title":"【Bluetooth蓝牙开发】三、一篇文章，带你总览蓝牙协议","section":"Bluetooth蓝牙开发","content":"【Bluetooth|蓝牙开发】三、一篇文章，带你总览蓝牙协议 #  1、前言 #  在我们上一章节，学习了蓝牙的基础概念，发展历程，以及常见的蓝牙架构，相信大家对蓝牙也有了一定的了解！\n为了更好的去踏入蓝牙开发的大门，蓝牙协议栈是一个我们不得不去跨越的门槛！\n 蓝牙协议及其复杂，并非一文能够道尽，本篇文章主要在于对蓝牙整体的协议架构进行梳理，文末官方协议附下载链接。\n  2、蓝牙芯片架构 #  蓝牙的核心架构，由一个Host和一个或多个Controller组成。\n BT Host：一个逻辑实体，在HCI（Host Controller Interface）的上层。 BT Controller：一个逻辑实体，在HCI（Host Controller Interface）的下层。  Bluetooth的主控制器，可能是以下几种：\n BR/EDR Controller：内部包含Radio, Baseband，Link Manager，可选的HCI。 LE Controller ：内部包含LE PHY，Link Layer ，可选的HCI BR/EDR \u0026amp; LE Controller ：BR/EDR与LE的组合的控制器 MAC/PHY (AMP) Controller：二级控制器，可替代的，内部包含 802.11 PAL (Protocol Adaptation Layer)，802.11 MAC，PHY，可选的HCI。   根据Host与Controller的组成关系，常见的蓝牙芯片也分为以下几种架构：\n 单模蓝牙芯片：单一传统蓝牙的芯片，单一低功耗蓝牙的芯片。即（1个Host结合1个Controller） 双模蓝牙芯片：同时支持传统蓝牙和低功耗蓝牙的芯片。即（1个Host结合多个Controller）  如下图：\n 3、蓝牙协议架构——视角1 #  ==上图为官方协议中所提及的图片，由全局到局部来看==\n3.1 全局分析 #   由下到上分析\n Controller：\n BR/EDR Controller：由BR/EDR Radio、Link Controller、Link Manager组成 LE Controller：由 LE Radio 、Link Controller、Link Manager组成 AMP Controller：由, AMP PHY 、AMP MAC, 、AMP PAL组成  Host：\n BR/EDR Host：由 L2CAP、SDP 、GAP 组成 LE Host：由 L2CAP、SMP 、GAP 、Attribute protocol、GATT组成   5.3.2 局部分析 #   由上到下分析\n Host层 #   Channel Manager：通道管理，主要用于创建、管理、关闭L2CAP通道，用于服务协议和应用数据的传输。 L2CAP Resource Manage：L2CAP资源管理，主要负责管理分片的PDU的正确提交。 Security Manager Protocol：SMP安全管理协议，主要负责生成加密密钥和身份密钥。 Attribute Protocol：ATT，属性协议，主要负责服务端与客户端点到点的数据传输。 AMP Manager Protocol：直接使用L2CAP与远程设备通信。 Generic Attribute Profile：GATT，提供更多的功能，概要文件描述了属性服务器中使用的服务层次结构、特征和属性，用于LE设备 Generic Access Profile：GAP，标识了基础的蓝牙设备的通用功能  Controller层 #   Device Manager：控制蓝牙设备的通用行为，负责与蓝牙通信过程中，所有的与数据无关的操作，如查询设备，连接设备 Link Manager：链路管理，主要负责创建，修改，释放逻辑链路。 Baseband Resource Manager：基带资源管理，主要负责所有的访问无线电媒体 Link Controller：链路控制，主要负责从编码和解码蓝牙数据包 PHY：物理层，主要负责发送，接收物理通道的信息包  以上为官方手册提供的视图，Host通过HCI（Host Controll Interface）接口，来控制Controller执行相应的动作。\n 4、蓝牙协议架构——视角2 #  下面是参考网上的一位博主的文章，写的较为详细，遂分享出来。\n以上架构图，将蓝牙协议分为了HW层，Transport层，Host层。\n 4.1 HW层——蓝牙芯片层 #  HW层，指的是蓝牙芯片层，也就是我们上面说的Controller，包括以下几个部分：\n  RF（RADIO）：射频层，本地蓝牙数据通过射频发送给远端设备，并且通过射频接收来自远端蓝牙设备的数据。\n  BB（BASEBAND）：基带层，进行射频信号与数字或语音信号的相互转化，实现基带协议和其它的底层连接规程。\n  LMP（LINK MANAGER PROTOCOL）：链路管理层，负责管理蓝牙设备之间的通信，实现链路的建立、验证、链路配置等操作\n  HCI（HOST CONTROLLER INTERFACE）：主机控制器接口层，HCI层在芯片以及协议栈都有，芯片层面的HCI负责把协议栈的数据做处理，转换为芯片内部动作，并且接收到远端的数据，通过HCI上报给协议栈。\n  BLE PHY：BLE的物理层，通信通道的划分。\n  BLE LL：BLE的链路层，对通信通道的复用以及逻辑划分。\n   4.2 Transport——数据传输层 #  Transport层，主机控制层接口，通过硬件接口UART/USB/SDIO把HOST协议层的数据发送给Controller层，并且接收Controller层的数据。\n该部分有几个协议：\n H2：基于USB的传输 H4：基于UART的传输，最简单的传输方式，只在HCI raw data前面加上一个type，基于流控 H5: 基于UART的传输，普通模式下即可传输。 BCSP: 基于UART的传输 SDIO ：基于SDIO的传输  H4需要蓝牙芯片的UART_TX/UART_RX/UART_CTS/UART_RTS/VCC/GND接到MCU；而H5只需要蓝牙芯片的UART_TX/UART_RX/VCC/GND接到MCU就可以通信。\n 4.3 HOST——协议层 #  HOST层，此部分就是蓝牙协议栈，该部分包括多个协议：\n L2CAP（Logical Link Control and Adaptation Protocol）：逻辑链路控制与适配协议，将ACL数据分组，对高层应用的数据进行分组，并提供协议复用和服务质量交换等功能。通过协议多路复用、分段重组操作和组概念,向高层提供面向连接的和无连接的数据服务   SDP（SERVICE DISCOVERY PROTOCOL）：服务发现协议，为应用程序提供发现可用服务，并确定服务特征的方法。    RFCOMM（Serial Port Emulation）：串口仿真协议，上层协议蓝牙电话，蓝牙透传SPP等协议都是直接走的RFCOMM\n  OBEX：对象交换协议，蓝牙电话本，蓝牙短信，文件传输等协议都是走的OBEX\n  HFP（Hands-Free）：蓝牙免提协议\n  HSP：蓝牙耳机协议，最开始的蓝牙耳机协议，目前已经没有产品在用这个了吧，至少我没有看到了。算是一个简化版的HFP。\n  SPP（SERIAL PORT PROFILE）：蓝牙串口协议\n  IAP：苹果的特有协议，分为IAP1/IAP2，一般做Carplay或者iPod功能的人肯定接触过这块，有需要这块的私下联系我\n  PBAP（Phone Book Access）：蓝牙电话本访问协议\n  MAP（MESSAGE ACCESS PROFILE）：蓝牙短信访问协议\n  HID（HUMAN INTERFACE DEVICE）：人机接口协议，HID还是有很多广泛的用途的，比如蓝牙鼠标，蓝牙键盘，蓝牙自拍杆，蓝牙手柄等。\n  A2DP（Advanced Audio Distribution）: 蓝牙音乐协议\n  SM: 蓝牙BLE安全管理协议\n  GAP（GENERIC ACCESS PROFILE）：它定义了蓝牙设备的基本要求。\n 对于BR/EDR，它定义了一个蓝牙设备，包括无线电、基带、链路管理器、L2CAP和服务发现协议功能。 对于LE，它定义一个物理层，链路层，L2CAP，安全管理器，属性协议和通用属性配置文件。  它联系了所有的不同的层之间的交互，也描述了设备发现、建立连接、安全、认证、关联模型和发现服务的行为和方法。\n   ATT（Attribute Protocol）：蓝牙属性协议,用于发现、读、写对端设备的协议(针对BLE设备),ATT允许设备作为服务端提供拥有关联值的属性集 ，让作为客户端的设备来发现、读、写这些属性；同时服务端能主动通知客户端。 GATT（Generic Attribute Profile）：蓝牙通用属性协议，描述了一种使用ATT的服务框架 ，该框架定义了数据交换的格式。   5、总结时刻 #  蓝牙芯片的架构：根据Host与Controller的结合关系，可以分为单模芯片和双模芯片。\n蓝牙协议的架构：蓝牙协议分为三层，即：Host层，Transport层，Controller层。每一层又由多种不同的协议组成。\n本篇文章先进行初步了解，后续会逐步深入讲解！\n 6、参考文章\u0026amp;下载地址 #  [1]：参考文章\n[2]：蓝牙协议下载地址\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":26,"href":"/docs/linux/linux_led_subsystem/led%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90%E4%B8%89%E7%A1%AC%E4%BB%B6%E9%A9%B1%E5%8A%A8%E5%B1%82%E8%AF%A6%E8%A7%A3/","title":"【LED子系统深度剖析】三、硬件驱动层详解","section":"Linux LED子系统","content":"【LED子系统深度剖析】三、硬件驱动层详解 #   上篇文章我们了解了子系统的框架，下面我们来分析驱动框架中每层的实现以及作用。\n 在LED子系统中，硬件驱动层相关文件在包括：kernel/drivers/leds/ 目录下，其主要的函数有：led-gpio.c、led-xxx.c，其中led-gpio.c为通用的平台驱动程序，led-xxx.c为不同厂家提供的平台驱动程序。\n 我们在这里主要分析led-gpio.c\n 1、gpio_led_probe分析 #  打开该文件，直接找到加载驱动的入口函数gpio_led_probe\n1.1 相关数据结构 #  1.1.1 gpio_led_platform_data #  struct gpio_led_platform_data { int num_leds; const struct gpio_led *leds; #define GPIO_LED_NO_BLINK_LOW\t0\t/* No blink GPIO state low */#define GPIO_LED_NO_BLINK_HIGH\t1\t/* No blink GPIO state high */#define GPIO_LED_BLINK\t2\t/* Please, blink */ gpio_blink_set_t\tgpio_blink_set; }; 结构体名称：gpio_led_platform_data\n文件位置：include/linux/leds.h\n主要作用：LED的平台数据，用于对LED硬件设备的统一管理\n 这个结构体用于父节点向子节点传递的数据时使用\n 1.1.2 gpio_leds_priv #  struct gpio_leds_priv { int num_leds; struct gpio_led_data leds[]; }; 结构体名称：gpio_leds_priv\n文件位置：drivers/leds/leds-gpio.c\n主要作用：LED驱动的私有数据类型，管理全部的LED设备。\n 这里的num_leds通过解析设备树的子节点的个数来获取\nleds[]根据获取的num_leds个数，分配对应的空间，来初始化相关数据\n 1.2 实现流程 #  static int gpio_led_probe(struct platform_device *pdev) { struct gpio_led_platform_data *pdata = dev_get_platdata(\u0026amp;pdev-\u0026gt;dev);\t//\t检索设备的平台数据  struct gpio_leds_priv *priv; int i, ret = 0; if (pdata \u0026amp;\u0026amp; pdata-\u0026gt;num_leds) {\t//\t判断平台数据LED数量  priv = devm_kzalloc(\u0026amp;pdev-\u0026gt;dev, sizeof_gpio_leds_priv(pdata-\u0026gt;num_leds), GFP_KERNEL); if (!priv) return -ENOMEM; priv-\u0026gt;num_leds = pdata-\u0026gt;num_leds; for (i = 0; i \u0026lt; priv-\u0026gt;num_leds; i++) { ret = create_gpio_led(\u0026amp;pdata-\u0026gt;leds[i], \u0026amp;priv-\u0026gt;leds[i], \u0026amp;pdev-\u0026gt;dev, NULL, pdata-\u0026gt;gpio_blink_set); if (ret \u0026lt; 0) return ret; } } else { priv = gpio_leds_create(pdev);\t//\t创建LED设备\t if (IS_ERR(priv)) return PTR_ERR(priv); } platform_set_drvdata(pdev, priv); return 0; } 函数介绍：gpio_led_probe是LED驱动的入口函数，也是LED子系统中，硬件设备和驱动程序匹配后，第一个执行的函数。\n实现思路：\n 通过dev_get_platdata检索设备的平台数据，如果平台数据中的LED数量大于零，则使用devm_kzalloc为其分配内存空间，并且使用create_gpio_led进行初始化 如果平台数据不存在或LED的数量为零，则使用gpio_leds_create创建LED。 最后，设置驱动程序数据，并返回0，表示操作成功。  数据结构：该函数主要包括了两个数据结构gpio_led_platform_data和gpio_leds_priv\n2、gpio_leds_create分析 #  2.1 相关数据结构 #  2.1.1 gpio_led #  /* For the leds-gpio driver */ struct gpio_led { const char *name;\t// LED名称  const char *default_trigger;\t// 默认触发类型\t unsigned gpio;\t// GPIO编号  unsigned\tactive_low : 1;\t// 低电平有效  unsigned\tretain_state_suspended : 1; unsigned\tpanic_indicator : 1; unsigned\tdefault_state : 2;\t// 默认状态  unsigned\tretain_state_shutdown : 1; /* default_state should be one of LEDS_GPIO_DEFSTATE_(ON|OFF|KEEP) */ struct gpio_desc *gpiod;\t// GPIO Group }; 结构体名称：gpio_led\n文件位置：include/linux/leds.h\n主要作用：LED的硬件描述结构，包括名称，GPIO编号，有效电平等等信息。\n 该结构体的信息大多由解析设备树获得，将设备树中label解析为name，gpios解析为gpiod，linux,default-trigger解析为default_trigger等\n 2.1.2 gpio_led_data #  struct gpio_led_data { struct led_classdev cdev;\t// LED Class  struct gpio_desc *gpiod;\t// GPIO description  u8 can_sleep;\tu8 blinking;\t// 闪烁  gpio_blink_set_t platform_gpio_blink_set;\t// 闪烁设置 }; 结构体名称：gpio_led_data\n文件位置：drivers/leds/leds-gpio.c\n主要作用：LED相关数据信息，主要在于led_classdev，用于注册设备节点信息\n 由设备树解析出来的gpio_led，然后将部分属性赋值到gpio_led_data中，并且初始化led_classdev相关属性，并且实现led_classdev结构体中的部分函数。\n 2.2 实现流程 #  static struct gpio_leds_priv *gpio_leds_create(struct platform_device *pdev) { struct device *dev = \u0026amp;pdev-\u0026gt;dev; struct fwnode_handle *child; struct gpio_leds_priv *priv; int count, ret; count = device_get_child_node_count(dev);\t//\t获取子节点数量  if (!count) return ERR_PTR(-ENODEV); priv = devm_kzalloc(dev, sizeof_gpio_leds_priv(count), GFP_KERNEL); if (!priv) return ERR_PTR(-ENOMEM); device_for_each_child_node(dev, child) { struct gpio_led_data *led_dat = \u0026amp;priv-\u0026gt;leds[priv-\u0026gt;num_leds];\t//\t与gpio_leds_priv结构体关联  struct gpio_led led = {}; const char *state = NULL; struct device_node *np = to_of_node(child); ret = fwnode_property_read_string(child, \u0026#34;label\u0026#34;, \u0026amp;led.name);\t//\t读设备树属性，赋值gpio_led结构体  if (ret \u0026amp;\u0026amp; IS_ENABLED(CONFIG_OF) \u0026amp;\u0026amp; np) led.name = np-\u0026gt;name; if (!led.name) { fwnode_handle_put(child); return ERR_PTR(-EINVAL); } led.gpiod = devm_fwnode_get_gpiod_from_child(dev, NULL, child, GPIOD_ASIS, led.name); if (IS_ERR(led.gpiod)) { fwnode_handle_put(child); return ERR_CAST(led.gpiod); } fwnode_property_read_string(child, \u0026#34;linux,default-trigger\u0026#34;, \u0026amp;led.default_trigger); if (!fwnode_property_read_string(child, \u0026#34;default-state\u0026#34;, \u0026amp;state)) { if (!strcmp(state, \u0026#34;keep\u0026#34;)) led.default_state = LEDS_GPIO_DEFSTATE_KEEP; else if (!strcmp(state, \u0026#34;on\u0026#34;)) led.default_state = LEDS_GPIO_DEFSTATE_ON; else led.default_state = LEDS_GPIO_DEFSTATE_OFF; } if (fwnode_property_present(child, \u0026#34;retain-state-suspended\u0026#34;)) led.retain_state_suspended = 1; if (fwnode_property_present(child, \u0026#34;retain-state-shutdown\u0026#34;)) led.retain_state_shutdown = 1; if (fwnode_property_present(child, \u0026#34;panic-indicator\u0026#34;)) led.panic_indicator = 1; ret = create_gpio_led(\u0026amp;led, led_dat, dev, np, NULL);\t//\t将gpio_led结构体、gpio_led_data关联起来  if (ret \u0026lt; 0) { fwnode_handle_put(child); return ERR_PTR(ret); } led_dat-\u0026gt;cdev.dev-\u0026gt;of_node = np; priv-\u0026gt;num_leds++; } return priv; } 函数介绍：gpio_leds_create主要用于创建LED设备。\n实现思路：\n 通过device_get_child_node_count获取设备树中LED子节点的数量，根据获取到的子节点数量，分配LED设备对应的内存空间 通过device_for_each_child_node遍历每个子节点，并为每个子节点创建对应的LED设备 对于每个子节点，使用fwnode_property_read_string接口，读取设备树中相关的属性信息，如：label、linux,default-trigger等，将这些信息赋值给gpio_led结构体中 最后将遍历的每个LED，调用create_gpio_led进行设备的创建  3、create_gpio_led分析 #  3.1 相关数据结构 #  3.1.1 led_classdev #   该数据结构属于核心层，在硬件驱动层需要与其进行关联，遂在此介绍。\n struct led_classdev { const char\t*name; enum led_brightness\tbrightness; enum led_brightness\tmax_brightness; int\tflags; /* Lower 16 bits reflect status */ #define LED_SUSPENDED\tBIT(0) #define LED_UNREGISTERING\tBIT(1)  /* Upper 16 bits reflect control information */ #define LED_CORE_SUSPENDRESUME\tBIT(16) #define LED_SYSFS_DISABLE\tBIT(17) #define LED_DEV_CAP_FLASH\tBIT(18) #define LED_HW_PLUGGABLE\tBIT(19) #define LED_PANIC_INDICATOR\tBIT(20) #define LED_BRIGHT_HW_CHANGED\tBIT(21) #define LED_RETAIN_AT_SHUTDOWN\tBIT(22)  /* set_brightness_work / blink_timer flags, atomic, private. */ unsigned long\twork_flags; #define LED_BLINK_SW\t0 #define LED_BLINK_ONESHOT\t1 #define LED_BLINK_ONESHOT_STOP\t2 #define LED_BLINK_INVERT\t3 #define LED_BLINK_BRIGHTNESS_CHANGE 4 #define LED_BLINK_DISABLE\t5  /* Set LED brightness level * Must not sleep. Use brightness_set_blocking for drivers * that can sleep while setting brightness. */ void\t(*brightness_set)(struct led_classdev *led_cdev, enum led_brightness brightness); /* * Set LED brightness level immediately - it can block the caller for * the time required for accessing a LED device register. */ int (*brightness_set_blocking)(struct led_classdev *led_cdev, enum led_brightness brightness); /* Get LED brightness level */ enum led_brightness (*brightness_get)(struct led_classdev *led_cdev); /* * Activate hardware accelerated blink, delays are in milliseconds * and if both are zero then a sensible default should be chosen. * The call should adjust the timings in that case and if it can\u0026#39;t * match the values specified exactly. * Deactivate blinking again when the brightness is set to LED_OFF * via the brightness_set() callback. */ int\t(*blink_set)(struct led_classdev *led_cdev, unsigned long *delay_on, unsigned long *delay_off); struct device\t*dev; const struct attribute_group\t**groups; struct list_head\tnode;\t/* LED Device list */ const char\t*default_trigger;\t/* Trigger to use */ unsigned long\tblink_delay_on, blink_delay_off; struct timer_list\tblink_timer; int\tblink_brightness; int\tnew_blink_brightness; void\t(*flash_resume)(struct led_classdev *led_cdev); struct work_struct\tset_brightness_work; int\tdelayed_set_value; #ifdef CONFIG_LEDS_TRIGGERS  /* Protects the trigger data below */ struct rw_semaphore\ttrigger_lock; struct led_trigger\t*trigger; struct list_head\ttrig_list; void\t*trigger_data; /* true if activated - deactivate routine uses it to do cleanup */ bool\tactivated; #endif  #ifdef CONFIG_LEDS_BRIGHTNESS_HW_CHANGED  int\tbrightness_hw_changed; struct kernfs_node\t*brightness_hw_changed_kn; #endif  /* Ensures consistent access to the LED Flash Class device */ struct mutex\tled_access; }; 结构体名称：led_classdev\n文件位置：include/linux/leds.h\n主要作用：该结构体所包括的内容较多，主要有以下几个功能\n brightness当前亮度值，max_brightness最大亮度 LED闪烁功能控制：blink_timer、blink_brightness、new_blink_brightness等 attribute_group：创建sysfs文件节点，向上提供用户访问接口   由上面可知，在创建gpio_led_data时，顺便初始化 led_classdev结构体，赋值相关属性以及部分回调函数，最终将led_classdev注册进入LED子系统框架中，在sysfs中创建对应的文件节点。\n 3.2 实现流程 #  static int create_gpio_led(const struct gpio_led *template, struct gpio_led_data *led_dat, struct device *parent, struct device_node *np, gpio_blink_set_t blink_set) { int ret, state; led_dat-\u0026gt;gpiod = template-\u0026gt;gpiod; if (!led_dat-\u0026gt;gpiod) { /* * This is the legacy code path for platform code that * still uses GPIO numbers. Ultimately we would like to get * rid of this block completely. */ unsigned long flags = GPIOF_OUT_INIT_LOW; /* skip leds that aren\u0026#39;t available */ if (!gpio_is_valid(template-\u0026gt;gpio)) {\t//\t判断是否gpio合法  dev_info(parent, \u0026#34;Skipping unavailable LED gpio %d (%s)\\n\u0026#34;, template-\u0026gt;gpio, template-\u0026gt;name); return 0; } if (template-\u0026gt;active_low) flags |= GPIOF_ACTIVE_LOW; ret = devm_gpio_request_one(parent, template-\u0026gt;gpio, flags, template-\u0026gt;name); if (ret \u0026lt; 0) return ret; led_dat-\u0026gt;gpiod = gpio_to_desc(template-\u0026gt;gpio);\t//\t获取gpio组  if (!led_dat-\u0026gt;gpiod) return -EINVAL; } led_dat-\u0026gt;cdev.name = template-\u0026gt;name;\t//\t赋值一些属性信息  led_dat-\u0026gt;cdev.default_trigger = template-\u0026gt;default_trigger; led_dat-\u0026gt;can_sleep = gpiod_cansleep(led_dat-\u0026gt;gpiod); if (!led_dat-\u0026gt;can_sleep) led_dat-\u0026gt;cdev.brightness_set = gpio_led_set;\t//\t设置LED  else led_dat-\u0026gt;cdev.brightness_set_blocking = gpio_led_set_blocking; led_dat-\u0026gt;blinking = 0; if (blink_set) { led_dat-\u0026gt;platform_gpio_blink_set = blink_set; led_dat-\u0026gt;cdev.blink_set = gpio_blink_set; } if (template-\u0026gt;default_state == LEDS_GPIO_DEFSTATE_KEEP) { state = gpiod_get_value_cansleep(led_dat-\u0026gt;gpiod); if (state \u0026lt; 0) return state; } else { state = (template-\u0026gt;default_state == LEDS_GPIO_DEFSTATE_ON); } led_dat-\u0026gt;cdev.brightness = state ? LED_FULL : LED_OFF; if (!template-\u0026gt;retain_state_suspended) led_dat-\u0026gt;cdev.flags |= LED_CORE_SUSPENDRESUME; if (template-\u0026gt;panic_indicator) led_dat-\u0026gt;cdev.flags |= LED_PANIC_INDICATOR; if (template-\u0026gt;retain_state_shutdown) led_dat-\u0026gt;cdev.flags |= LED_RETAIN_AT_SHUTDOWN; ret = gpiod_direction_output(led_dat-\u0026gt;gpiod, state); if (ret \u0026lt; 0) return ret; return devm_of_led_classdev_register(parent, np, \u0026amp;led_dat-\u0026gt;cdev);\t//\t将LED设备注册到子系统中 } 函数介绍：create_gpio_led创建LED设备的核心函数\n实现思路：\n 先通过gpio_is_valid接口，判断GPIO是否合法 将上层从设备树解析出来的信息，填充到gpio_led_data字段中，并且初始化部分字段，如：led_classdev、gpio_desc等 填充回调函数，实现相应的动作，如：gpio_led_set、gpio_led_set_blocking、gpio_blink_set等 最后调用devm_of_led_classdev_register接口，将LED设备注册到LED框架之中。  4、回调函数分析 #   硬件驱动层，肯定包括最终操作硬件的部分，也就是上面提到的一些回调函数，属于我们驱动工程师开发的内容。\n 4.1 gpio_blink_set #  static int gpio_blink_set(struct led_classdev *led_cdev, unsigned long *delay_on, unsigned long *delay_off) { struct gpio_led_data *led_dat = cdev_to_gpio_led_data(led_cdev); led_dat-\u0026gt;blinking = 1; return led_dat-\u0026gt;platform_gpio_blink_set(led_dat-\u0026gt;gpiod, GPIO_LED_BLINK, delay_on, delay_off); } 函数介绍：gpio_blink_set主要用于设置闪烁的时延\n4.2 gpio_led_set 和gpio_led_set_blocking #  static inline struct gpio_led_data * cdev_to_gpio_led_data(struct led_classdev *led_cdev) { return container_of(led_cdev, struct gpio_led_data, cdev); } static void gpio_led_set(struct led_classdev *led_cdev, enum led_brightness value) { struct gpio_led_data *led_dat = cdev_to_gpio_led_data(led_cdev); int level; if (value == LED_OFF) level = 0; else level = 1; if (led_dat-\u0026gt;blinking) { led_dat-\u0026gt;platform_gpio_blink_set(led_dat-\u0026gt;gpiod, level, NULL, NULL); led_dat-\u0026gt;blinking = 0; } else { if (led_dat-\u0026gt;can_sleep) gpiod_set_value_cansleep(led_dat-\u0026gt;gpiod, level); else gpiod_set_value(led_dat-\u0026gt;gpiod, level); } } static int gpio_led_set_blocking(struct led_classdev *led_cdev, enum led_brightness value) { gpio_led_set(led_cdev, value); return 0; } 函数介绍：gpio_led_set 和gpio_led_set_blocking主要用于设置亮度，区别在于gpio_led_set 是不可睡眠的，gpio_led_set_blocking是可休眠的。\n5、总结 #  上面我们了解了硬件驱动层的实现流程以及相关数据结构，总结来看：\n5.1 数据结构之间的关系如下 #  5.2 函数实现流程如下 #  gpio_led_probe(drivers/leds/leds-gpio.c) |--\u0026gt; gpio_leds_create |--\u0026gt; create_gpio_led // 创建LED设备  |--\u0026gt; devm_of_led_classdev_register 5.3 主要作用如下 #   从设备树获取LED相关属性信息，赋值给gpio_led结构体 将gpio_led、gpio_leds_priv、led_classdev等数据结构关联起来 将LED设备注册进入LED子系统中   欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":27,"href":"/docs/linux/linux_kernel_lock/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3linux%E9%94%81%E6%9C%BA%E5%88%B6%E4%B8%89%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/","title":"【深入理解Linux锁机制】三、原子操作","section":"Linux 内核锁详解","content":"【深入理解Linux内核锁】三、原子操作 #  1、原子操作思想 #  原子操作（atomic operation），不可分割的操作。其通过原子变量来实现，以保证单个CPU周期内，读写该变量不能被打断，进而判断该变量的值，来解决并发引起的互斥。\nAtomic类型的变量可以在执行期间禁止中断，并保证在访问变量时的原子性。\n 简单来说，我们可以把原子变量看作为一个标志位，然后再来检测该标志位的值。\n其原子性表现在：操作该标志位的值，不可被打断。\n 在Linux内核中，提供了两类原子操作的接口，分别是针对位和整型变量的原子操作。\n2、整型变量原子操作 #  2.1 API接口 #   对于整形变量的原子操作，内核提供了一系列的 API接口\n /*设置原子变量的值*/ atomic_t v = ATOMIC_INIT(0); /* 定义原子变量v并初始化为0 */ void atomic_set(atomic_t *v, int i); /* 设置原子变量的值为i */ /*获取原子变量的值*/ atomic_read(atomic_t *v); /* 返回原子变量的值*/ /*原子变量的加减*/ void atomic_add(int i, atomic_t *v); /* 原子变量增加i */ void atomic_sub(int i, atomic_t *v); /* 原子变量减少i */ /*原子变量的自增，自减*/ void atomic_inc(atomic_t *v);\t/* 原子变量增加1 */ void atomic_dec(atomic_t *v); /* 原子变量减少1 */ /*原子变量的操作并测试*/ int atomic_inc_and_test(atomic_t *v);\t/*进行对应操作后，测试原子变量值是否为0*/ int atomic_dec_and_test(atomic_t *v); int atomic_sub_and_test(int i, atomic_t *v); /*原子变量的操作并返回*/ int atomic_add_return(int i, atomic_t *v);\t/*进行对应操作后，返回新的值*/ int atomic_sub_return(int i, atomic_t *v); int atomic_inc_return(atomic_t *v); int atomic_dec_return(atomic_t *v); 2.2 API实现 #   我们下面就介绍几个稍微有代表性的接口实现\n以下基于Linux内核源码4.19，刚看是看的时候，有点摸不着头脑，因为定义的地方和引用的地方较多，不太容易找到，后来才慢慢得窥门径。\n 2.2.1 原子变量结构体 #  typedef struct { int counter; } atomic_t; 结构体名称：atomic_t\n文件位置：include/linux/types.h\n主要作用：原子变量结构体，该结构体只包含一个整型成员变量counter，用于存储原子变量的值。\n2.2.2 设置原子变量操作 #   设置原子变量的值的方式有两种：\n 通过ATOMIC_INIT宏定义来设置 通过atomic_set函数来定义   2.2.2.1 ATOMIC_INIT #  #define ATOMIC_INIT(i)\t{ (i) } 函数介绍：定义了一个ATOMIC类型的变量，并初始化为给定的值。\n文件位置：arch/arm/include/asm/atomic.h\n实现方法：这个宏定义比较简单，通过大括号将值包裹起来作为一个结构体，结构体的第一个成员就是给定的该值。\n2.2.2.2 atomic_set #  // arch/arm/include/asm/atomic.h #define atomic_set(v,i)\tWRITE_ONCE(((v)-\u0026gt;counter), (i))  // include/linux/compiler.h #define WRITE_ONCE(x, val) \\ ({\t\\ union { typeof(x) __val; char __c[1]; } __u =\t\\ { .__val = (__force typeof(x)) (val) }; \\ __write_once_size(\u0026amp;(x), __u.__c, sizeof(x));\t\\ __u.__val;\t\\ })  static __always_inline void __write_once_size(volatile void *p, void *res, int size) { switch (size) { case 1: *(volatile __u8 *)p = *(__u8 *)res; break; case 2: *(volatile __u16 *)p = *(__u16 *)res; break; case 4: *(volatile __u32 *)p = *(__u32 *)res; break; case 8: *(volatile __u64 *)p = *(__u64 *)res; break; default: barrier(); __builtin_memcpy((void *)p, (const void *)res, size); barrier(); } } 函数介绍：该函数也用作初始化原子变量\n文件位置：arch/arm/include/asm/atomic.h\n实现方式：通过调用WRITE_ONCE来实现，其中WRITE_ONCE宏实现了一些屏蔽编译器优化的技巧，确保写入操作是原子的。\n atomic_set调用WRITE_ONCE将i的值写入原子变量(v)-\u0026gt;counter中，WRITE_ONCE以保证操作的原子性 WRITE_ONCE用来保证操作的原子性，它是怎么实现的呢？  创建union联合体，包括__val和__C成员变量 定义一个__U变量，使用强制转换将参数__val转换为typeof(x)类型，传递给联合体变量__u.__val 调用__write_once_size函数，将__u.__c指向的内存块的内容写入到变量x的内存空间中，大小为sizeof(x)。 函数返回__u.__val，也就是写入的值   union联合体  它的特点是存储多种数据类型的值，但是所有成员共享同一个内存空间，这样可以节省内存空间。 主要作用是将一个非字符类型的数据x强制转换为一个字符类型的数据，以字符类型数据来访问该区块的内存单元。   __write_once_size函数实现了操作的原子性，核心有以下几点：  该函数在向内存写入数据时使用了volatile关键字，告诉编译器不要进行优化，每次操作都从内存中读取最新的值。 函数中的switch语句保证了对不同大小的数据类型使用不同的存储方式，可以保证内存访问的原子性。 对于默认情况，则使用了__builtin_memcpy函数进行复制，而这个函数具有原子性。 barrier()函数指示CPU要完成所有之前的内存操作，以及确保执行顺序与其他指令不发生重排。    2.2.3 原子变量的加减 #  2.2.3.1 ATOMIC_OPS #  /* * ARMv6 UP and SMP safe atomic ops. We use load exclusive and * store exclusive to ensure that these are atomic. We may loop * to ensure that the update happens. */ #define ATOMIC_OP(op, c_op, asm_op)\t\\ static inline void atomic_##op(int i, atomic_t *v)\t\\ {\t\\ unsigned long tmp;\t\\ int result;\t\\ \\ prefetchw(\u0026amp;v-\u0026gt;counter);\t\\ __asm__ __volatile__(\u0026#34;@ atomic_\u0026#34; #op \u0026#34;\\n\u0026#34;\t\\ \u0026#34;1:\tldrex\t%0, [%3]\\n\u0026#34;\t\\ \u0026#34;\t\u0026#34; #asm_op \u0026#34;\t%0, %0, %4\\n\u0026#34;\t\\ \u0026#34;\tstrex\t%1, %0, [%3]\\n\u0026#34;\t\\ \u0026#34;\tteq\t%1, #0\\n\u0026#34;\t\\ \u0026#34;\tbne\t1b\u0026#34;\t\\ : \u0026#34;=\u0026amp;r\u0026#34; (result), \u0026#34;=\u0026amp;r\u0026#34; (tmp), \u0026#34;+Qo\u0026#34; (v-\u0026gt;counter)\t\\ : \u0026#34;r\u0026#34; (\u0026amp;v-\u0026gt;counter), \u0026#34;Ir\u0026#34; (i)\t\\ : \u0026#34;cc\u0026#34;);\t\\ }\t\\ #define ATOMIC_OP_RETURN(op, c_op, asm_op)\t\\ static inline int atomic_##op##_return_relaxed(int i, atomic_t *v)\t\\ {\t\\ unsigned long tmp;\t\\ int result;\t\\ \\ prefetchw(\u0026amp;v-\u0026gt;counter);\t\\ \\ __asm__ __volatile__(\u0026#34;@ atomic_\u0026#34; #op \u0026#34;_return\\n\u0026#34;\t\\ \u0026#34;1:\tldrex\t%0, [%3]\\n\u0026#34;\t\\ \u0026#34;\t\u0026#34; #asm_op \u0026#34;\t%0, %0, %4\\n\u0026#34;\t\\ \u0026#34;\tstrex\t%1, %0, [%3]\\n\u0026#34;\t\\ \u0026#34;\tteq\t%1, #0\\n\u0026#34;\t\\ \u0026#34;\tbne\t1b\u0026#34;\t\\ : \u0026#34;=\u0026amp;r\u0026#34; (result), \u0026#34;=\u0026amp;r\u0026#34; (tmp), \u0026#34;+Qo\u0026#34; (v-\u0026gt;counter)\t\\ : \u0026#34;r\u0026#34; (\u0026amp;v-\u0026gt;counter), \u0026#34;Ir\u0026#34; (i)\t\\ : \u0026#34;cc\u0026#34;);\t\\ \\ return result;\t\\ }  #define ATOMIC_FETCH_OP(op, c_op, asm_op)\t\\ static inline int atomic_fetch_##op##_relaxed(int i, atomic_t *v)\t\\ {\t\\ unsigned long tmp;\t\\ int result, val;\t\\ \\ prefetchw(\u0026amp;v-\u0026gt;counter);\t\\ \\ __asm__ __volatile__(\u0026#34;@ atomic_fetch_\u0026#34; #op \u0026#34;\\n\u0026#34;\t\\ \u0026#34;1:\tldrex\t%0, [%4]\\n\u0026#34;\t\\ \u0026#34;\t\u0026#34; #asm_op \u0026#34;\t%1, %0, %5\\n\u0026#34;\t\\ \u0026#34;\tstrex\t%2, %1, [%4]\\n\u0026#34;\t\\ \u0026#34;\tteq\t%2, #0\\n\u0026#34;\t\\ \u0026#34;\tbne\t1b\u0026#34;\t\\ : \u0026#34;=\u0026amp;r\u0026#34; (result), \u0026#34;=\u0026amp;r\u0026#34; (val), \u0026#34;=\u0026amp;r\u0026#34; (tmp), \u0026#34;+Qo\u0026#34; (v-\u0026gt;counter)\t\\ : \u0026#34;r\u0026#34; (\u0026amp;v-\u0026gt;counter), \u0026#34;Ir\u0026#34; (i)\t\\ : \u0026#34;cc\u0026#34;);\t\\ \\ return result;\t\\ }  #define ATOMIC_OPS(op, c_op, asm_op)\t\\ ATOMIC_OP(op, c_op, asm_op)\t\\ ATOMIC_OP_RETURN(op, c_op, asm_op)\t\\ ATOMIC_FETCH_OP(op, c_op, asm_op)  ATOMIC_OPS(add, +=, add) ATOMIC_OPS(sub, -=, sub)  找atomic_add找半天，还找到了不同的架构下面。:(\n原来内核通过各种宏定义将其操作全部管理起来，宏定义在内核中的使用也是非常广泛了。\n 函数作用：通过一些列宏定义，来实现原子变量的add、sub、and、or等原子变量操作\n文件位置：arch/arm/include/asm/atomic.h\n实现方式：\n 我们以atomic_##op为例来介绍，其他大同小异！\n #define ATOMIC_OP(op, c_op, asm_op)\t\\ static inline void atomic_##op(int i, atomic_t *v)\t\\ {\t\\ unsigned long tmp;\t\\ int result;\t\\ \\ prefetchw(\u0026amp;v-\u0026gt;counter);\t\\ __asm__ __volatile__(\u0026#34;@ atomic_\u0026#34; #op \u0026#34;\\n\u0026#34;\t\\ \u0026#34;1:\tldrex\t%0, [%3]\\n\u0026#34;\t\\ \u0026#34;\t\u0026#34; #asm_op \u0026#34;\t%0, %0, %4\\n\u0026#34;\t\\ \u0026#34;\tstrex\t%1, %0, [%3]\\n\u0026#34;\t\\ \u0026#34;\tteq\t%1, #0\\n\u0026#34;\t\\ \u0026#34;\tbne\t1b\u0026#34;\t\\ : \u0026#34;=\u0026amp;r\u0026#34; (result), \u0026#34;=\u0026amp;r\u0026#34; (tmp), \u0026#34;+Qo\u0026#34; (v-\u0026gt;counter)\t\\ : \u0026#34;r\u0026#34; (\u0026amp;v-\u0026gt;counter), \u0026#34;Ir\u0026#34; (i)\t\\ : \u0026#34;cc\u0026#34;);\t\\ }\t 首先是函数名称atomic_##op，通过##来实现字符串的拼接，使函数名称可变，如atomic_add、atomic_sub等 调用prefetchw函数，预取数据到L1缓存，方便操作，提高程序性能，但是不要滥用。 __asm__ __volatile__：表示汇编指令 \u0026quot;@ atomic_\u0026quot; #op \u0026quot;\\n\u0026quot;：添加汇编注释，也就是我们的函数名字，如：atomic_add、atomic_sub \u0026quot;1: ldrex %0, [%3]\\n\u0026quot;：将%3存储地址的数据，读入到%0地址中，ldrex为独占式的读取操作。 \u0026quot; \u0026quot; #asm_op \u0026quot; %0, %0, %4\\n\u0026quot;：\u0026quot; #asm_op \u0026quot;表示作为宏定义传进来的参数，表示不同的操作码add、sub等，操作%0和%4对应的地址的值，并将结果返回到%0地址处 \u0026quot; strex %1, %0, [%3]\\n\u0026quot; ：表示将%0地址处的值写入%3地址处，strex为独占式的写操作，写入的结果会返回到%1地址中 \u0026quot; teq %1, #0\\n\u0026quot;：测试%1寄存器的值是否为0，如果不等于0，则执行下面的\u0026quot; bne 1b\u0026quot; 操作，跳转到1代码标签的位置，也就是ldrex前面的1的位置 : \u0026quot;=\u0026amp;r\u0026quot; (result), \u0026quot;=\u0026amp;r\u0026quot; (tmp), \u0026quot;+Qo\u0026quot; (v-\u0026gt;counter)：根据汇编语法，前两个为输出操作数，第三个为输入输出操作数 : \u0026quot;r\u0026quot; (\u0026amp;v-\u0026gt;counter), \u0026quot;Ir\u0026quot; (i)：根据汇编语法，这两个为输入操作数 : \u0026quot;cc\u0026quot;：表示可能会修改条件码寄存器，编译期间需要优化。  总结：上述原子操作，通过ldrex和strex也就是我们说的load和store指令，来完成数据的读写，同时也保证了其原子性！\n 这一部分，牵涉到汇编的语法，需要提前了解下基础的汇编指令。\n 2.2.3.2 atomic_add和atomic_sub定义 #  ATOMIC_OPS(add, +=, add) ATOMIC_OPS(sub, -=, sub)  通过宏定义来实现atomic_add和atomic_sub的定义，下面我们就不一一分析了，原理都是通过ARM提供的ldrex strex也就是我们常说的Load和Store指令实现读取操作，确保操作的原子性。\n 3、位原子操作 #   对于位原子操作，内核也提供了一系列的 API接口\n 3.1 API接口 #  void set_bit(nr, void *addr);\t//\t设置位：设置addr地址的第nr位，所谓设置位即是将位写为1 void clear_bit(nr, void *addr);\t//\t清除位：清除addr地址的第nr位，所谓清除位即是将位写为0 void change_bit(nr, void *addr);\t//\t改变位：对addr地址的第nr位进行反置。 int test_bit(nr, void *addr);\t//\t测试位：返回addr地址的第nr位。 int test_and_set_bit(nr, void *addr);//\t测试并设置位 int test_and_clear_bit(nr, void *addr);\t//\t测试并清除位 int test_and_change_bit(nr, void *addr);//\t测试并改变位 3.2 API实现 #   同样，我们还是简单介绍几个接口，其他核心实现原理相同\n 3.2.1 set_bit #  #define set_bit(nr,p)\tATOMIC_BITOP(set_bit,nr,p)  #define ATOMIC_BITOP(name,nr,p)\t\\ (__builtin_constant_p(nr) ? ____atomic_##name(nr, p) : _##name(nr,p))  extern void _set_bit(int nr, volatile unsigned long * p); /* * These functions are the basis of our bit ops. * * First, the atomic bitops. These use native endian. */ static inline void ____atomic_set_bit(unsigned int bit, volatile unsigned long *p) { unsigned long flags; unsigned long mask = BIT_MASK(bit); p += BIT_WORD(bit); raw_local_irq_save(flags); *p |= mask; raw_local_irq_restore(flags); } #define BIT_MASK(nr)\t(1UL \u0026lt;\u0026lt; ((nr) % BITS_PER_LONG)) #define BIT_WORD(nr)\t((nr) / BITS_PER_LONG)  #ifdef CONFIG_64BIT #define BITS_PER_LONG 64 #else #define BITS_PER_LONG 32 #endif /* CONFIG_64BIT */函数介绍：该函数用于原子操作某个地址的某一位。\n文件位置：/arch/arm/include/asm/bitops.h\n实现方式：\n __builtin_constant_p：GCC的一个内置函数，用来判断表达式是否为常量，如果为常量，则返回值为1 ____atomic_set_bit函数中BIT_MASK，用于获取操作位的掩码，将要设置的位设置为1，其他为0 BIT_WORD：确定要操作位的偏移，要偏移多少个字 最后通过raw_local_irq_save和raw_local_irq_restore中断屏蔽来保证位操作*p |= mask;的原子性  4、总结 #  该文章主要详细了解了Linux内核锁的原子操作，原子操作分为两种：整型变量的原子操作和位原子操作。\n 整型变量的原子操作：通过ldrex和strex来实现 位原子操作：通过中断屏蔽来实现。   欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":28,"href":"/docs/linux/linux_nvmem_subsystem/nvmem%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E4%B8%89%E6%A0%B8%E5%BF%83%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%8F%8A%E5%86%85%E5%9C%A8%E5%85%B3%E8%81%94/","title":"【NVMEM子系统深入剖析】三、核心数据结构及内在关联","section":"Linux NVMEM 子系统","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":29,"href":"/docs/embeded_tech/embeded_interview/linux%E7%94%A8%E6%88%B7%E6%80%81%E5%92%8C%E5%86%85%E6%A0%B8%E6%80%81%E4%BA%A4%E4%BA%92%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/","title":"Linux用户态和内核态交互的几种方式","section":"嵌入式面经","content":"Linux用户态和内核态交互的几种方式 #  Linux分为内核态Kernel Mode和用户态User Mode，其通信方式主要有：\n 系统调用System Call：最常见的用户态和内核态之间的通信方式。通过系统调用接口（open、read、write、fork等）请求内核执行特定的动作。 中断Interrupts：中断包括软中断和硬中断，每当中断到来的时候，CPU会暂停当前执行的用户态代码，切换到内核态来处理中断。 信号Signal：内核通过Signal通知用户态进程发生了某些事件，用户态注册信号处理函数，来响应特定的信号事件。如 SIGTERM、SIGINT 等。 共享内存Share Memory：允许多个进程在它们的地址空间中共享一块内存区域，从而实现用户态和内核态之间的高效通信。这种方式避免了用户态和内核态之间频繁切换的问题，但是也需要考虑到数据的同步问题，保证数据一致性。   用户态User Mode访问内核态Kernel Mode的数据交互的方式有：\n  procfs进程文件系统：一个伪文件系统，因为其不占用外部存储空间，只占有少量的内存，挂载在/proc目录下\n  sysctl：它也是一个Linux命令，主要用来修改内核的运行时参数，也就是在内核运行时，动态修改内核参数。\n 和 procfs 的区别在于：procfs 主要是输出只读数据，而 sysctl 输出的大部分信息是可写的。\n   sysfs虚拟文件系统：通过/sys来完成用户态和内核的通信，和 procfs 不同的是，sysfs 是将一些原本在 procfs 中的，关于设备和驱动的部分，独立出来，以 “设备树” 的形式呈现给用户。\n  netlink 接口：也是最常用的一种方式，本质是socket接口，使用netlink用于网络相关的内核和用户进程之间的消息传递。\n  共享内存Share Memory：允许多个进程在它们的地址空间中共享一块内存区域，从而实现用户态和内核态之间的高效数据传输。\n   欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":30,"href":"/docs/uboot/%E4%B8%89uboot%E9%A9%B1%E5%8A%A8%E6%A8%A1%E5%9E%8B/","title":"三、Uboot驱动模型","section":"Uboot开发","content":"三、Uboot驱动模型 #   全文耗时一周，精心汇总，希望对大家有所帮助，感觉可以的点赞，关注，不迷路，后续还有更多干货！\n看文章前，答应我，静下心来，慢慢品！\n 3.1、什么是Uboot驱动模型 #  学过Linux的朋友基本都知道Linux的设备驱动模型，Uboot根据Linux的驱动模型架构，也引入了Uboot的驱动模型（driver model ：DM）。\n**这种驱动模型为驱动的定义和访问接口提供了统一的方法。**提高了驱动之间的兼容性以及访问的标准型，uboot驱动模型和kernel中的设备驱动模型类似。\n3.2、为什么要有驱动模型呢 #   无论是Linux还是Uboot，一个新对象的产生必定有其要解决的问题，驱动模型也不例外！\n  提高代码的可重用性：为了能够使代码在不同硬件平台，不同体系架构下运行，必须要最大限度的提高代码的可重用性。 高内聚，低耦合：分层的思想也是为了达到这一目标，低耦合体现在对外提供统一的抽象访问接口，高内聚将相关度紧密的集中抽象实现。 便于管理：在不断发展过程中，硬件设备越来越多，驱动程序也越来越多，为了更好的管理驱动，也需要一套优秀的驱动架构！  3.3、如何使用uboot的DM模型 #   DM模型的使用，可以通过menuconfig来配置。\nmake menuconfig\n ①：menuconfig配置全局DM模型 #  Device Drivers -\u0026gt; Generic Driver Options -\u0026gt; Enable Driver Model 通过上面的路径来打开Driver Model模型，最终配置在.config文件中，CONFIG_DM=y\n②：指定某个驱动的DM模型 #  全局的DM模型打开后，我们对于不通的驱动模块，使能或者失能DM功能。如MMC驱动为例：\nDevice Drivers -\u0026gt; MMC Host controller Support -\u0026gt; Enable MMC controllers using Driver Model 最终反映在.config文件中的CONFIG_DM_MMC=y\n在对应的驱动中，可以看到判断#if !CONFIG_IS_ENABLED(DM_MMC)，来判断是否打开DM驱动模型。\n在管理驱动的Makefile文件中，也能看到obj-$(CONFIG_$(SPL_)DM_MMC) += mmc-uclass.o，来判断是否将驱动模型加入到编译选项中。\n总之，我们要打开DM模型，最后反映在几个配置信息上：\n CONFIG_DM=y，全局DM模型打开 CONFIG_DM_XXX=y，某个驱动的DM模型的打开 可以通过Kconifg、Makefile来查看对应宏的编译情况  3.4、DM模型数据结构 #  要想了解DM模型整套驱动框架，我们必须先了解它的一砖一瓦！也就是组成驱动框架的各个数据结构。\n① global_data #  typedef struct global_data { ... #ifdef CONFIG_DM  struct udevice\t*dm_root;\t/* Root instance for Driver Model */ struct udevice\t*dm_root_f;\t/* Pre-relocation root instance */ struct list_head uclass_root;\t/* Head of core tree */ #endif ... } global_data，管理着整个Uboot的全局变量，其中dm_root，dm_root_f，uclass_root用来管理整个DM模型。这几个变量代表什么意思呢？\n dm_root：DM模型的根设备 dm_root_f：重定向前的根设备 uclass_root：uclass链表的头  这几个变量，最终要的作用就是：管理整个模型中的udevice设备信息和uclass驱动类。\n② uclass #  我们首先看一下uclass这个结构体\n/** * struct uclass - a U-Boot drive class, collecting together similar drivers * * A uclass provides an interface to a particular function, which is * implemented by one or more drivers. Every driver belongs to a uclass even * if it is the only driver in that uclass. An example uclass is GPIO, which * provides the ability to change read inputs, set and clear outputs, etc. * There may be drivers for on-chip SoC GPIO banks, I2C GPIO expanders and * PMIC IO lines, all made available in a unified way through the uclass. * * @priv: Private data for this uclass * @uc_drv: The driver for the uclass itself, not to be confused with a * \u0026#39;struct driver\u0026#39; * @dev_head: List of devices in this uclass (devices are attached to their * uclass when their bind method is called) * @sibling_node: Next uclass in the linked list of uclasses */ struct uclass { void *priv;\t//uclass的私有数据  struct uclass_driver *uc_drv;\t//uclass类的操作函数集合  struct list_head dev_head;\t//该uclass的所有设备  struct list_head sibling_node;\t//下一个uclass的节点 }; 根据注释，我们就可以了解到，uclass相当于老师，管理着==对应某一个类别下==的所有的udevice。\n 例如：一个IIC驱动程序，其驱动程序框架是一致的，只有一种，但是IIC驱动的设备可以有很多，如EEPROM，MCU6050等；\n所有在这里呢，dev_head链表就是用来管理该驱动类下的所有的设备。\n 总结：uclass，来管理该类型下的所有设备，并且有对应的uclass_driver驱动。\n  定义 #    uclass是uboot自动生成的，并且不是所有uclass都会生成，有对应uclass_driver并且有被udevice匹配到的uclass才会生成。\n  存放 #    所有生成的uclass都会被挂载gd-\u0026gt;uclass_root链表上。\n  相关API #     直接遍历链表gd-\u0026gt;uclass_root链表并且根据uclass_id来获取到相应的uclass。\n int uclass_get(enum uclass_id key, struct uclass **ucp); // 从gd-\u0026gt;uclass_root链表获取对应的ucla ss ③ uclass_driver #  正如上面，我们看到了uclass类所包含uclass_driver结构体，uclass_driver正如其名，它就是uclass的驱动程序。其主要作用是：为uclass提供统一管理的接口，结构体如下：\n/** * struct uclass_driver - Driver for the uclass * * A uclass_driver provides a consistent interface to a set of related * drivers. */ struct uclass_driver { const char *name; // 该uclass_driver的命令  enum uclass_id id; // 对应的uclass id /* 以下函数指针主要是调用时机的区别 */ int (*post_bind)(struct udevice *dev); // 在udevice被绑定到该uclass之后调用  int (*pre_unbind)(struct udevice *dev); // 在udevice被解绑出该uclass之前调用  int (*pre_probe)(struct udevice *dev); // 在该uclass的一个udevice进行probe之前调用  int (*post_probe)(struct udevice *dev); // 在该uclass的一个udevice进行probe之后调用  int (*pre_remove)(struct udevice *dev);// 在该uclass的一个udevice进行remove之前调用  int (*child_post_bind)(struct udevice *dev); // 在该uclass的一个udevice的一个子设备被绑定到该udevice之后调用  int (*child_pre_probe)(struct udevice *dev); // 在该uclass的一个udevice的一个子设备进行probe之前调用  int (*init)(struct uclass *class); // 安装该uclass的时候调用  int (*destroy)(struct uclass *class); // 销毁该uclass的时候调用  int priv_auto_alloc_size; // 需要为对应的uclass分配多少私有数据  int per_device_auto_alloc_size; //  int per_device_platdata_auto_alloc_size; //  int per_child_auto_alloc_size; //  int per_child_platdata_auto_alloc_size; //  const void *ops; //操作集合  uint32_t flags; // 标识为 };   定义 #    uclass_driver主要通过UCLASS_DRIVER来定义，这里就简单说明一下底层代码，耐心看哦！\n 下面以pinctrl为例\n UCLASS_DRIVER(pinctrl) = { .id = UCLASS_PINCTRL, .post_bind = pinctrl_post_bind, .flags = DM_UC_FLAG_SEQ_ALIAS, .name = \u0026#34;pinctrl\u0026#34;, }; /* Declare a new uclass_driver */ #define UCLASS_DRIVER(__name)\t\\ ll_entry_declare(struct uclass_driver, __name, uclass)  #define ll_entry_declare(_type, _name, _list)\t\\ _type _u_boot_list_2_##_list##_2_##_name __aligned(4)\t\\ __attribute__((unused,\t\\ section(\u0026#34;.u_boot_list_2_\u0026#34;#_list\u0026#34;_2_\u0026#34;#_name))) 上面基本上就是我们的底层代码了，稍微有点绕，但是也不难！我们只需要将宏进行替换就行了！\n通过上面的定义，我们替换掉宏之后，最终得到的定义如下：\nstruct uclass_driver _u_boot_list_2_uclass_2_pinctrl = { .id = UCLASS_PINCTRL, .post_bind = pinctrl_post_bind, .flags = DM_UC_FLAG_SEQ_ALIAS, .name = \u0026#34;pinctrl\u0026#34;, } //同时存放在段._u_boot_list_2_uclass_2_pinctrl中，也就是section段的内容   存放 #    由上面结构体可得，其定义之后都被存放在了段._u_boot_list_2_uclass_2_pinctrl中，那么去哪里可以看到呢？\n在u-boot.map文件中搜索，._u_boot_list_2_uclass_2_pinctrl，就可以查到程序中定义的所有驱动程序。\n这里相信大家会有疑问，为什么是uclass_2呢？我们大概看一下，也会看到uclass_1和uclass_3，这两个代表什么呢？往下看！\n  相关API #     想要获取uclass_driver需要先获取uclass_driver table。\n struct uclass_driver *uclass = ll_entry_start(struct uclass_driver, uclass); // 会根据.u_boot_list_2_uclass_1的段地址来得到uclass_driver table的地址  const int n_ents = ll_entry_count(struct uclass_driver, uclass); // 获得uclass_driver table的长度  struct uclass_driver *lists_uclass_lookup(enum uclass_id id) // 从uclass_driver table中获取uclass id为id的uclass_driver。 正如注释描述，上文中提到的uclass_1和uclass_3起到定位作用，用于计算uclass_2的长度！\n上述的API，主要用于根据uclass_id来查找到对应的uclass_driver，进而操作对应的uclass下的udevice。\n④ uclass_id #  我们在uclass_driver中，看到一个uclass_id类型，这种类型与uclass有什么关系呢？\n我们知道，uclass代表驱动的一个类别，uclass_driver是uclass的驱动程序，为uclass提供统一操作接口。而对于不同类型的驱动，就需要uclass_id来区分了！\n事实上，每一种类型的设备uclass都有唯一对应的uclass_id，贯穿设备模型，也是udevice与uclass相关联的关键之处。\nenum uclass_id { /* These are used internally by driver model */ UCLASS_ROOT = 0, UCLASS_DEMO, UCLASS_TEST, UCLASS_TEST_FDT, UCLASS_TEST_BUS, UCLASS_TEST_PROBE, ...... /* U-Boot uclasses start here - in alphabetical order */ UCLASS_ACPI_PMC,\t/* (x86) Power-management controller (PMC) */ UCLASS_ADC,\t/* Analog-to-digital converter */ UCLASS_AHCI,\t/* SATA disk controller */ UCLASS_AUDIO_CODEC,\t/* Audio codec with control and data path */ UCLASS_AXI,\t/* AXI bus */ UCLASS_BLK,\t/* Block device */ UCLASS_BOARD,\t/* Device information from hardware */ ...... }; 在这里，我们就把他当作一个设备识别的标志即可！\n 最后，压轴的两个结构体出来了，也是DM模型最终操作的对象。\n ⑤ udevice #  /** * struct udevice - An instance of a driver * * This holds information about a device, which is a driver bound to a * particular port or peripheral (essentially a driver instance). * */ struct udevice { const struct driver *driver;\t//device 对应的driver  const char *name;\t//device 的名称  void *platdata; void *parent_platdata; void *uclass_platdata; ofnode node;\t//设备树节点  ulong driver_data; struct udevice *parent;\t//父设备  void *priv;\t// 私有数据的指针  struct uclass *uclass;\t//驱动所属的uclass  void *uclass_priv; void *parent_priv; struct list_head uclass_node; struct list_head child_head; struct list_head sibling_node; uint32_t flags; int req_seq; int seq; #ifdef CONFIG_DEVRES  struct list_head devres_head; #endif };   定义 #   **硬编码：**代码中调用U_BOOT_DEVICE宏来定义设备资源，实际为一个设备实例。 **设备树：**将设备描述信息写在对应的DTS文件中，然后编译成DTB，最终由uboot解析设备树后动态生成的。 传参方式：通过命令行或者接口将设备资源信息传递进来，非常灵活。    存放 #    udevice是最基础的一个设备单元，我们把它作为一个独立的个体，上层所有的操作，最终都与该结构体有关。\n我们创建一个设备后，为了服从统一的管理，该结构体会被连接到DM模型下，并入到机制中。那么udevice会被连接到哪里呢？\n 将udevice连接到对应的uclass中，uclass主要用来管理着同一类的驱动 除此之外，有父子关系的udevice，还会连接到udevice-\u0026gt;child_head链表下，方便调用  大概可以理解为下面这样：\n  相关API #    #define uclass_foreach_dev(pos, uc) \\ list_for_each_entry(pos, \u0026amp;uc-\u0026gt;dev_head, uclass_node)  #define uclass_foreach_dev_safe(pos, next, uc) \\ list_for_each_entry_safe(pos, next, \u0026amp;uc-\u0026gt;dev_head, uclass_node)  int uclass_get_device(enum uclass_id id, int index, struct udevice **devp); // 通过索引从uclass中获取udevice int uclass_get_device_by_name(enum uclass_id id, const char *name, // 通过设备名从uclass中获取udevice  struct udevice **devp); int uclass_get_device_by_seq(enum uclass_id id, int seq, struct udevice **devp); int uclass_get_device_by_of_offset(enum uclass_id id, int node, struct udevice **devp); int uclass_get_device_by_phandle(enum uclass_id id, struct udevice *parent, const char *name, struct udevice **devp); int uclass_first_device(enum uclass_id id, struct udevice **devp); int uclass_first_device_err(enum uclass_id id, struct udevice **devp); int uclass_next_device(struct udevice **devp); int uclass_resolve_seq(struct udevice *dev); 这些相关的API，主要作用就是根据uclass_id，查找对应的uclass，然后根据索引值或者名称，来查找到对应的udevice\n③ driver #  struct driver { char *name;\t//驱动名称  enum uclass_id id;\t//驱动所对应的uclass_id\t const struct udevice_id *of_match;\t//匹配函数  int (*bind)(struct udevice *dev);\t//绑定函数  int (*probe)(struct udevice *dev);\t//注册函数  int (*remove)(struct udevice *dev); int (*unbind)(struct udevice *dev); int (*ofdata_to_platdata)(struct udevice *dev); int (*child_post_bind)(struct udevice *dev); int (*child_pre_probe)(struct udevice *dev); int (*child_post_remove)(struct udevice *dev); int priv_auto_alloc_size; int platdata_auto_alloc_size; int per_child_auto_alloc_size; int per_child_platdata_auto_alloc_size; const void *ops;\t/* driver-specific operations */ uint32_t flags; #if CONFIG_IS_ENABLED(ACPIGEN)  struct acpi_ops *acpi_ops; #endif };   定义 #    driver对象，主要通过U_BOOT_DRIVER来定义\n 以pinctrl来举例\n U_BOOT_DRIVER(xxx_pinctrl) = { .name\t= \u0026#34;xxx_pinctrl\u0026#34;, .id\t= UCLASS_PINCTRL, .of_match\t= arobot_pinctrl_match, .priv_auto_alloc_size = sizeof(struct xxx_pinctrl), .ops\t= \u0026amp;arobot_pinctrl_ops, .probe\t= arobot_v2s_pinctrl_probe, .remove = arobot_v2s_pinctrl_remove, }; /* Declare a new U-Boot driver */ #define U_BOOT_DRIVER(__name)\t\\ ll_entry_declare(struct driver, __name, driver)  #define ll_entry_declare(_type, _name, _list)\t\\ _type _u_boot_list_2_##_list##_2_##_name __aligned(4)\t\\ __attribute__((unused,\t\\ section(\u0026#34;.u_boot_list_2_\u0026#34;#_list\u0026#34;_2_\u0026#34;#_name))) 通过上面的定义，最终我们定义的结构体如下：\nstruct driver _u_boot_list_2_driver_2_xxx_pinctrl = { .name\t= \u0026#34;xxx_pinctrl\u0026#34;, .id\t= UCLASS_PINCTRL, .of_match\t= arobot_pinctrl_match, .priv_auto_alloc_size = sizeof(struct xxx_pinctrl), .ops\t= \u0026amp;arobot_pinctrl_ops, .probe\t= arobot_v2s_pinctrl_probe, .remove = arobot_v2s_pinctrl_remove, } //同时存放在段._u_boot_list_2_driver_2_xxx_pinctrl中   存放 #    由上面结构体可得，其定义之后都被存放在了段._u_boot_list_2_driver_2_xxx中，那么去哪里可以看到呢？\n在u-boot.map文件中搜索，._u_boot_list_2_driver，就可以查到程序中定义的所有驱动程序。\n最终，所有driver结构体以列表的形式被放在.u_boot_list_2_driver_1和.u_boot_list_2_driver_3的区间中。\n  相关API #    /*先获取driver table 表*/ struct driver *drv = ll_entry_start(struct driver, driver);\t// 会根据.u_boot_list_2_driver_1的段地址来得到uclass_driver table的地址  const int n_ents = ll_entry_count(struct driver, driver);\t// 通过.u_boot_list_2_driver_3的段地址 减去 .u_boot_list_2_driver_1的段地址 获得driver table的长度  /*遍历所有的driver*/ struct driver *lists_driver_lookup_name(const char *name)\t// 从driver table中获取名字为name的driver。 正如注释描述，上文中提到的driver_1和driver_3起到定位作用，用于计算driver_2的长度！\n上述的API，主要用于根据name来查找到对应的driver驱动程序。\n综上，DM模型相关的数据结构介绍完毕，整体设计的架构如下：\n正如红线部分，如何实现driver和udevice的绑定、uclass、uclass_driver的绑定呢？\n要想真正搞懂这些，我们不得不去深入到DM的初始化流程。\n3.5、DM驱动模型之上帝视角 #   对于DM模型，我们站在上帝视角来观察整套模型框架是如何的！\n 从对象设计的角度来看，Uboot的驱动模型可以分为静态形式和动态形式。\n  **静态模式：**对象是离散的，和其他对象分隔开，减小对象复杂度，利于模块化设计。\n  动态模式：运行态表达形式的对象是把所有的静态对象组合成层次视图，有清晰的数据关联视图\n  在静态模式下，驱动模型主要将对象分为udevice和driver，即设备和驱动程序，两个就像火车的两条轨道，永远也不会产生交集，驱动和设备可以想注册多少就注册多少。\n我们看一下udevice的描述：\n/** * struct udevice - An instance of a driver * * This holds information about a device, which is a driver bound to a * particular port or peripheral (essentially a driver instance). * */ udevice是driver的一个实例，两个不相交的铁轨，终归也是想要发生爱情的。那么如何让其产生交集呢？这就是动态模式需要做的工作了！\n**在动态模式下，**引入了uclass和uclass_driver两个数据结构，实现了对udevice和driver的管理。\n看一下uclass和uclass_driver两个结构体的说明：\n/** * struct uclass - a U-Boot drive class, collecting together similar drivers * */ /** * struct uclass_driver - Driver for the uclass * * A uclass_driver provides a consistent interface to a set of related * drivers. * */  **uclass：**设备组公共属性对象，作为udevice的一个属性，主要用来管理某个驱动类的所有的设备。 **uclass_driver：**设备组公共行为对象，uclass的驱动程序，主要将uclass管理的设备和驱动实现绑定、注册，移除等操作。  通过这两个结构体的引入，可以将毫不相关的udevice是driver关联起来！\nudevice与driver的绑定：通过驱动的of_match和compatible属性来配对，绑定。\nudevice与uclass的绑定：udevice内的driver下的uclass_id，来与uclass对应的uclass_driver的uclass_id进行匹配。\nuclass与uclass_driver的绑定：已知udevice内的driver下的uclass_id，创建uclass的同时，通过``uclass_id找到对应的uclass_driver对象，然后将uclass_driver绑定到uclass`上！\n整体结构如下：\n3.6、DM模型——Udevice与driver绑定 #   相信站在上帝视角看完DM的整体架构，大家都对DM框架有一定了解，下面我们来看看具体的实现细节！\n DM的初始化分为两个部分，一个是在relocate重定向之前的初始化：initf_dm，一个是在relocate重定向之后的初始化：initr_dm。\n我们对比这两个函数：\nstatic int initf_dm(void) { #if defined(CONFIG_DM) \u0026amp;\u0026amp; CONFIG_VAL(SYS_MALLOC_F_LEN)  int ret; bootstage_start(BOOTSTAGE_ID_ACCUM_DM_F, \u0026#34;dm_f\u0026#34;); ret = dm_init_and_scan(true);\t//这里为true  bootstage_accum(BOOTSTAGE_ID_ACCUM_DM_F); if (ret) return ret; #endif #ifdef CONFIG_TIMER_EARLY  ret = dm_timer_init(); if (ret) return ret; #endif  return 0; } static int initr_dm(void) { int ret; /* Save the pre-reloc driver model and start a new one */ gd-\u0026gt;dm_root_f = gd-\u0026gt;dm_root; gd-\u0026gt;dm_root = NULL; #ifdef CONFIG_TIMER  gd-\u0026gt;timer = NULL; #endif  bootstage_start(BOOTSTAGE_ID_ACCUM_DM_R, \u0026#34;dm_r\u0026#34;); ret = dm_init_and_scan(false);\t//这里为false  bootstage_accum(BOOTSTAGE_ID_ACCUM_DM_R); if (ret) return ret; return 0; } 两个均调用了dm_init_and_scan这个接口，这两个的关键区别在于参数的不同。\n 首先说明一下dts节点中的“u-boot,dm-pre-reloc”属性，当设置了这个属性时，则表示这个设备在relocate之前就需要使用。 当dm_init_and_scan的参数为true时，只会对带有“u-boot,dm-pre-reloc”属性的节点进行解析。而当参数为false的时候，则会对所有节点都进行解析。  DM初始化的大体步骤如下：\n 如上程序执行流程图，下面我们详细讲解几个函数。\n ① dm_init #  int dm_init(bool of_live) { int ret; if (gd-\u0026gt;dm_root) { dm_warn(\u0026#34;Virtual root driver already exists!\\n\u0026#34;); return -EINVAL; } INIT_LIST_HEAD(\u0026amp;DM_UCLASS_ROOT_NON_CONST); #if defined(CONFIG_NEEDS_MANUAL_RELOC)  fix_drivers(); fix_uclass(); fix_devices(); #endif  ret = device_bind_by_name(NULL, false, \u0026amp;root_info, \u0026amp;DM_ROOT_NON_CONST);\t//查找root_driver驱动，并绑定  if (ret) return ret; #if CONFIG_IS_ENABLED(OF_CONTROL) # if CONFIG_IS_ENABLED(OF_LIVE)  if (of_live) DM_ROOT_NON_CONST-\u0026gt;node = np_to_ofnode(gd-\u0026gt;of_root); else #endif  DM_ROOT_NON_CONST-\u0026gt;node = offset_to_ofnode(0); #endif  ret = device_probe(DM_ROOT_NON_CONST);\t//probe激活root_driver驱动  if (ret) return ret; return 0; } dm_init这个函数，名字起的容易让人误导，这个函数主要做的就是初始化了根设备root_driver，根据这个跟设备，初始化了global_data中的dm_root、uclass_root。\n② lists_bind_fdt #   我们通常会使用设备树来定义各种设备，所以这个函数才是主角。\n 这个函数主要用来查找子设备，并且根据查找到的子设备，进而查找对应驱动进行绑定！即：实现了driver和device的绑定。\nint lists_bind_fdt(struct udevice *parent, ofnode node, struct udevice **devp, bool pre_reloc_only) { struct driver *driver = ll_entry_start(struct driver, driver);\t//获得驱动列表的起始地址  const int n_ents = ll_entry_count(struct driver, driver);\t//获得驱动列表的总数量  const struct udevice_id *id; struct driver *entry; struct udevice *dev; bool found = false; const char *name, *compat_list, *compat; int compat_length, i; int result = 0; int ret = 0; if (devp) *devp = NULL; name = ofnode_get_name(node); log_debug(\u0026#34;bind node %s\\n\u0026#34;, name); compat_list = ofnode_get_property(node, \u0026#34;compatible\u0026#34;, \u0026amp;compat_length);\t//得到compatible属性，用于匹配driver驱动  if (!compat_list) { if (compat_length == -FDT_ERR_NOTFOUND) { log_debug(\u0026#34;Device \u0026#39;%s\u0026#39; has no compatible string\\n\u0026#34;, name); return 0; } dm_warn(\u0026#34;Device tree error at node \u0026#39;%s\u0026#39;\\n\u0026#34;, name); return compat_length; } /* * Walk through the compatible string list, attempting to match each * compatible string in order such that we match in order of priority * from the first string to the last. */ for (i = 0; i \u0026lt; compat_length; i += strlen(compat) + 1) { compat = compat_list + i; log_debug(\u0026#34; - attempt to match compatible string \u0026#39;%s\u0026#39;\\n\u0026#34;, compat); for (entry = driver; entry != driver + n_ents; entry++) {\t//循环判断所有驱动是否匹配\t ret = driver_check_compatible(entry-\u0026gt;of_match, \u0026amp;id, compat); if (!ret) break; } if (entry == driver + n_ents) continue; if (pre_reloc_only) { if (!ofnode_pre_reloc(node) \u0026amp;\u0026amp; !(entry-\u0026gt;flags \u0026amp; DM_FLAG_PRE_RELOC)) { log_debug(\u0026#34;Skipping device pre-relocation\\n\u0026#34;); return 0; } } log_debug(\u0026#34; - found match at \u0026#39;%s\u0026#39;: \u0026#39;%s\u0026#39; matches \u0026#39;%s\u0026#39;\\n\u0026#34;, entry-\u0026gt;name, entry-\u0026gt;of_match-\u0026gt;compatible, id-\u0026gt;compatible); ret = device_bind_with_driver_data(parent, entry, name, id-\u0026gt;data, node, \u0026amp;dev);\t//该函数，用于创建udevice对象，并与查找到的driver绑定  if (ret == -ENODEV) { log_debug(\u0026#34;Driver \u0026#39;%s\u0026#39; refuses to bind\\n\u0026#34;, entry-\u0026gt;name); continue; } if (ret) { dm_warn(\u0026#34;Error binding driver \u0026#39;%s\u0026#39;: %d\\n\u0026#34;, entry-\u0026gt;name, ret); return ret; } else { found = true; if (devp) *devp = dev; } break; } if (!found \u0026amp;\u0026amp; !result \u0026amp;\u0026amp; ret != -ENODEV) log_debug(\u0026#34;No match for node \u0026#39;%s\u0026#39;\\n\u0026#34;, name); return result; } lists_bind_fdt这个函数，主要用来扫描设备树中的各个节点；\n根据扫描到的udevice设备信息，通过compatible来匹配compatible相同的driver，匹配成功后，就会创建对应的struct udevice结构体，它会同时指向设备资源和driver，这样设备资源和driver就绑定在一起了。\n3.7、DM模型——probe探测函数的执行 #   上述，完成了DM模型的初始化，但是我们只是建立了driver和udevice的绑定关系，那么何时调用到我们驱动中的probe探测函数呢？uclass与driver又何时匹配的呢？\n 上文呢，dm_init只是负责初始化并绑定了udevice和driver，那么probe探测函数的执行，当然是在该驱动初始化的时候喽！\n 下文以mmc驱动为例！其初始化流程如下：\n 详细代码在这里就不展开来叙述了！\n在MMC驱动初始化后，有没有注意到mmc_probe这个函数，该函数就是间接调用了我们驱动编写的probe函数。\n执行流程在上面已经很清楚了：根据uclass_id，调用``uclass_get_device_by_seq来得到udevice，进而调用device_probe来找到对应驱动的probe`。\nint device_probe(struct udevice *dev) { const struct driver *drv; int ret; int seq; if (!dev) return -EINVAL; if (dev-\u0026gt;flags \u0026amp; DM_FLAG_ACTIVATED) return 0; drv = dev-\u0026gt;driver;\t//获取driver  assert(drv); ret = device_ofdata_to_platdata(dev); if (ret) goto fail; /* Ensure all parents are probed */ if (dev-\u0026gt;parent) {\t//父设备probe  ret = device_probe(dev-\u0026gt;parent); if (ret) goto fail; /* * The device might have already been probed during * the call to device_probe() on its parent device * (e.g. PCI bridge devices). Test the flags again * so that we don\u0026#39;t mess up the device. */ if (dev-\u0026gt;flags \u0026amp; DM_FLAG_ACTIVATED) return 0; } seq = uclass_resolve_seq(dev); if (seq \u0026lt; 0) { ret = seq; goto fail; } dev-\u0026gt;seq = seq; dev-\u0026gt;flags |= DM_FLAG_ACTIVATED; /* * Process pinctrl for everything except the root device, and * continue regardless of the result of pinctrl. Don\u0026#39;t process pinctrl * settings for pinctrl devices since the device may not yet be * probed. */ if (dev-\u0026gt;parent \u0026amp;\u0026amp; device_get_uclass_id(dev) != UCLASS_PINCTRL) pinctrl_select_state(dev, \u0026#34;default\u0026#34;); if (CONFIG_IS_ENABLED(POWER_DOMAIN) \u0026amp;\u0026amp; dev-\u0026gt;parent \u0026amp;\u0026amp; (device_get_uclass_id(dev) != UCLASS_POWER_DOMAIN) \u0026amp;\u0026amp; !(drv-\u0026gt;flags \u0026amp; DM_FLAG_DEFAULT_PD_CTRL_OFF)) { ret = dev_power_domain_on(dev); if (ret) goto fail; } ret = uclass_pre_probe_device(dev); if (ret) goto fail; if (dev-\u0026gt;parent \u0026amp;\u0026amp; dev-\u0026gt;parent-\u0026gt;driver-\u0026gt;child_pre_probe) { ret = dev-\u0026gt;parent-\u0026gt;driver-\u0026gt;child_pre_probe(dev); if (ret) goto fail; } /* Only handle devices that have a valid ofnode */ if (dev_of_valid(dev)) { /* * Process \u0026#39;assigned-{clocks/clock-parents/clock-rates}\u0026#39; * properties */ ret = clk_set_defaults(dev, 0); if (ret) goto fail; } if (drv-\u0026gt;probe) {\tret = drv-\u0026gt;probe(dev);\t//调用驱动的probe  if (ret) goto fail; } ret = uclass_post_probe_device(dev); if (ret) goto fail_uclass; if (dev-\u0026gt;parent \u0026amp;\u0026amp; device_get_uclass_id(dev) == UCLASS_PINCTRL) pinctrl_select_state(dev, \u0026#34;default\u0026#34;); return 0; fail_uclass: if (device_remove(dev, DM_REMOVE_NORMAL)) { dm_warn(\u0026#34;%s: Device \u0026#39;%s\u0026#39; failed to remove on error path\\n\u0026#34;, __func__, dev-\u0026gt;name); } fail: dev-\u0026gt;flags \u0026amp;= ~DM_FLAG_ACTIVATED; dev-\u0026gt;seq = -1; device_free(dev); return ret; } 主要工作归纳如下：\n 根据udevice获取driver 然后判断是否父设备被probe 对父设备进行probe 调用driver的probe函数  3.8、DM模型——uclass与uclass_driver绑定 #   上述完成了driver的probe函数调用，基本底层都已经准备好了，uclass何时与uclass_driver绑定，给上层提供统一的API呢？\n uclass与uclass_driver绑定，也是在驱动probe之后，确保该驱动存在，设备存在，最后为该驱动绑定uclass与uclass_driver，为上层提供统一接口。\n 以根据MMC驱动为例\n 回到上文的驱动流程图，看到mmc_do_preinit这个函数了嘛？里面调用了ret = uclass_get(UCLASS_MMC, \u0026amp;uc);，该函数才是真正的将uclass与uclass_driver绑定。\nint uclass_get(enum uclass_id id, struct uclass **ucp) { struct uclass *uc; *ucp = NULL; uc = uclass_find(id); if (!uc) return uclass_add(id, ucp); *ucp = uc; return 0; } uclass_get主要实现了：根据uclass_id查找对应的uclass是否被添加到global_data-\u0026gt;uclass_root链表中，如果没有添加到，就调用uclass_add函数，实现uclass与uclass_driver的绑定，并将其添加到global_data-\u0026gt;uclass_root链表中。\nstatic int uclass_add(enum uclass_id id, struct uclass **ucp) { struct uclass_driver *uc_drv; struct uclass *uc; int ret; *ucp = NULL; uc_drv = lists_uclass_lookup(id);\t//根据uclass_id查找到对应的driver  if (!uc_drv) { debug(\u0026#34;Cannot find uclass for id %d: please add the UCLASS_DRIVER() declaration for this UCLASS_... id\\n\u0026#34;, id); /* * Use a strange error to make this case easier to find. When * a uclass is not available it can prevent driver model from * starting up and this failure is otherwise hard to debug. */ return -EPFNOSUPPORT; } uc = calloc(1, sizeof(*uc)); if (!uc) return -ENOMEM; if (uc_drv-\u0026gt;priv_auto_alloc_size) { uc-\u0026gt;priv = calloc(1, uc_drv-\u0026gt;priv_auto_alloc_size); if (!uc-\u0026gt;priv) { ret = -ENOMEM; goto fail_mem; } } uc-\u0026gt;uc_drv = uc_drv;\t//uclass与uclass_driver绑定  INIT_LIST_HEAD(\u0026amp;uc-\u0026gt;sibling_node); INIT_LIST_HEAD(\u0026amp;uc-\u0026gt;dev_head); list_add(\u0026amp;uc-\u0026gt;sibling_node, \u0026amp;DM_UCLASS_ROOT_NON_CONST);\t//添加到global_data-\u0026gt;uclass_root链表中  if (uc_drv-\u0026gt;init) { ret = uc_drv-\u0026gt;init(uc); if (ret) goto fail; } *ucp = uc; return 0; fail: if (uc_drv-\u0026gt;priv_auto_alloc_size) { free(uc-\u0026gt;priv); uc-\u0026gt;priv = NULL; } list_del(\u0026amp;uc-\u0026gt;sibling_node); fail_mem: free(uc); return ret; } 好啦，到这里基本就把Uboot的DM模型全部理清楚啦，耗时一个周，总感觉想要自己去讲明白，真的不是一件容易的事情呢！\n如果对你们有帮助，记得点个赞哦！\n3.9 参考文档 #  [1] : https://www.dazhuanlan.com/archevalier/topics/1323360\n[2] : https://www.cnblogs.com/gs1008612/p/8253213.html\n[3] : https://blog.csdn.net/kunkliu/article/details/103168591\n[4] : https://blog.csdn.net/ooonebook/article/details/53234020\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":31,"href":"/docs/linux/linux_memory_manage/%E4%B8%89%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E7%AE%A1%E7%90%86/","title":"三、虚拟地址空间管理","section":"Linux 内存管理","content":"Linux内存管理 | 三、虚拟地址空间管理 #  上一节，我们主要了解了虚拟内存空间的布局情况，趁热打铁，我们直接从源代码的视角，来看一下Linux内核是如何管理虚拟内存空间的。\n废话不多说，直接开始！\n1、用户态空间管理 #  读完上一节我们知道，用户态的布局情况如下：\n我们运行的可执行程序，被加载进内存后，会作为一个进程存在，这个进程Linux内核会将其抽象成一个结构体。没错，它就是task_struct。\n1.1 task_struct结构体 #  task_struct结构体是进程的抽象，进程所涉及到的内容非常多，下面只列举出一些重要的数据结构，方面理解。\n// include/linux/sched.h struct task_struct { ... pid_t\tpid;\t//\t进程PID  pid_t\ttgid;\t//\t线程PID  struct files_struct\t*files;\t// 进程打开的文件信息  struct mm_struct\t*mm;\t//\t进程虚拟内存空间的内存描述符  ... } 如上，进程抽象为task_struct结构体，通过mm_struct结构体来管理虚拟内存空间。\n1.2 mm_struct结构体 #  每个进程都有唯一的 mm_struct 结构体，也就是前边提到的每个进程的虚拟地址空间都是独立，互不干扰的。\nmm_struct的结构体如下：\n//\tinclude/linux/mm_types.h struct mm_struct { ... struct { ... unsigned long task_size;\t/* size of task vm space */ ... unsigned long mmap_base;\t/* base of mmap area */ unsigned long total_vm;\t/* Total pages mapped */ unsigned long locked_vm;\t/* Pages that have PG_mlocked set */ unsigned long pinned_vm;\t/* Refcount permanently increased */ unsigned long data_vm;\t/* VM_WRITE \u0026amp; ~VM_SHARED \u0026amp; ~VM_STACK */ unsigned long exec_vm;\t/* VM_EXEC \u0026amp; ~VM_WRITE \u0026amp; ~VM_STACK */ unsigned long stack_vm;\t/* VM_STACK */ unsigned long start_code, end_code, start_data, end_data; unsigned long start_brk, brk, start_stack; unsigned long arg_start, arg_end, env_start, env_end; ... struct vm_area_struct *mmap;\t/* list of VMAs */ struct rb_root mm_rb; ... }__randomize_layout; ... }  1.3 内核态和用户态的划分 #  mm_struct里面定义的task_size变量，就是用来划分虚拟内存的用户空间和内核空间的。\nunsigned long task_size; task_size也就是两者的分界线，下面我们看下task_size是如何被赋值的。\n 当我们执行一个新的进程的时候，Linux内核会执行load_elf_binary的API接口，进而调用setup_new_exec函数来实现新进程的创建。\n在setup_new_exec函数中，会执行\ncurrent-\u0026gt;mm-\u0026gt;task_size = TASK_SIZE; 这个TASK_SIZE就是我们设置的内核空间地址和用户空间地址的分界线，由我们自定义配置。\n#ifdef CONFIG_X86_32 /* * User space process size: 3GB (default). */ #define TASK_SIZE\tPAGE_OFFSET #define TASK_SIZE_MAX\tTASK_SIZE /* config PAGE_OFFSET hex default 0xC0000000 depends on X86_32 */ #else /* * User space process size. 47bits minus one guard page. */ #define TASK_SIZE_MAX\t((1UL \u0026lt;\u0026lt; 47) - PAGE_SIZE) #define TASK_SIZE\t(test_thread_flag(TIF_ADDR32) ? \\ IA32_PAGE_OFFSET : TASK_SIZE_MAX) ...... 这里我们只需要知道TASK_SIZE默认值3为PAGE_OFFSET，并且默认为0xC0000000为分界线的，即用户空间3GB，内核空间1GB；当然这个可以由我们动态配置，可以配置PAGE_OFFSET为0x80000000，即用户空间和内核空间均为2GB，取决于我们的应用场合，当你看到与我们讲解不同时，也不用大惊小怪。\n 以上，表达的概念很简单，如下图：\n  1.4 位置信息描述 #  我们知道用户态内存空间分为几个区域：代码段、数据段、BSS段、堆、文件映射和匿名映射区、栈等几个部分，同样在mm_struct中，定义了这些区域的统计信息和位置。\nunsigned long mmap_base;\t/* base of mmap area */ unsigned long total_vm;\t/* Total pages mapped */ unsigned long locked_vm;\t/* Pages that have PG_mlocked set */ unsigned long pinned_vm;\t/* Refcount permanently increased */ unsigned long data_vm;\t/* VM_WRITE \u0026amp; ~VM_SHARED \u0026amp; ~VM_STACK */ unsigned long exec_vm;\t/* VM_EXEC \u0026amp; ~VM_WRITE \u0026amp; ~VM_STACK */ unsigned long stack_vm;\t/* VM_STACK */ unsigned long start_code, end_code, start_data, end_data; unsigned long start_brk, brk, start_stack; unsigned long arg_start, arg_end, env_start, env_end;  total_vm：总映射页面的数目。（这么大的虚拟内存空间，不可能全部映射到真实的物理内存，都是按需映射的，这里表示当前映射的页面总数目）   由于物理内存比较小，当内存吃紧的时候，就会发生换入换出的操作，即将暂时不用的页面换出到硬盘上，有的页面比较重要，不能换出。\n  locked_vm：被锁定不能换出的页面 pinned_vm ：不能换出、也不能移动的页面 data_vm：存放数据页的页的数目 exec_vm：存放可执行文件的页的数目 stack_vm：存放堆栈信息页的数目 start_code、end_code：表示可执行代码开始和结束的位置 start_data、end_data：表示已初始化数据的开始位置和结束位置 start_brk、brk：堆的起始地址，结束地址 start_stack：是栈的起始位置，在 RBP 寄存器中存储，栈的结束位置也就是栈顶指针，在 RSP 寄存器中存储。在栈中内存地址的增长方向也是由高地址向低地址增长。 arg_start、arg_end：参数列表的起始位置和结束位置 env_start、env_end：环境变量的起始位置和结束位置   整体的布局情况如下：\n 1.5 区域属性描述 #  尽管已经有了一些变量来描述每一个段的信息，但是Linux内核在mm_struct结构体里面，还有一个专门的数据结构vm_area_struct来管理每个区域的属性。\nstruct vm_area_struct *mmap;\t/* list of VMAs */ struct rb_root mm_rb; mmap：为一个单链表，将所有的区域串联起来\nmm_rb：为一个红黑树，方便查找和修改内存区域。\n 下面看一下vm_area_struct数据结构：\nstruct vm_area_struct { /* The first cache line has the info for VMA tree walking. */ unsigned long vm_start;\t/* Our start address within vm_mm. */ unsigned long vm_end;\t/* The first byte after our end address within vm_mm. */ /* linked list of VM areas per task, sorted by address */ struct vm_area_struct *vm_next, *vm_prev; struct rb_node vm_rb; struct mm_struct *vm_mm;\t/* The address space we belong to. */ struct list_head anon_vma_chain; /* Serialized by mmap_sem \u0026amp; * page_table_lock */ struct anon_vma *anon_vma;\t/* Serialized by page_table_lock */ /* Function pointers to deal with this struct. */ const struct vm_operations_struct *vm_ops; struct file * vm_file;\t/* File we map to (can be NULL). */ void * vm_private_data;\t/* was vm_pte (shared mem) */ } __randomize_layout;   vm_start、vm_end：为该区域在用户空间的起始和结束地址\n  vm_next、vm_prev：将该区域添加到链表上，便于管理。\n  vm_rb：将这个区域放到红黑树上\n  vm_ops：对该区域可以进行的内存操作\n  anon_vma：匿名映射\n  vm_file：文件映射\n   用户态空间的每个区域都由该结构体来管理，最终形成下面的这个结构：\n  顺便介绍一下 我的圈子：高级工程师聚集地，期待大家的加入。\n 2、内核态空间管理 #  上面，我们从源码角度了解了用户态空间管理，下面我们看内核态空间管理。\n回顾一下，我们内核态的布局情况是怎么样的呢，还记得吗？\n我们要知道：\n 内核态的虚拟空间和任何一个进程都没有关系，所有的进程看到的内核态虚拟空间都是一样的。 在内核态，我们直接操作的依旧是虚拟地址，而非物理地址 不同CPU结构下，内核态空间的布局格式是不变的，但是大小会有所调整，比如ARM和X86的大小空间有所不同。   内核态空间管理并不像用户态那样使用结构体来统一管理，而是直接使用宏来定义每个区域的分界线，\n 下面我们以x86架构来分析内核态空间的管理\n 2.1 分界线定义 #  /* * User space process size: 3GB (default). */ #define TASK_SIZE\tPAGE_OFFSET  /* PAGE_OFFSET - the virtual address of the start of the kernel image */ #define PAGE_OFFSET\t((unsigned long)__PAGE_OFFSET)  #define __PAGE_OFFSET\t__PAGE_OFFSET_BASE  #define __PAGE_OFFSET_BASE\t_AC(CONFIG_PAGE_OFFSET, UL)  config PAGE_OFFSET hex default 0xB0000000 if VMSPLIT_3G_OPT default 0x80000000 if VMSPLIT_2G default 0x78000000 if VMSPLIT_2G_OPT default 0x40000000 if VMSPLIT_1G default 0xC0000000 depends on X86_32 TASK_SIZE：内核态空间与用户态空间的分界线\nPAGE_OFFSET：该宏表示内核镜像起始的虚拟地址。\nCONFIG_PAGE_OFFSET：这个宏定义的值，根据实际情况自行设定，默认为0XC0000000，可以设置为0X80000000等。\n以上，TASK_SIZE就被定义为0XC0000000作为用户态空间和内核态空间的分界线，将4G虚拟内存分配为3G/1G结构。\n2.2 直接映射区定义 #  直接映射区是定义在PAGE_OFFSET和high_memory之间的区域。\n PAGE_OFFSET：表示内核镜像的起始地址，上文已经说明。 high_memory也是表示的就是896M这个值，表示高端内存的分界线。   顺便说明以下，TASK_SIZE和PAGE_OFFSET在不同架构下是不同的，在ARM架构下，两者并不相等，本文以X86架构为例\n 2.3 安全保护区定义 #  系统会在high_memory和VMALLOC_START之间预留8M的安全保护区，防止访问越界。\nVMALLOC_OFFSET表示的是内核动态映射区的偏移，也就是所谓的安全保护区。\n#define VMALLOC_START\t(((unsigned long)high_memory + VMALLOC_OFFSET) \u0026amp; ~(VMALLOC_OFFSET-1))  #define VMALLOC_OFFSET\t(8*1024*1024) 可以很清楚的看到VMALLOC_OFFSET定义了8M的空间，VMALLOC_START在high_memory基础上，偏移了VMALLOC_OFFSET 8M空间大小作为安全保护区，以防越界访问。\n2.3 动态映射区定义 #  VMALLOC_START和VMALLOC_END之间称为内核动态映射区。\n和用户态进程使用 malloc 申请内存一样，在这块动态映射区内核是使用 vmalloc 进行内存分配。\n#define VMALLOC_START\t(((unsigned long)high_memory + VMALLOC_OFFSET) \u0026amp; ~(VMALLOC_OFFSET-1))  #ifdef CONFIG_HIGHMEM # define VMALLOC_END\t(PKMAP_BASE - 2 * PAGE_SIZE) #else # define VMALLOC_END\t(LDT_BASE_ADDR - 2 * PAGE_SIZE) #endif  PKMAP_BASE：是永久映射区的起始地址。\nVMALLOC_END：在永久映射区的起始地址下，偏移2个PAGE_SIZE作为安全保护区。\n2.4 永久映射区定义 #  PKMAP_BASE 到 FIXADDR_START 的空间称为永久内核映射，在内核的这段虚拟地址空间中允许建立与物理高端内存的长期映射关系。\n 比如内核通过 alloc_pages() 函数在物理内存的高端内存中申请获取到的物理内存页，这些物理内存页可以通过调用 kmap 映射到永久映射区中。\n #define PKMAP_BASE\t\\ ((LDT_BASE_ADDR - PAGE_SIZE) \u0026amp; PMD_MASK)  #define LDT_BASE_ADDR\t\\ ((CPU_ENTRY_AREA_BASE - PAGE_SIZE) \u0026amp; PMD_MASK)  #define CPU_ENTRY_AREA_BASE\t\\ ((FIXADDR_TOT_START - PAGE_SIZE * (CPU_ENTRY_AREA_PAGES + 1)) \\ \u0026amp; PMD_MASK)  #define FIXADDR_TOT_START\t(FIXADDR_TOP - FIXADDR_TOT_SIZE)  #define FIXADDR_TOP\t((unsigned long)__FIXADDR_TOP)  #define FIXADDR_TOT_SIZE\t(__end_of_fixed_addresses \u0026lt;\u0026lt; PAGE_SHIFT)  unsigned long __FIXADDR_TOP = 0xfffff000; #define PMD_MASK\t(~(PMD_SIZE - 1))  #define PAGE_SHIFT\t12 #define PAGE_SIZE\t(_AC(1,UL) \u0026lt;\u0026lt; PAGE_SHIFT)  #define CPU_ENTRY_AREA_PAGES\t(NR_CPUS * 40)  #define FIXADDR_START\t(FIXADDR_TOP - FIXADDR_SIZE)   PKMAP_BASE：是永久映射区的起始地址，它经过一系列的计算得到，具体可以看上面的宏定义，我们大概了解就行了，不同体系结构的定义位置还不一样。 FIXADDR_START：是固定映射区的起始地址，也是永久映射区的结束地址。  2.5 固定映射区定义 #  FIXADDR_START到FIXADDR_TOP的空间称为固定映射区，主要用于满足特殊的需求。\n#define FIXADDR_TOP\t((unsigned long)__FIXADDR_TOP)  unsigned long __FIXADDR_TOP = 0xfffff000; 固定映射区中的虚拟地址，可以自由映射到物理内存的高端地址空间上，特点是其映射的虚拟地址是不变的，物理地址是可以改变的。\n2.6 临时映射区定义 #  最后FIXADDR_TOP到0xFFFFFFFF之间的区域称为临时映射区。\n 它主要用来做什么呢，网上举的一个例子，大家参考以下。\n 假设用户态的进程要映射一个文件到内存中，先要映射用户态进程空间的一段虚拟地址到物理内存，然后将文件内容写入这个物理内存供用户态进程访问。\n给用户态进程分配物理内存页可以通过 alloc_pages()，分配完毕后，按说将用户态进程虚拟地址和物理内存的映射关系放在用户态进程的页表中，就完事大吉了。这个时候，用户态进程可以通过用户态的虚拟地址，也即 0 至 3G 的部分，经过页表映射后访问物理内存，并不需要内核态的虚拟地址里面也划出一块来，映射到这个物理内存页。\n但是如果要把文件内容写入物理内存，这件事情要内核来干了，这就只好通过 kmap_atomic 做一个临时映射，写入物理内存完毕后，再 kunmap_atomic 来解映射即可。\n以上，就是内核态空间的布局以及管理。\n3、总结 #  该篇文章，主要从源码角度来了解用户态空间和内核态空间是如何管理的，挪用大佬的一个图片，结合上面所讲的，相信很快就能茅塞顿开。\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":32,"href":"/docs/linux/linux_debug/%E4%B8%80%E6%96%87%E7%A7%92%E6%87%82linux%E5%86%85%E6%A0%B8%E8%B0%83%E8%AF%95%E5%B7%A5%E5%85%B7devmem/","title":"【一文秒懂】Linux内核调试工具——devmem","section":"Linux 调试工具","content":"【一文秒懂】Linux内核调试工具——devmem #  1、介绍 #  我们在底层开发过程中，经常需要在终端查看或者修改设备寄存器的值，有这样一个工具\u0026mdash;-devmem，可用于读取或者修改物理寄存器的值，非常方便！\n简而言之，devmem就是在Linux命令行模式下，直接操作我们设备寄存器的值！\n2、如何使用 #  2.1 配置devmem #  进入menuconfig选项中，按下/搜索关键词即可！\n2.2、使用devmem #   进入Linux后，输入devmem -h查看帮助信息即可！\n Usage: devmem ADDRESS [WIDTH [VALUE]] Read/write from physical address ADDRESS Address to act upon WIDTH Width (8/16/...) VALUE Data to be written []内部为可选内容，比较简单，这里直接上使用代码！\n 读物理内存  devmem 0x10000000\t#读指定的物理内存值 devmem 0x10000000 16\t#读16bit物理内存的值  写物理内存  devmem 0x10000000 32 0x00000000\t#以32bit写入给定的值到指定物理内存 devmem 0x10000000 8 0x010\t#以8bit写入给定的值到指定物理内存  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":33,"href":"/docs/linux/linux_mmc_subsystem/mmc%E5%AD%90%E7%B3%BB%E7%BB%9F%E5%9B%9Bmmc%E6%8E%A7%E5%88%B6%E5%99%A8%E9%A9%B1%E5%8A%A8%E5%B1%82/","title":"【MMC子系统】四、MMC控制器驱动层","section":"Linux MMC 子系统","content":"【MMC子系统】四、MMC控制器驱动层 #  MMC控制器驱动层一般为chip manufacturer做的事，不同的芯片实现方式不尽相同。\n Linux内核源码，相当大的一部分都是由Device Drivers程序代码组成，其次另一大部分就是那些你从来都没有听说过的Filesystem Format组成，真正核心的代码非常短小精悍的。\n 当然，设备驱动程序也有一套既定的框架，按照框架来编写，实现对应的接口就可以了，在这里，我们主要分析一下MMC控制器驱动的实现框架，不拘泥于细节。\n 下文以sunxi-mmc.c为例来分析，基于Linux4.19\n  4.1 通用驱动框架 #  static int sunxi_mmc_probe(struct platform_device *pdev) { ..... } static const struct of_device_id sunxi_mmc_of_match[] = { { .compatible = \u0026#34;allwinner,sun4i-a10-mmc\u0026#34;, .data = \u0026amp;sun4i_a10_cfg }, { .compatible = \u0026#34;allwinner,sun5i-a13-mmc\u0026#34;, .data = \u0026amp;sun5i_a13_cfg }, { .compatible = \u0026#34;allwinner,sun7i-a20-mmc\u0026#34;, .data = \u0026amp;sun7i_a20_cfg }, { .compatible = \u0026#34;allwinner,sun8i-a83t-emmc\u0026#34;, .data = \u0026amp;sun8i_a83t_emmc_cfg }, { .compatible = \u0026#34;allwinner,sun9i-a80-mmc\u0026#34;, .data = \u0026amp;sun9i_a80_cfg }, { .compatible = \u0026#34;allwinner,sun50i-a64-mmc\u0026#34;, .data = \u0026amp;sun50i_a64_cfg }, { .compatible = \u0026#34;allwinner,sun50i-a64-emmc\u0026#34;, .data = \u0026amp;sun50i_a64_emmc_cfg }, { /* sentinel */ } }; MODULE_DEVICE_TABLE(of, sunxi_mmc_of_match); static const struct dev_pm_ops sunxi_mmc_pm_ops = { SET_RUNTIME_PM_OPS(sunxi_mmc_runtime_suspend, sunxi_mmc_runtime_resume, NULL) }; static struct platform_driver sunxi_mmc_driver = { .driver = { .name\t= \u0026#34;sunxi-mmc\u0026#34;, .of_match_table = of_match_ptr(sunxi_mmc_of_match), .pm = \u0026amp;sunxi_mmc_pm_ops, }, .probe\t= sunxi_mmc_probe, .remove\t= sunxi_mmc_remove, }; module_platform_driver(sunxi_mmc_driver); MODULE_DESCRIPTION(\u0026#34;Allwinner\u0026#39;s SD/MMC Card Controller Driver\u0026#34;); MODULE_LICENSE(\u0026#34;GPL v2\u0026#34;); MODULE_AUTHOR(\u0026#34;David Lanzendörfer \u0026lt;david.lanzendoerfer@o2s.ch\u0026gt;\u0026#34;); MODULE_ALIAS(\u0026#34;platform:sunxi-mmc\u0026#34;); 这套基本的框架，老生常谈，其主要功能就是：按照of_match_table匹配表，来实现platform_device和platform_driver的匹配，然后执行probe函数。\n 4.2 注册与注销函数 #  static int sunxi_mmc_probe(struct platform_device *pdev) { ..... } static int sunxi_mmc_remove(struct platform_device *pdev) { ...... } 比较重要的两个函数，我们一般insmod xxx.ko后，执行完_init函数后，最终如果设备树和驱动匹配成功，会调用probe函数，相同，卸载驱动时，也会调用到remove函数。\n4.2.1 probe函数 #   probe函数很长，我们挑重点来了解\n static int sunxi_mmc_probe(struct platform_device *pdev) { struct sunxi_mmc_host *host; struct mmc_host *mmc; int ret; mmc = mmc_alloc_host(sizeof(struct sunxi_mmc_host), \u0026amp;pdev-\u0026gt;dev); if (!mmc) { dev_err(\u0026amp;pdev-\u0026gt;dev, \u0026#34;mmc alloc host failed\\n\u0026#34;); return -ENOMEM; } platform_set_drvdata(pdev, mmc); host = mmc_priv(mmc); host-\u0026gt;dev = \u0026amp;pdev-\u0026gt;dev; host-\u0026gt;mmc = mmc; spin_lock_init(\u0026amp;host-\u0026gt;lock); // 1. 获取设备树资源  ret = sunxi_mmc_resource_request(host, pdev); if (ret) goto error_free_host; ...... // 2. 初始化MMC控制器  mmc-\u0026gt;ops\t= \u0026amp;sunxi_mmc_ops; mmc-\u0026gt;max_blk_count\t= 8192; mmc-\u0026gt;max_blk_size\t= 4096; mmc-\u0026gt;max_segs\t= PAGE_SIZE / sizeof(struct sunxi_idma_des); mmc-\u0026gt;max_seg_size\t= (1 \u0026lt;\u0026lt; host-\u0026gt;cfg-\u0026gt;idma_des_size_bits); mmc-\u0026gt;max_req_size\t= mmc-\u0026gt;max_seg_size * mmc-\u0026gt;max_segs; /* 400kHz ~ 52MHz */ mmc-\u0026gt;f_min\t= 400000; mmc-\u0026gt;f_max\t= 52000000; mmc-\u0026gt;caps\t|= MMC_CAP_MMC_HIGHSPEED | MMC_CAP_SD_HIGHSPEED | MMC_CAP_ERASE | MMC_CAP_SDIO_IRQ; if (host-\u0026gt;cfg-\u0026gt;clk_delays || host-\u0026gt;use_new_timings) mmc-\u0026gt;caps |= MMC_CAP_1_8V_DDR | MMC_CAP_3_3V_DDR; ret = mmc_of_parse(mmc); if (ret) goto error_free_dma; /* TODO: This driver doesn\u0026#39;t support HS400 mode yet */ mmc-\u0026gt;caps2 \u0026amp;= ~MMC_CAP2_HS400; ret = sunxi_mmc_init_host(host); if (ret) goto error_free_dma; ....... // 3. 将mmc控制器加入到子系统中  ret = mmc_add_host(mmc); if (ret) goto error_free_dma; dev_info(\u0026amp;pdev-\u0026gt;dev, \u0026#34;initialized, max. request size: %u KB%s\\n\u0026#34;, mmc-\u0026gt;max_req_size \u0026gt;\u0026gt; 10, host-\u0026gt;use_new_timings ? \u0026#34;, uses new timings mode\u0026#34; : \u0026#34;\u0026#34;); return 0; error_free_dma: dma_free_coherent(\u0026amp;pdev-\u0026gt;dev, PAGE_SIZE, host-\u0026gt;sg_cpu, host-\u0026gt;sg_dma); error_free_host: mmc_free_host(mmc); return ret; } 函数作用：从设备树获取配置信息，并初始化mmc控制器，最后将mmc加入到子系统中。\n 上面代码已经作了简单注释\n 4.2.2 remove函数 #  remove函数看起来就比较简单了，就是probe函数的反操作\nstatic int sunxi_mmc_remove(struct platform_device *pdev) { struct mmc_host\t*mmc = platform_get_drvdata(pdev); struct sunxi_mmc_host *host = mmc_priv(mmc); // 1. 移除子系统  mmc_remove_host(mmc); pm_runtime_force_suspend(\u0026amp;pdev-\u0026gt;dev); disable_irq(host-\u0026gt;irq); sunxi_mmc_disable(host); dma_free_coherent(\u0026amp;pdev-\u0026gt;dev, PAGE_SIZE, host-\u0026gt;sg_cpu, host-\u0026gt;sg_dma); // 2. 释放mmc内存  mmc_free_host(mmc); return 0; } 函数作用：将mmc移除子系统，并释放内存。\n  更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n \u0026amp;nbsp\n4.3 ops函数实现 #  了解过基本驱动框架的都知道，最为核心的就是ops相关的接口了，上层调用底层代码，全靠它。\n在probe函数中，我们看到mmc-\u0026gt;ops = \u0026amp;sunxi_mmc_ops的代码，就是注册了ops结构体，最后通过mmc_add_host接口，打通核心层与MMC控制器驱动层的界限。\nstatic const struct mmc_host_ops sunxi_mmc_ops = { .request\t= sunxi_mmc_request, .set_ios\t= sunxi_mmc_set_ios, .get_ro\t= mmc_gpio_get_ro, .get_cd\t= mmc_gpio_get_cd, .enable_sdio_irq = sunxi_mmc_enable_sdio_irq, .start_signal_voltage_switch = sunxi_mmc_volt_switch, .hw_reset\t= sunxi_mmc_hw_reset, .card_busy\t= sunxi_mmc_card_busy, };  .request：上层发送命令请求 .set_ios：上层设置时钟频率，总线数量的接口 .get_ro：表示卡的读写状态 .get_cd：检测卡是否存在的接口 .enable_sdio_irq：提供给上层打开sdio中断的接口 .hw_reset：硬件重置接口 .card_busy：反映卡的状态接口  具体怎么实现，就是chip manufacturer做的事情，我们这里只需要知道，上层通过封装的接口，最终通过ops-\u0026gt;xxx函数来将控制寄存器进行数据传输。\n4.4 PM接口 #  PM就是我们说的Power Manager电源管理，用于功耗控制。\n#ifdef CONFIG_PM static int sunxi_mmc_runtime_resume(struct device *dev) { struct mmc_host\t*mmc = dev_get_drvdata(dev); struct sunxi_mmc_host *host = mmc_priv(mmc); int ret; ret = sunxi_mmc_enable(host); if (ret) return ret; sunxi_mmc_init_host(host); sunxi_mmc_set_bus_width(host, mmc-\u0026gt;ios.bus_width); sunxi_mmc_set_clk(host, \u0026amp;mmc-\u0026gt;ios); enable_irq(host-\u0026gt;irq); return 0; } static int sunxi_mmc_runtime_suspend(struct device *dev) { struct mmc_host\t*mmc = dev_get_drvdata(dev); struct sunxi_mmc_host *host = mmc_priv(mmc); /* * When clocks are off, it\u0026#39;s possible receiving * fake interrupts, which will stall the system. * Disabling the irq will prevent this. */ disable_irq(host-\u0026gt;irq); sunxi_mmc_reset_host(host); sunxi_mmc_disable(host); return 0; } #endif  其主要功能就是：确保休眠时，所有外设的时钟使能需要关闭，来确保功耗最低。\n MMC控制器驱动就是就是这么简单，不需要过多了解的，咱们就先不关心，聚焦于整个框架。\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":34,"href":"/docs/linux/bluetooth/bluetooth%E8%93%9D%E7%89%99%E5%BC%80%E5%8F%91%E5%9B%9Bble%E5%8D%8F%E8%AE%AE%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82%E6%B5%85%E6%9E%90/","title":"【Bluetooth蓝牙开发】四、BLE协议之物理层浅析","section":"Bluetooth蓝牙开发","content":"【Bluetooth|蓝牙开发】四、BLE协议之物理层浅析 #  1、前言 #  上文，通过对蓝牙协议框架进行整体了解，其包含BR/EDR((Basic Rate / Enhanced Data Rate))、AMP(Alternate MAC/PHYs)、LE(Low Energy)三种技术，不同技术对应不同的协议栈，本专栏目前对于BLE技术进行详解！\n==下面我们将BLE部分单独抽离出来，单独对其进行研究。==\n BLE的协议可分为Bluetooth Application和Bluetooth Core两大部分，而Bluetooth Core又包含BLE Controller和BLE Host两部分。\n 快把小本本拿起来，一定要记住！\n 我们先从Physical Layer开始分析\n2、Physical Channel #  任何一个通信系统，首先要确定的就是通信介质（物理通道，Physical Channel），BLE也不例外。在BLE协议中，“通信介质”的定义是由Physical Layer负责。\nPhysical Layer是这样描述BLE的通信介质的：\n  BLE属于无线通信，则其通信介质是一定频率范围下的频带资源（Frequency Band）\n  BLE的市场定位是个体和民用，因此使用免费的ISM频段（频率范围是2.400-2.4835 GHz）\n  为了同时支持多个设备，将整个频带分为40份，每份的带宽为2MHz，称作RF Channel。\n  经过上面的定义之后，BLE的物理通道划分已经明了了！ $$ 频点(f)=2402(MHz)+k*2(MHz),k=(0\u0026hellip;39) $$ 每个Channel的带宽为2MHz，如下图：\n 3、Physical Channel的细分 #  上面我们已经知道了，物理层被划分为了40个赛道，由于传输数据量的不同，为了更加充分利用好物理资源，进一步对通道进行了划分！\n40个Physical Channel物理通道分别划分为3个广播通道advertising channel，和37个Data Channel数据通道。\n对于数据量少，发送不频繁，时延不敏感的场景，使用广播通道通信。\n 例如一个传感器节点（如温度传感器），需要定时（如1s）向处理中心发送传感器数据（如温度）。\n针对这种场景，BLE的Link Layer采取了一种比较懒的处理方式\u0026mdash;-广播通信：\n 对于数据量大，发送频率高，时延较敏感的场景，使用数据通道。\n BLE为这种场景里面的通信双方建立单独的通道（data channel）。这就是连接（connection）的过程。\n 同时，为了增加信道容量，增大抗干扰能力，连接不会长期使用一个固定的Physical Channel，而是在多个通道（如37个）之间随机但有规律的切换，这就是BLE的跳频（Hopping）技术。\n对物理层的了解先止步于此，再往下面深入分析，意义不大。我们把重点放在BLE的Link Layer\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":35,"href":"/docs/linux/linux_led_subsystem/led%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90%E5%9B%9B%E6%A0%B8%E5%BF%83%E5%B1%82%E8%AF%A6%E8%A7%A3%E4%B8%80/","title":"【LED子系统深度剖析】四、核心层详解（一）","section":"Linux LED子系统","content":"【LED子系统深度剖析】四、核心层详解（一） #  1、前言 #   上篇文章我们了解了子系统的硬件驱动层，下面我们来分析驱动框架中核心层的实现以及作用。\n 在LED子系统框架中，核心层包括几个部分：核心层的实现部分（led-core.c）、sysfs文件节点创建（led-class.c）、触发功能实现(led-triggers.c、driver/leds/triggers/led-xxx.c)\n 其中，触发功能部分较为独立，我们暂且先不去分析。\n 我们先从led-class.c文件开始分析\n2、leds_init分析 #   该函数其主要是为了创建LED设备文件节点，方便用户通过节点直接访问。\n 该文件，我们直接拉下底部，我们直接看入口函数：leds_init\n2.1 相关数据结构 #  2.1.1 class #  /** * struct class - device classes * @name:\tName of the class. * @owner:\tThe module owner. * @class_groups: Default attributes of this class. * @dev_groups:\tDefault attributes of the devices that belong to the class. * @dev_kobj:\tThe kobject that represents this class and links it into the hierarchy. * @dev_uevent:\tCalled when a device is added, removed from this class, or a *\tfew other things that generate uevents to add the environment *\tvariables. * @devnode:\tCallback to provide the devtmpfs. * @class_release: Called to release this class. * @dev_release: Called to release the device. * @shutdown_pre: Called at shut-down time before driver shutdown. * @ns_type:\tCallbacks so sysfs can detemine namespaces. * @namespace:\tNamespace of the device belongs to this class. * @get_ownership: Allows class to specify uid/gid of the sysfs directories *\tfor the devices belonging to the class. Usually tied to *\tdevice\u0026#39;s namespace. * @pm:\tThe default device power management operations of this class. * @p:\tThe private data of the driver core, no one other than the *\tdriver core can touch this. * * A class is a higher-level view of a device that abstracts out low-level * implementation details. Drivers may see a SCSI disk or an ATA disk, but, * at the class level, they are all simply disks. Classes allow user space * to work with devices based on what they do, rather than how they are * connected or how they work. */ struct class { const char\t*name; struct module\t*owner; const struct attribute_group\t**class_groups; const struct attribute_group\t**dev_groups; struct kobject\t*dev_kobj; int (*dev_uevent)(struct device *dev, struct kobj_uevent_env *env); char *(*devnode)(struct device *dev, umode_t *mode); void (*class_release)(struct class *class); void (*dev_release)(struct device *dev); int (*shutdown_pre)(struct device *dev); const struct kobj_ns_type_operations *ns_type; const void *(*namespace)(struct device *dev); void (*get_ownership)(struct device *dev, kuid_t *uid, kgid_t *gid); const struct dev_pm_ops *pm; struct subsys_private *p; }; 结构体名称：class\n文件位置：include/linux/device.h\n主要作用：设备类，表示某一类设备，在此是为了创建led设备类，源代码为：static struct class *leds_class;\n2.2 实现流程 #  static int __init leds_init(void) { leds_class = class_create(THIS_MODULE, \u0026#34;leds\u0026#34;);\t//\t创建leds文件节点  if (IS_ERR(leds_class)) return PTR_ERR(leds_class); leds_class-\u0026gt;pm = \u0026amp;leds_class_dev_pm_ops; leds_class-\u0026gt;dev_groups = led_groups;\treturn 0; } static void __exit leds_exit(void) { class_destroy(leds_class); } subsys_initcall(leds_init); module_exit(leds_exit); MODULE_AUTHOR(\u0026#34;John Lenz, Richard Purdie\u0026#34;); MODULE_LICENSE(\u0026#34;GPL\u0026#34;); MODULE_DESCRIPTION(\u0026#34;LED Class Interface\u0026#34;); 函数介绍：leds_init该函数在内核在加载的时候执行，调用class_create创建LED设备类，用于管理该类设备。\n实现思路：\n 调用class_create创建led类设备 赋值结构体leds_class-\u0026gt;pm，leds_class_dev_pm_ops配置电源管理接口，用于休眠唤醒, 赋值结构体leds_class-\u0026gt;dev_groups，设置该类设备的文件属性。  3、leds_class_dev_pm_ops分析 #   上面我们在创建led类的时候，赋值了leds_class_dev_pm_ops电源管理接口，那么该接口是怎么定义的呢？\n 3.1 相关数据结构 #  3.1.1 dev_pm_ops #  struct dev_pm_ops { int (*prepare)(struct device *dev); void (*complete)(struct device *dev); int (*suspend)(struct device *dev); int (*resume)(struct device *dev); int (*freeze)(struct device *dev); int (*thaw)(struct device *dev); int (*poweroff)(struct device *dev); int (*restore)(struct device *dev); int (*suspend_late)(struct device *dev); int (*resume_early)(struct device *dev); int (*freeze_late)(struct device *dev); int (*thaw_early)(struct device *dev); int (*poweroff_late)(struct device *dev); int (*restore_early)(struct device *dev); int (*suspend_noirq)(struct device *dev); int (*resume_noirq)(struct device *dev); int (*freeze_noirq)(struct device *dev); int (*thaw_noirq)(struct device *dev); int (*poweroff_noirq)(struct device *dev); int (*restore_noirq)(struct device *dev); int (*runtime_suspend)(struct device *dev); int (*runtime_resume)(struct device *dev); int (*runtime_idle)(struct device *dev); }; 结构体名称：dev_pm_ops\n文件位置：include/linux/pm.h\n主要作用：主要定义了设备电源管理的回调函数接口，我们一般使用suspend和resume两个，用于休眠，唤醒。\n3.2 相关实现 #  /** * led_classdev_suspend - suspend an led_classdev. * @led_cdev: the led_classdev to suspend. */ void led_classdev_suspend(struct led_classdev *led_cdev) { led_cdev-\u0026gt;flags |= LED_SUSPENDED; led_set_brightness_nopm(led_cdev, 0);\t//\t灯灭 } EXPORT_SYMBOL_GPL(led_classdev_suspend); /** * led_classdev_resume - resume an led_classdev. * @led_cdev: the led_classdev to resume. */ void led_classdev_resume(struct led_classdev *led_cdev) { led_set_brightness_nopm(led_cdev, led_cdev-\u0026gt;brightness);\t//\t灯亮  if (led_cdev-\u0026gt;flash_resume) led_cdev-\u0026gt;flash_resume(led_cdev); led_cdev-\u0026gt;flags \u0026amp;= ~LED_SUSPENDED; } EXPORT_SYMBOL_GPL(led_classdev_resume); #ifdef CONFIG_PM_SLEEP static int led_suspend(struct device *dev) { struct led_classdev *led_cdev = dev_get_drvdata(dev); if (led_cdev-\u0026gt;flags \u0026amp; LED_CORE_SUSPENDRESUME) led_classdev_suspend(led_cdev); return 0; } static int led_resume(struct device *dev) { struct led_classdev *led_cdev = dev_get_drvdata(dev); if (led_cdev-\u0026gt;flags \u0026amp; LED_CORE_SUSPENDRESUME) led_classdev_resume(led_cdev); return 0; } #endif  static SIMPLE_DEV_PM_OPS(leds_class_dev_pm_ops, led_suspend, led_resume); /* * Use this if you want to use the same suspend and resume callbacks for suspend * to RAM and hibernation. */ #define SIMPLE_DEV_PM_OPS(name, suspend_fn, resume_fn) \\ const struct dev_pm_ops name = { \\ SET_SYSTEM_SLEEP_PM_OPS(suspend_fn, resume_fn) \\ } 函数介绍：代码的这一部分，主要用于实现休眠唤醒功能，然后将相关函数赋值给dev_pm_ops结构体中的回调函数\n实现思路：\n 定义了几个函数led_suspend、led_resume、led_classdev_resume、led_classdev_suspend作为休眠唤醒的实现 通过宏定义SIMPLE_DEV_PM_OPS将函数绑定到leds_class-\u0026gt;pm中，作为该类设备的休眠唤醒管理。   这里SIMPLE_DEV_PM_OPS宏定义较为简单，自行翻阅源码查看即可！\n 4、led_groups分析 #   上面我们在创建led类的时候，将led_groups赋值给了struct attribute_group属性组结构体，那么led_groups是怎么定义的呢？\n 4.1 相关数据结构 #  4.1.1 attribute_group #  /** * struct attribute_group - data structure used to declare an attribute group. * @name:\tOptional: Attribute group name *\tIf specified, the attribute group will be created in *\ta new subdirectory with this name. * @is_visible:\tOptional: Function to return permissions associated with an *\tattribute of the group. Will be called repeatedly for each *\tnon-binary attribute in the group. Only read/write *\tpermissions as well as SYSFS_PREALLOC are accepted. Must *\treturn 0 if an attribute is not visible. The returned value *\twill replace static permissions defined in struct attribute. * @is_bin_visible: *\tOptional: Function to return permissions associated with a *\tbinary attribute of the group. Will be called repeatedly *\tfor each binary attribute in the group. Only read/write *\tpermissions as well as SYSFS_PREALLOC are accepted. Must *\treturn 0 if a binary attribute is not visible. The returned *\tvalue will replace static permissions defined in *\tstruct bin_attribute. * @attrs:\tPointer to NULL terminated list of attributes. * @bin_attrs:\tPointer to NULL terminated list of binary attributes. *\tEither attrs or bin_attrs or both must be provided. */ struct attribute_group { const char\t*name; umode_t\t(*is_visible)(struct kobject *, struct attribute *, int); umode_t\t(*is_bin_visible)(struct kobject *, struct bin_attribute *, int); struct attribute\t**attrs; struct bin_attribute\t**bin_attrs; }; 结构体名称：attribute_group\n文件位置：include/linux/sysfs.h\n主要作用：定义一个属性组，其中包括一组属性和二进制属性，这些属性可以与内核对象相关联，以便用户的访问。\n4.1.2 attribute #  struct attribute { const char\t*name; umode_t\tmode; #ifdef CONFIG_DEBUG_LOCK_ALLOC  bool\tignore_lockdep:1; struct lock_class_key\t*key; struct lock_class_key\tskey; #endif }; 结构体名称：attribute\n文件位置：include/linux/sysfs.h\n主要作用：代表一个属性\n4.2 相关实现 #  static ssize_t brightness_show(struct device *dev, struct device_attribute *attr, char *buf) { struct led_classdev *led_cdev = dev_get_drvdata(dev); /* no lock needed for this */ led_update_brightness(led_cdev); return sprintf(buf, \u0026#34;%u\\n\u0026#34;, led_cdev-\u0026gt;brightness); } static ssize_t brightness_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t size) { struct led_classdev *led_cdev = dev_get_drvdata(dev); unsigned long state; ssize_t ret; mutex_lock(\u0026amp;led_cdev-\u0026gt;led_access); if (led_sysfs_is_disabled(led_cdev)) { ret = -EBUSY; goto unlock; } ret = kstrtoul(buf, 10, \u0026amp;state); if (ret) goto unlock; if (state == LED_OFF) led_trigger_remove(led_cdev); led_set_brightness(led_cdev, state); ret = size; unlock: mutex_unlock(\u0026amp;led_cdev-\u0026gt;led_access); return ret; } static DEVICE_ATTR_RW(brightness); static ssize_t max_brightness_show(struct device *dev, struct device_attribute *attr, char *buf) { struct led_classdev *led_cdev = dev_get_drvdata(dev); return sprintf(buf, \u0026#34;%u\\n\u0026#34;, led_cdev-\u0026gt;max_brightness); } static DEVICE_ATTR_RO(max_brightness); #ifdef CONFIG_LEDS_TRIGGERS static DEVICE_ATTR(trigger, 0644, led_trigger_show, led_trigger_store); //\t属性文件 static struct attribute *led_trigger_attrs[] = { \u0026amp;dev_attr_trigger.attr, NULL, }; static const struct attribute_group led_trigger_group = { .attrs = led_trigger_attrs, }; #endif  //\t属性文件 static struct attribute *led_class_attrs[] = { \u0026amp;dev_attr_brightness.attr, \u0026amp;dev_attr_max_brightness.attr, NULL, }; static const struct attribute_group led_group = { .attrs = led_class_attrs, }; static const struct attribute_group *led_groups[] = { \u0026amp;led_group, #ifdef CONFIG_LEDS_TRIGGERS  \u0026amp;led_trigger_group, #endif  NULL, }; 代码介绍：该部分代码主要用于创建LED属性组，并且负责实现用户空间操作的接口。\n实现思路：\n 定义一个led_groups属性组的二维数组，管理该类设备的所有属性 这个led_groups二维数组，其中又包括两个属性组：led_group、和led_trigger_group，一个用于LED亮度控制，一个用于触发控制。 led_group属性组中又包括多个属性，如：dev_attr_brightness.attr、dev_attr_max_brightness.attr，分别表示LED亮度和最大亮度的设置。 led_trigger_group属性组包括一个属性，如：dev_attr_trigger.attr，用于控制触发属性 定义完属性后，需要提供操作属性的接口，就是上面的led_trigger_show、led_trigger_store、max_brightness_show、brightness_show、brightness_store，其中xxx_show表示读属性，xxx_store表示写属性 至此，所有的属性定义完毕，并且将其读写属性的接口与该属性进行了绑定 最后，通过在leds_init接口中，调用leds_class-\u0026gt;dev_groups = led_groups;，将属性组注册到LED类中进行管理。   阅读代码时，从下网上看，更容易理解！\n 4.3 扩展 #   可能代码中有些地方对于初学者不太容易理解，如：为什么没有找到brightness_store和brightness_show与属性绑定的地方？为什么函数要定义成这个名字？\n 解答这个问题，也是涉及到了Linux内核的设计模式，其中充斥着大量的宏定义，主要用作字符串的拼接，最终生成想要的定义！\n4.3.1 DEVICE_ATTR_RW分析 #  static ssize_t brightness_show(struct device *dev, struct device_attribute *attr, char *buf) { struct led_classdev *led_cdev = dev_get_drvdata(dev); /* no lock needed for this */ led_update_brightness(led_cdev); return sprintf(buf, \u0026#34;%u\\n\u0026#34;, led_cdev-\u0026gt;brightness); } static ssize_t brightness_store(struct device *dev, struct device_attribute *attr, const char *buf, size_t size) { struct led_classdev *led_cdev = dev_get_drvdata(dev); unsigned long state; ssize_t ret; mutex_lock(\u0026amp;led_cdev-\u0026gt;led_access); if (led_sysfs_is_disabled(led_cdev)) { ret = -EBUSY; goto unlock; } ret = kstrtoul(buf, 10, \u0026amp;state); if (ret) goto unlock; if (state == LED_OFF) led_trigger_remove(led_cdev); led_set_brightness(led_cdev, state); ret = size; unlock: mutex_unlock(\u0026amp;led_cdev-\u0026gt;led_access); return ret; } static DEVICE_ATTR_RW(brightness); 代码介绍：上面定义了两个函数（brightness_show，brightness_store）和一个属性名称（brightness），并且通过DEVICE_ATTR_RW宏定义将属性和函数关联起来。\n实现思路：\n 我们分析一下DEVICE_ATTR_RW的宏定义\n //\tstatic DEVICE_ATTR_RW(brightness);  #define DEVICE_ATTR_RW(_name) \\ struct device_attribute dev_attr_##_name = __ATTR_RW(_name)  //\tstatic struct device_attribute dev_attr_brightness = __ATTR_RW(brightness)  #define __ATTR_RW(_name) __ATTR(_name, 0644, _name##_show, _name##_store)  //\tstatic struct device_attribute dev_attr_brightness = __ATTR(brightness, 0644, brightness_show, brightness_store)  #define __ATTR(_name, _mode, _show, _store) {\t\\ .attr = {.name = __stringify(_name),\t\\ .mode = VERIFY_OCTAL_PERMISSIONS(_mode) },\t\\ .show\t= _show,\t\\ .store\t= _store,\t\\ }  //\tstatic struct device_attribute dev_attr_brightness = { //\t.attr = { //\t.name = __stringify(brightness), //\t.mode = VERIFY_OCTAL_PERMISSIONS(0644), //\t} //\t.show = brightness_show, //\t.store = brightness_store, //} 上面屏蔽的内容就是static DEVICE_ATTR_RW(brightness)展开的原貌，这样就与上面的两个函数（brightness_show，brightness_store）关联了起来！\n5、led class的注册注销分析 #   在led-class.c还剩下一部分代码，那就是负责提供注册，注销led设备的相关接口\n 5.1 相关实现 #  5.1.1 devm_of_led_classdev_register #  /** * devm_of_led_classdev_register - resource managed led_classdev_register() * * @parent: parent of LED device * @led_cdev: the led_classdev structure for this device. */ int devm_of_led_classdev_register(struct device *parent, struct device_node *np, struct led_classdev *led_cdev) { struct led_classdev **dr; int rc; dr = devres_alloc(devm_led_classdev_release, sizeof(*dr), GFP_KERNEL); if (!dr) return -ENOMEM; rc = of_led_classdev_register(parent, np, led_cdev);\t//\t注册到子系统  if (rc) { devres_free(dr); return rc; } *dr = led_cdev; devres_add(parent, dr); return 0; } EXPORT_SYMBOL_GPL(devm_of_led_classdev_register); 函数介绍：devm_of_led_classdev_register是of_led_classdev_register函数的资源管理版本。即：在of_led_classdev_register之上，进行了资源的管理。\n实现思路：\n 先通过struct led_classdev **dr创建一个新对象，并将其与给定的设备节点关联 该函数分配了一个devres结构来管理led_classdev对象的生命周期。 如果注册成功，则led_classdev对象将存储在devres结构中，并与父设备关联。  5.1.2 of_led_classdev_register #  /** * of_led_classdev_register - register a new object of led_classdev class. * * @parent: parent of LED device * @led_cdev: the led_classdev structure for this device. * @np: DT node describing this LED */ int of_led_classdev_register(struct device *parent, struct device_node *np, struct led_classdev *led_cdev) { char name[LED_MAX_NAME_SIZE]; int ret; ret = led_classdev_next_name(led_cdev-\u0026gt;name, name, sizeof(name));\t//\t生成唯一的节点名称  if (ret \u0026lt; 0) return ret; mutex_init(\u0026amp;led_cdev-\u0026gt;led_access); mutex_lock(\u0026amp;led_cdev-\u0026gt;led_access); led_cdev-\u0026gt;dev = device_create_with_groups(leds_class, parent, 0, led_cdev, led_cdev-\u0026gt;groups, \u0026#34;%s\u0026#34;, name);\t//\t关联属性文件  if (IS_ERR(led_cdev-\u0026gt;dev)) { mutex_unlock(\u0026amp;led_cdev-\u0026gt;led_access); return PTR_ERR(led_cdev-\u0026gt;dev); } led_cdev-\u0026gt;dev-\u0026gt;of_node = np; if (ret) dev_warn(parent, \u0026#34;Led %s renamed to %s due to name collision\u0026#34;, led_cdev-\u0026gt;name, dev_name(led_cdev-\u0026gt;dev)); if (led_cdev-\u0026gt;flags \u0026amp; LED_BRIGHT_HW_CHANGED) { ret = led_add_brightness_hw_changed(led_cdev); if (ret) { device_unregister(led_cdev-\u0026gt;dev); mutex_unlock(\u0026amp;led_cdev-\u0026gt;led_access); return ret; } } led_cdev-\u0026gt;work_flags = 0; #ifdef CONFIG_LEDS_TRIGGERS  init_rwsem(\u0026amp;led_cdev-\u0026gt;trigger_lock); #endif #ifdef CONFIG_LEDS_BRIGHTNESS_HW_CHANGED  led_cdev-\u0026gt;brightness_hw_changed = -1; #endif  /* add to the list of leds */ down_write(\u0026amp;leds_list_lock); list_add_tail(\u0026amp;led_cdev-\u0026gt;node, \u0026amp;leds_list); up_write(\u0026amp;leds_list_lock); if (!led_cdev-\u0026gt;max_brightness) led_cdev-\u0026gt;max_brightness = LED_FULL; led_update_brightness(led_cdev); led_init_core(led_cdev);\t//\t核心层初始化  #ifdef CONFIG_LEDS_TRIGGERS  led_trigger_set_default(led_cdev); #endif  mutex_unlock(\u0026amp;led_cdev-\u0026gt;led_access); dev_dbg(parent, \u0026#34;Registered led device: %s\\n\u0026#34;, led_cdev-\u0026gt;name); return 0; } 函数介绍：of_led_classdev_register注册一个新的led_classdev对象。\n led_classdev该结构体在上一篇文章中有介绍到，我们在led-gpio.c中创建并初始化，随后我们会调用注册函数devm_of_led_classdev_register来将创建的led_classdev对象注册到LED子系统中，也就是该函数的作用。\n 实现思路：\n 通过led_classdev_next_name来对LED名字添加序号，生成唯一名称 使用device_create_with_groups接口，将led_classdev对象与leds_class关联，创建一个新的设备 最后调用led_init_core接口，初始化了 LED 核心并为设备设置了默认触发器。   led_init_core就是在led-core.c中实现的啦，我们下一篇文章分析。\n 5.1.3 led_classdev_next_name #  static int led_classdev_next_name(const char *init_name, char *name, size_t len) { unsigned int i = 0; int ret = 0; struct device *dev; strlcpy(name, init_name, len); while ((ret \u0026lt; len) \u0026amp;\u0026amp; (dev = class_find_device(leds_class, NULL, name, match_name))) { put_device(dev); ret = snprintf(name, len, \u0026#34;%s_%u\u0026#34;, init_name, ++i); } if (ret \u0026gt;= len) return -ENOMEM; return i; } 函数介绍：led_classdev_next_name，该函数根据提供的初始名称，生成一个唯一的 LED 设备名称。它通过在初始名称后添加下划线和数字来实现。\n实现思路：\n 调用strlcpy接口，将初始名称拷贝到name 缓冲区中 调用class_find_device去循环检查leds_class 类中是否已经存在一个具有当前 name 的设备。 如果存在，则将调用snprintf在 init_name 后面添加下划线和数字  6、总结 #  led-class.c该文件的详细分析如上文，我们回顾一下其主要作用：\n 创建leds_class，并且初始化相关字段，如：pm电源管理，dev_groups设备属性 定义suspend、resume函数，用于LED类设备的休眠唤醒；定义多个属性组，多个属性，并且实现对应的函数，如：brightness_show、brightness_store等，并将其注册到LED类中，以便LED属性在用户空间的读写 提供注册注销函数：devm_of_led_classdev_register、devm_led_classdev_unregister，为了底层将创建的led_classdev与leds_class关联，注册进入子系统   欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":36,"href":"/docs/linux/linux_kernel_lock/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3linux%E9%94%81%E6%9C%BA%E5%88%B6%E5%9B%9B%E8%87%AA%E6%97%8B%E9%94%81/","title":"【深入理解Linux锁机制】四、自旋锁","section":"Linux 内核锁详解","content":"【深入理解Linux内核锁】四、自旋锁 #   上两节主要讲解了中断屏蔽和原子操作，这两个作为最底层的操作，几乎在Linux内核中都不单独使用，下面我们来带大家了解一下常用的自旋锁！\n 1、什么是自旋锁？ #  自旋锁是一种典型的对临界资源进行互斥访问的手段。\n它的底层实现逻辑是：原子变量+判断检测。\n原子变量我们可以理解为一把锁，通过操作原子变量（锁）的状态，并对其进行判断，如果锁未被锁定，我们就继续往下执行；如果锁已经被锁定，我们就原地自旋，直到等到锁被打开。\n在ARM平台下，自旋锁的实现使用了ldrex、strex、以及内存屏障指令dmb、dsb、wfe、sev等。\n2、自旋锁思想 #   自旋锁主要针对于SMP或者单CPU但内核可抢占的情况，对于单CPU内核不可抢占的情况时，自旋锁退化为空操作。 自旋锁实际为忙等锁，当锁不可用时，CPU一直处于等待状态，直到该锁被释放。 自旋锁可能会导致内核死锁，当递归使用自旋锁时，则将该CPU锁死。 在多核SMP的情况下，任何一个核拿到了自旋锁，该核上的抢占调度也暂时禁止了，但是没有禁止另外一个核的抢占调度。 在自旋锁锁定期间，不能调用引起进程调度的函数，如copy_from_user()、copy_to_user()、kmalloc()和msleep()，否则会导致内核崩溃  3、自旋锁的定义及实现 #  3.1 API接口 #  //\t定义自旋锁 spinlock_t lock; //\t初始化自旋锁 spin_lock_init(\u0026amp;lock) //\t获得自旋锁 spin_lock(\u0026amp;lock)\t//\t获取自旋锁，如果立即获得锁，则直接返回，否则，自旋等待，直到锁被释放 spin_trylock(\u0026amp;lock)\t//\t尝试获取自旋锁，如果立即获得锁，返回true，否则直接返回false，不原地等待  //\t释放自旋锁 spin_unlock(\u0026amp;lock) 自旋锁保证了不受其他CPU或者单CPU内的抢占进程的干扰，但是对于临界区代码，仍然有可能会受到中断和底半部的影响。\n为了解决这种问题，我们就要使用自旋锁的衍生。\nspin_lock_irq() = spin_lock() + local_irq_disable()\t//\t获取自旋锁并关中断 spin_unlock_irq() = spin_unlock() + local_irq_enable()\t//\t释放自旋锁并开中断 spin_lock_irqsave() = spin_lock() + local_irq_save()\t//\t获取自旋锁并关中断，保存中断状态 spin_unlock_irqrestore() = spin_unlock() + local_irq_restore()//释放自旋锁，开中断并恢复中断状态 spin_lock_bh() = spin_lock() + local_bh_disable()\t//\t获取自旋锁并关底半部中断 spin_unlock_bh() = spin_unlock() + local_bh_enable()\t//\t释放自旋锁并发开底半部中断 当我们的临界区代码，有可能被进程或者中断访问时，就需要在进程上下文中，调用spin_lock_irqsave()、spin_unlock_irqrestore()，在中断上下文中调用spin_lock()、spin_unlock()，如下图：\n TODO：替换图片\n 3.2 API实现 #  3.2.1 结构体spinlock_t、raw_spinlock、arch_spinlock_t #  typedef struct spinlock { union { struct raw_spinlock rlock; #ifdef CONFIG_DEBUG_LOCK_ALLOC # define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))  struct { u6 __padding[LOCK_PADSIZE]; struct lockdep_map dep_map; }; #endif  }; } spinlock_t; typedef struct raw_spinlock { arch_spinlock_t raw_lock; #ifdef CONFIG_DEBUG_SPINLOCK  unsigned int magic, owner_cpu; void *owner; #endif #ifdef CONFIG_DEBUG_LOCK_ALLOC  struct lockdep_map dep_map; #endif } raw_spinlock_t; typedef struct { union { u32 slock; struct __raw_tickets { #ifdef __ARMEB__  u16 next; u16 owner; #else  u16 owner; u16 next; #endif  } tickets; }; } arch_spinlock_t; 结构体名称：spinlock_t、raw_spinlock、arch_spinlock_t\n文件位置：include/linux/spinlock.h、arch/arm/include/asm/spinlock_types.h\n主要作用：结构体层层嵌套，用于定义一个自旋锁。\n  slock：32位无符号整形数据，用于锁的控制\n  __raw_tickets：union类型，用于基于票证锁算法的自旋锁。\n owner ：表示当前持有自旋锁的线程的索引 next ：表示下一个等待获取自旋锁的线程的索引   每个线程进入代码段时，会尝试获取自旋锁，如果获取失败，它们会在锁的等待队列中排队。然后，等待队列中的线程会按照优先级顺序依次抢占锁的拥有权，直到某个线程成功获取自旋锁并执行完关键代码，释放锁资源为止。\n    这里使用的union联合体，其共享内存空间，其具体区别可看下面：\nstruct与union区别：https://blog.csdn.net/lishuo0204/article/details/118957959\n 3.2.2 spin_lock_init #  #define spin_lock_init(_lock)\t\\ do {\t\\ spinlock_check(_lock);\t\\ raw_spin_lock_init(\u0026amp;(_lock)-\u0026gt;rlock);\t\\ } while (0)  static __always_inline raw_spinlock_t *spinlock_check(spinlock_t *lock) { return \u0026amp;lock-\u0026gt;rlock; }} # define raw_spin_lock_init(lock)\t\\ do { *(lock) = __RAW_SPIN_LOCK_UNLOCKED(lock); } while (0)  #define __RAW_SPIN_LOCK_UNLOCKED(lockname)\t\\ (raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname)  #define __RAW_SPIN_LOCK_INITIALIZER(lockname)\t\\ {\t\\ .raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,\t\\ SPIN_DEBUG_INIT(lockname)\t\\ SPIN_DEP_MAP_INIT(lockname) }  #define __ARCH_SPIN_LOCK_UNLOCKED\t{ { 0 } } 函数名称：spin_lock_init\n文件位置：include/linux/spinlock.h\n主要作用：初始化自旋锁\n函数调用流程：\n// spin_lock_init spin_lock_init(include/linux/spinlock.h) |--\u0026gt; spinlock_check // 对锁进行检查，判断是否存在  |--\u0026gt; raw_spin_lock_init // 初始化锁  |--\u0026gt; __RAW_SPIN_LOCK_UNLOCKED(include/linux/spinlock_types.h) |--\u0026gt; __RAW_SPIN_LOCK_INITIALIZER // 将锁初始为__ARCH_SPIN_LOCK_UNLOCKED未上锁状态 上述函数主要通过宏定义给变量.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED赋值，初始化为0，即为未上锁的状态；并且提供了两个调试接口：CONFIG_DEBUG_SPINLOCK、CONFIG_DEBUG_LOCK_ALLOC，默认为关闭。\n这里面有个关于spinlock_check存在的意义的讨论，感兴趣的可以看一下：https://stackoverflow.com/questions/52551594/spinlock-initialization-function\n3.2.3 spin_lock #  static __always_inline void spin_lock(spinlock_t *lock) { raw_spin_lock(\u0026amp;lock-\u0026gt;rlock); } #define raw_spin_lock(lock)\t_raw_spin_lock(lock)  #ifndef CONFIG_INLINE_SPIN_LOCK void __lockfunc _raw_spin_lock(raw_spinlock_t *lock) { __raw_spin_lock(lock); } EXPORT_SYMBOL(_raw_spin_lock); #ifndef CONFIG_INLINE_SPIN_LOCK void __lockfunc _raw_spin_lock(raw_spinlock_t *lock) { __raw_spin_lock(lock); } EXPORT_SYMBOL(_raw_spin_lock); static inline void __raw_spin_lock(raw_spinlock_t *lock) { preempt_disable(); spin_acquire(\u0026amp;lock-\u0026gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock); } #define preempt_disable() \\ do { \\ preempt_count_inc(); \\ barrier(); \\ } while (0)  #define LOCK_CONTENDED(_lock, try, lock) \\ lock(_lock)  static inline void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock) { __acquire(lock); arch_spin_lock(\u0026amp;lock-\u0026gt;raw_lock); } static inline void arch_spin_lock(arch_spinlock_t *lock) { unsigned long tmp; u32 newval; arch_spinlock_t lockval; prefetchw(\u0026amp;lock-\u0026gt;slock); __asm__ __volatile__( \u0026#34;1:\tldrex\t%0, [%3]\\n\u0026#34; \u0026#34;\tadd\t%1, %0, %4\\n\u0026#34; \u0026#34;\tstrex\t%2, %1, [%3]\\n\u0026#34; \u0026#34;\tteq\t%2, #0\\n\u0026#34; \u0026#34;\tbne\t1b\u0026#34; : \u0026#34;=\u0026amp;r\u0026#34; (lockval), \u0026#34;=\u0026amp;r\u0026#34; (newval), \u0026#34;=\u0026amp;r\u0026#34; (tmp) : \u0026#34;r\u0026#34; (\u0026amp;lock-\u0026gt;slock), \u0026#34;I\u0026#34; (1 \u0026lt;\u0026lt; TICKET_SHIFT) : \u0026#34;cc\u0026#34;); while (lockval.tickets.next != lockval.tickets.owner) { wfe(); lockval.tickets.owner = READ_ONCE(lock-\u0026gt;tickets.owner); } smp_mb(); } 函数名称：spin_lock\n文件位置：include/linux/spinlock.h\n主要作用：用于在进程或线程首次尝试获取锁的时候进行自旋，不停地检查锁的状态，如果锁已经被其他进程或线程占用，则自旋等待，直到锁被释放。\n函数调用流程：\n// spin_lock spin_lock(include/linux/spinlock.h) |--\u0026gt; raw_spin_lock |--\u0026gt; _raw_spin_lock(include/linux/spinlock_api_smp.h) |--\u0026gt; __raw_spin_lock |--\u0026gt; __raw_spin_lock |--\u0026gt; preempt_disable |--\u0026gt; preempt_count_inc |--\u0026gt; barrier |--\u0026gt; spin_acquire |--\u0026gt; LOCK_CONTENDED |--\u0026gt; do_raw_spin_lock |--\u0026gt; arch_spin_lock(arch/arm/include/asm/spinlock.h) 实现流程：\n  preempt_disable(); 禁用内核抢占，确保当前 CPU 执行该代码时不会被其他进程或线程抢占。\n 其通过preempt_count_inc增加抢占计数器的值，通过抢占计数器来实现对任务的执行顺序进行管理。 通过内存屏障barrier来确保前面的操作完成后再继续执行后面的代码。    spin_acquire(\u0026amp;lock-\u0026gt;dep_map, 0, 0, _RET_IP_); 通过调用 spin_acquire() 函数获取自旋锁，用于保护共享资源不被两个、或者多个线程所修改。\n spin_acquire是lockdep工具的一部分，主要用于动态检测死锁。 lock-\u0026gt;dep_map是锁的依赖地图，_RET_IP_是调用者的返回地址。这两个参数都是用于lockdep的调试信息。 lockdep是一个强大的锁调试工具，它可以跟踪锁的所有获取和释放，并动态地检测可能的死锁情况。    LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock); 实际为调用do_raw_spin_lock函数来实现获取锁并自旋的操作。\n  下面为arch_spin_lock汇编代码分析：\n  prefetchw(\u0026amp;lock-\u0026gt;slock) 函数用于提前加载锁的地址到处理器缓存中，从而提高锁的获取效率。\n  ldrex %0, [%3] 用于以原子方式读取锁的值到寄存器 %0 中，%3 为锁的地址。\n  add %1, %0, %4用于将当前获取锁的 CPU 分配的新值加上原锁值 %0 以及%4 固定常量，结果存放在%1新值中。\n  strex %2, %1, [%3] 用于以原子方式将更新的值 %1 写入锁的地址所指定的内存位置，%2 为写入结果。\n 这里解释一下为什么要做add处理：\n由spinlock_t的结构体可知，是由联合体组成，并且可以通过owner和next两个字段访问，next在高16位，owner在低16位。\n这里上锁的操作，是将其值设置为1 \u0026lt;\u0026lt; TICKET_SHIFT，也就是高16位，即next字段设置为1，表示下一个等待获取自旋锁的线程的索引。\n   teq %2, #0：用于测试写入结果 %2 是否为0，如果为0，表示锁获取成功，反之则跳转到标签1b处执行。\n  当获取锁成功之后，程序会执行 while 循环，不断等待锁的所有权被赋予当前 CPU, 直到锁的所有权的拥有者持有锁为止。\n  smp_mb() 函数执行一条内存屏障，确保所有关键数据的顺序性已经刷新到内存中。\n  综上，spin_lock代码的作用是获取自旋锁，让当前线程获得临界资源的控制权，避免多个线程同时修改共享资源而造成数据冲突。同时，通过禁用内核抢占和使用内联函数优化的方式，保证了原子操作的执行效率和可靠性。\n3.2.4 spin_unlock #  static __always_inline void spin_unlock(spinlock_t *lock) { raw_spin_unlock(\u0026amp;lock-\u0026gt;rlock); } #define raw_spin_unlock(lock)\t_raw_spin_unlock(lock)  void __lockfunc _raw_spin_unlock(raw_spinlock_t *lock) { __raw_spin_unlock(lock); } static inline void __raw_spin_unlock(raw_spinlock_t *lock) { spin_release(\u0026amp;lock-\u0026gt;dep_map, 1, _RET_IP_); do_raw_spin_unlock(lock); preempt_enable(); } static inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock) { arch_spin_unlock(\u0026amp;lock-\u0026gt;raw_lock); __release(lock); } static inline void arch_spin_unlock(arch_spinlock_t *lock) { smp_mb(); lock-\u0026gt;tickets.owner++; dsb_sev(); } 函数名称：spin_unlock\n文件位置：include/linux/spinlock.h\n主要作用：该宏释放自旋锁lock，它与spin_trylock或spin_lock配对使用。\n函数调用流程：\n// spin_unlock spin_unlock(include/linux/spinlock.h) |--\u0026gt; raw_spin_unlock |--\u0026gt; _raw_spin_unlock(kernel/locking/spinlock.c) |--\u0026gt; __raw_spin_unlock(include/linux/spinlock_api_smp.h) |--\u0026gt; spin_release |--\u0026gt; do_raw_spin_unlock |--\u0026gt; arch_spin_unlock(arch/arm/include/asm/spinlock.h) |--\u0026gt; preempt_enable 下面为arch_spin_unlock代码分析：\n  smp_mb() 函数执行一条内存屏障，确保所有关键数据的顺序性已经刷新到内存中。\n  lock-\u0026gt;tickets.owner++：递增自旋锁的锁拥有者(Ower)计数器，这个计数器的作用是向其他任务表明自旋锁已经被释放。\n 通过上述两个接口可知：\n spin_lock：将next值加1，表示下一个等待获取自旋锁的线程 spin_unlock：将owner的值加1，指向下一个获取自旋锁的线程。     dsb_sev() 则用于内存同步，强制发射缓存条 (FLUSH操作)，确保数据在多个核之间进行同步，以避免潜在的错误和问题。\n  3.2.5 spin_trylock #  static __always_inline int spin_trylock(spinlock_t *lock) { return raw_spin_trylock(\u0026amp;lock-\u0026gt;rlock); } #define raw_spin_trylock(lock)\t__cond_lock(lock, _raw_spin_trylock(lock))  static inline int __raw_spin_trylock(raw_spinlock_t *lock) { preempt_disable(); if (do_raw_spin_trylock(lock)) { spin_acquire(\u0026amp;lock-\u0026gt;dep_map, 0, 1, _RET_IP_); return 1; } preempt_enable(); return 0; } static inline int do_raw_spin_trylock(raw_spinlock_t *lock) { return arch_spin_trylock(\u0026amp;(lock)-\u0026gt;raw_lock); } static inline int arch_spin_trylock(arch_spinlock_t *lock) { unsigned long contended, res; u32 slock; prefetchw(\u0026amp;lock-\u0026gt;slock); do { __asm__ __volatile__( \u0026#34;\tldrex\t%0, [%3]\\n\u0026#34; \u0026#34;\tmov\t%2, #0\\n\u0026#34; \u0026#34;\tsubs\t%1, %0, %0, ror #16\\n\u0026#34; \u0026#34;\taddeq\t%0, %0, %4\\n\u0026#34; \u0026#34;\tstrexeq\t%2, %0, [%3]\u0026#34; : \u0026#34;=\u0026amp;r\u0026#34; (slock), \u0026#34;=\u0026amp;r\u0026#34; (contended), \u0026#34;=\u0026amp;r\u0026#34; (res) : \u0026#34;r\u0026#34; (\u0026amp;lock-\u0026gt;slock), \u0026#34;I\u0026#34; (1 \u0026lt;\u0026lt; TICKET_SHIFT) : \u0026#34;cc\u0026#34;); } while (res); if (!contended) { smp_mb(); return 1; } else { return 0; } } 函数名称：spin_trylock\n文件位置：include/linux/spinlock.h\n主要作用：该宏尝试获得自旋锁lock，如果能立即获得锁，它获得锁并返回true，否则立即返回false，实际上不再“在原地打转”，也就是去掉了while循环的操作！\n函数调用流程：\n// spin_trylock spin_trylock(include/linux/spinlock.h) |--\u0026gt; raw_spin_trylock |--\u0026gt; __raw_spin_trylock(include\\linux\\spinlock_api_smp.h) |--\u0026gt; preempt_disable // 禁止调度  |--\u0026gt; do_raw_spin_trylock |--\u0026gt; arch_spin_trylock(arch/arm/include/asm/spinlock.h) |--\u0026gt; preempt_enable // 打开调度 下面为arch_spin_trylock汇编代码分析：\n  prefetchw(\u0026amp;lock-\u0026gt;slock) 函数用于提前加载锁的地址到处理器缓存中，从而提高锁的获取效率。\n  ldrex %0, [%3] 用于以原子方式读取锁的值到寄存器 %0 中，%3 为锁的地址。\n  mov %2, #0：将变量res置为0\n  subs %1, %0, %0, ror #16：在这行代码中，%0 表示 slock 寄存器，%1 表示 contended 寄存器。该语句的作用是：将锁变量的值存储在slock中，然后将slock右旋16位，获得低16位的值，并将结果存储到 contended 中。接着将 slock 和 slock 旋转后的值相减，并将结果存储到 contended 中。\n 同上，这里解释一下为什么执行sub操作：\n这里的核心思想是：一个32位的值，其高16位为next，低16位为owner，将其右旋16位后，得到的值高16位变成了owner，低16位变成了next，这时候执行相减操作，如果结果为0，则表示owner与next两个字段相等！\n   addeq %0, %0, %4：将%0寄存器的值加上%4的值，结果存放于%0寄存器中\n  strexeq %2, %0, [%3]：用于以原子方式将更新的值 %0 写入锁的地址所指定的内存位置，%2 为写入结果。\n  然后，代码会检查contended的值。如果contended的值为0，表示进程已经成功获取了锁，函数返回1；否则，表示锁已经被其他进程持有，函数返回0。\n  smp_mb() 函数执行一条内存屏障，确保所有关键数据的顺序性已经刷新到内存中。\n  4、总结 #  自旋锁在我们编写内核代码时会较多涉及，其底层实现主要依赖于前面所提到的：内存屏障、中断屏蔽、原子操作！\n 最后，自旋锁使用的方法如下：\n /* 定义一个自旋锁*/ spinlock_t lock; spin_lock_init(\u0026amp;lock); spin_lock (\u0026amp;lock) ; /* 获取自旋锁，保护临界区 */ . . . /* 临界区*/ spin_unlock (\u0026amp;lock) ; /* 解锁 */ 相信通过上面详细的拆解，到这里我们就能够对自旋锁有一个较深的认识了吧！\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":37,"href":"/docs/linux/linux_nvmem_subsystem/nvmem%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E5%9B%9Befuse%E9%A9%B1%E5%8A%A8%E5%AE%9E%E7%8E%B0%E6%B5%81%E7%A8%8B/","title":"【NVMEM子系统深入剖析】四、efuse驱动实现流程","section":"Linux NVMEM 子系统","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":38,"href":"/docs/uboot/%E5%9B%9Buboot%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%A8%A1%E5%BC%8F%E5%88%86%E6%9E%90/","title":"四、Uboot命令行模式分析","section":"Uboot开发","content":"四、Uboot命令行模式分析 #   前几篇文章，我们也了解了Uboot的启动流程，那么这节就主要讲讲Uboot的命令行模式。\n另外，文章末尾还提供eMMC5.1官方标准协议.pdf和eMMC4.51官方标准协议-中文.pdf下载渠道，方便深入了解底层协议。\n正文如下：\n  4.1 如何进入命令行模式 #  我们正常启动流程，默认是直接跳过Uboot命令行模式的，因为Uboot主要的作用是引导Kernel，一般我们不进行uboot开发时，都默认跳过进入命令行模式。\n 那么，我们要想进入Uboot命令行模式，需要进行哪些配置呢？\n 打开我们准备好一份Uboot源码，进入menuconfig配置菜单，主要设置下列几个配置信息！\n  CONFIG_CMDLINE：命令行模式开关 CONFIG_SYS_PROMPT：命令行模式提示符 CONFIG_HUSH_PARSER：使用hush shell 来对命令进行解析 BOOTDELAY：设置启动延时   Tip：meneconfig中查找苦难？实时/符号，输入1或2或3，直接查找指定标识。\n   打开之后，重新编译，并将Uboot镜像烧录到开发板中，再次启动，我们就能够看到倒计时。\n[2022-03-02:13:33:47]U-Boot 2020.10-rc1-00043-ge62a6d17c6-dirty (Feb 08 2022 - 10:14:14 +0800) [2022-03-02:13:33:47] [2022-03-02:13:33:47]Model: xxxxxx [2022-03-02:13:33:47]MMC: mmc1@xxxxxx: 1 [2022-03-02:13:33:47]In: serial [2022-03-02:13:33:47]Out: serial [2022-03-02:13:33:47]Err: serial [2022-03-02:13:33:47]Model: xxxxxx [2022-03-02:13:33:49]Hit any key to stop autoboot: 2 Hit any key to stop autoboot：我们在倒计时结束前，任意键入一个按键，即可进入！\n 4.2 Uboot基本命令解析 #  进入Uboot命令行模式后，键入help或者?，可以查看所有支持的Uboot命令。\n注意：Uboot支持的命令大都远远超过显示的，还有好多没有打开，可以在menuconfig中，打开相应的功能，如mmc相关的，md内存相关的。\n 常用命令如下：\nversion\t#查看uboot版本 reset #重启Uboot printenv\t#打印uboot环境变量 setenv name value\t#设置环境变量 md addr\t#查看内存指令 nm addr\t#修改内存值 mm addr\t#自增修改内存值 mmc dev id\t#选择mmc卡 mmc rescan\t#扫描卡 echo $name\t#打印环境变量  更多指令使用，可以见文末整理的文档\n  4.3 命令行模式代码执行流程分析 #   结合下面的程序执行流程图，代码，一起分析。\n 上图为Uboot命令行模式的代码具体执行流程，结合 专栏系列（二）uboot启动流程分析，文章内已经详细分析函数内部实现。\n static int abortboot(int bootdelay) { int abort = 0; if (bootdelay \u0026gt;= 0) { if (IS_ENABLED(CONFIG_AUTOBOOT_KEYED)) abort = abortboot_key_sequence(bootdelay); else abort = abortboot_single_key(bootdelay);\t//按键检测 \t} if (IS_ENABLED(CONFIG_SILENT_CONSOLE) \u0026amp;\u0026amp; abort) gd-\u0026gt;flags \u0026amp;= ~GD_FLG_SILENT; return abort; } static int abortboot_single_key(int bootdelay) { int abort = 0; unsigned long ts; printf(\u0026#34;Hit any key to stop autoboot: %2d \u0026#34;, bootdelay);\t//打印倒计时  /* * Check if key already pressed */ if (tstc()) {\t/* we got a key press\t*/\t//获取按键 \t(void) getc(); /* consume input\t*/ puts(\u0026#34;\\b\\b\\b0\u0026#34;); abort = 1;\t/* don\u0026#39;t auto boot\t*/ } while ((bootdelay \u0026gt; 0) \u0026amp;\u0026amp; (!abort)) { --bootdelay; /* delay 1000 ms */ ts = get_timer(0); do { if (tstc()) {\t/* we got a key press\t*/\t//获取按键 \tint key; abort = 1;\t/* don\u0026#39;t auto boot\t*/ bootdelay = 0;\t/* no more delay\t*/ key = getc(); /* consume input\t*/ if (IS_ENABLED(CONFIG_USE_AUTOBOOT_MENUKEY)) menukey = key; break; } udelay(10000); } while (!abort \u0026amp;\u0026amp; get_timer(ts) \u0026lt; 1000);\t//延时1S  printf(\u0026#34;\\b\\b\\b%2d \u0026#34;, bootdelay); } putc(\u0026#39;\\n\u0026#39;); return abort; } abortboot_single_key：该函数主要用于while循环检测按键，如果有按键按下，将abort标志位置1，最后运行cli_loop命令行模式的函数。\n如果按键不按下，标志位abort不起作用，直接运行run_command_list(s, -1, 0);，s = env_get(\u0026quot;bootcmd\u0026quot;);，直接跳转到我们设置的环境变量bootcmd所设定的指令，而不执行cli_loop函数。\n 对照运行流程图看代码，容易理解！！！\n  4.4 如何添加Uboot命令 #  如何自定义一个Uboot命令呢？\n我们暂且先不考虑实现的原理，就仅仅照葫芦画瓢来实现一个简单的Uboot命令！\n 第一步：照葫芦 #  我们打开Uboot的源码文件，进入cmd目录，没错，所有的命令实现都存放在该目录下。\n有没有看到help.C这个文件呢，我们就拿help这个文件来类比。\nU_BOOT_CMD：用来定义一个命令\nhelp：用于命令行键入的指令\ndo_help：键入指令后，执行的函数\n要想进一步使用该命令，我们不得不去了解每个参数的含义。\nstruct cmd_tbl_s { char\t*name;\t/* Command Name\t*/ int\tmaxargs;\t/* maximum number of arguments\t*/ int\trepeatable;\t/* autorepeat allowed?\t*/ /* Implementation function\t*/ int\t(*cmd)(struct cmd_tbl_s *, int, int, char *[]); char\t*usage;\t/* Usage message\t(short)\t*/ char\t*help;\t/* Help message\t(long)\t*/ /* do auto completion on the arguments */ int\t(*complete)(int argc, char *argv[], char last_char, int maxv, char *cmdv[]); }; typedef struct cmd_tbl_s\tcmd_tbl_t; 每个参数分别对应了：命令名、可接收的最大参数、命令可重复、响应函数、使用示例、帮助信息。\n 第二步：画瓢 #  弄明白这个道理，假如我们想加入一个helpme的指令，该怎么做？\n 定义一个指令  U_BOOT_CMD( helpme,\tCONFIG_SYS_MAXARGS,\t1,\tdo_helpme, \u0026#34;helpme dong\u0026#34;, \u0026#34;\\n\u0026#34; \u0026#34;\t- print brief description of all commands\\n\u0026#34; \u0026#34;helpme command ...\\n\u0026#34; \u0026#34;\t- print detailed usage of \u0026#39;command\u0026#39;\u0026#34; );  定义一个执行函数  static int do_helpme(struct cmd_tbl *cmdtp, int flag, int argc, char *const argv[]) { printf(\u0026#34;Cmd test ok!\\r\\n\u0026#34;); printf(\u0026#34;argc = %d\\r\\n\u0026#34;, argc); printf(\u0026#34;argv = \u0026#34;); for(int i = 0; i \u0026lt; argc; ++i) { printf(\u0026#34;%s\\t\u0026#34;, argv[i]); } printf(\u0026#34;\\r\\n\u0026#34;); } 这样，就可以编译-\u0026gt;烧录-\u0026gt;运行了。\n进入Uboot命令行，键入help查看添加的命令helpme。\n 键入命令测试  =\u0026gt; helpme 123456 123 Cmd test ok! argc = 3 argv = helpme 123456 123  第三步：优雅 #  如果我们只是暂时测试，这样添加无伤大雅；如果我们需要投入正规项目使用，这么做有点激进了。\n更加合理的做法是：\n 在uboot/cmd目录下，建立一个文件XXX.c 将要添加的命令写入XXX.c该文件中 修改Makefile文件，编译该文件：obj-y += XXX.o 重新编译，烧录   说白了，就是创建一个文件，将自定义指令添加进去，尽量不修改源码！\n  4.5 Uboot命令底层实现分析 #  上面写了傻瓜式添加命令的方法，对于进行Uboot开发，当然我们需要去了解一下内部的实现原理。\n4.5.1 U_BOOT_CMD #  查看U_BOOT_CMD宏定义\n#define U_BOOT_CMD(_name, _maxargs, _rep, _cmd, _usage, _help)\t\\ U_BOOT_CMD_COMPLETE(_name, _maxargs, _rep, _cmd, _usage, _help, NULL) #define U_BOOT_CMD_COMPLETE(_name, _maxargs, _rep, _cmd, _usage, _help, _comp) \\ ll_entry_declare(struct cmd_tbl, _name, cmd) =\t\\ U_BOOT_CMD_MKENT_COMPLETE(_name, _maxargs, _rep, _cmd,\t\\ _usage, _help, _comp); #define U_BOOT_CMD_MKENT_COMPLETE(_name, _maxargs, _rep, _cmd,\t\\ _usage, _help, _comp)\t\\ { #_name, _maxargs,\t\\ _rep ? cmd_always_repeatable : cmd_never_repeatable,\t\\ _cmd, _usage, _CMD_HELP(_help) _CMD_COMPLETE(_comp) } #define ll_entry_declare(_type, _name, _list)\t\\ _type _u_boot_list_2_##_list##_2_##_name __aligned(4)\t\\ __attribute__((unused,\t\\ section(\u0026#34;.u_boot_list_2_\u0026#34;#_list\u0026#34;_2_\u0026#34;#_name)))   乍一看，都是宏定义，为什么看起来这么吃力？\n 在这里，不得不提到#和##的区别 #   #：转换为字符串  ... #define TO_STR(x) #x int main() { int value = 123; printf(\u0026#34;TO_STR(value) = %s\\n\u0026#34;, TO_STR(value)); printf(\u0026#34;TO_STR(123) = %s\\n\u0026#34;, TO_STR(123)); } //打印 TO_STR(value) = value; TO_STR(123) = 123;  ##：两个字符拼接  #define CONNECT(x,y) x##y #define VAR(y) data##y int main() { int xy = 123; printf(\u0026#34;xy = %d\\n\u0026#34;, CONNECT(x, y)); CONNECT(x, y) = 123456; printf(\u0026#34;xy = %d\\n\u0026#34;, CONNECT(x, y)); int VAR(1) = 100; printf(\u0026#34;VAR(1) = data1 = %d\\n\u0026#34;, data1); } //打印 xy = 123 xy = 123456 VAR(1) = data1 = 100  回到正文\n 上面的宏定义，简单来看，转换流程就是：\nU_BOOT_CMD -\u0026gt; U_BOOT_CMD_COMPLETE -\u0026gt; ll_entry_declare = U_BOOT_CMD_MKENT_COMPLETE -\u0026gt; _type xxx = {aaa, bbb, ccc , ...}\n其本质就是： struct my_struct test = {1, 2, 3};结构体赋值语句。\n 以help命令为例：\nU_BOOT_CMD( help,\tCONFIG_SYS_MAXARGS,\t1,\tdo_help, \u0026#34;print command description/usage\u0026#34;, \u0026#34;\\n\u0026#34; \u0026#34;\t- print brief description of all commands\\n\u0026#34; \u0026#34;help command ...\\n\u0026#34; \u0026#34;\t- print detailed usage of \u0026#39;command\u0026#39;\u0026#34; ); 直接展开来看：\nstruct cmd_tbl _u_boot_list_2_cmd_2_help __aligned(4) __attribute__((unused, section(\u0026#34;.u_boot_list_2_cmd_2_help\u0026#34;))) = {\u0026#34;help\u0026#34;, CONFIG_SYS_MAXARGS, cmd_always_repeatable, do_help, \u0026#34;xxx\u0026#34;, \u0026#34;xxx\u0026#34;}; 也就相当于，我们定义一个命令，给其赋值。\n定义的命令存放在哪里呢？ #  根据上面展开来看，section(\u0026quot;.u_boot_list_2_cmd_2_help\u0026quot;)，存放在段.u_boot_list_2_cmd_2_help中，打开u-boot.map文件，我们可以查找得到。\n有没有觉得很熟悉，没错，跟前面讲过的驱动模型很像。\n我们定义的命令，被u_boot_list_2_cmd_1和u_boot_list_2_cmd_3两个段所包括，用于遍历，最终查找得到我们想要的命令。\n4.6 Uboot命令响应流程 #  命令响应流程见图：\n 根据4.3 命令行模式代码执行流程分析，我们可以知道，命令行模式最终执行cli_loop函数，实现与用户的交互。\nvoid cli_loop(void) { bootstage_mark(BOOTSTAGE_ID_ENTER_CLI_LOOP); #ifdef CONFIG_HUSH_PARSER \tparse_file_outer(); /* This point is never reached */ for (;;); #elif defined(CONFIG_CMDLINE) \tcli_simple_loop(); #else \tprintf(\u0026#34;## U-Boot command line is disabled. Please enable CONFIG_CMDLINE\\n\u0026#34;); #endif /*CONFIG_HUSH_PARSER*/} 通过分析代码，Uboot的命令行有两种模式：一种是HUSH解析，另一种是通用解析。\n HUSH解析：调用parse_file_outer并不断循环 通用解析：调用cli_simple_loop并不断循环。   无论哪种命令行解析，说白了就是输入输出的处理，必定会读取数据，执行相应命令，打印出对应数据\n HUSH模式\n 输入数据处理：parse_stream 输出数据处理：run_list  通用模式：\n 输入数据处理：cli_readline 输出数据处理：run_command_repeatable   具体实现流程，参照上面的流程图！\n命令行模式的深入解析，准备在下节详细介绍！\n 目前，我们已经对命令行的整体运行流程进行梳理，熟悉整体的运行逻辑，并且能够添加自定义命令喽。\n 4.6 推荐文档 #  [1]：https://www.pianshen.com/article/21471247431/\n[2]：https://blog.csdn.net/weixin_44895651/article/details/108211268\n[3]：https://blog.51cto.com/u_2847568/4917530?b=totalstatistic\n[4]：https://blog.csdn.net/SilverFOX111/article/details/86892231\n[5]：https://blog.csdn.net/andy_wsj/article/details/8614905\n 另外，如果有同学想了解Emmc协议的，可以【戳这里】下载eMMC5.1官方标准协议.pdf和eMMC4.51官方标准协议-中文.pdf\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":39,"href":"/docs/linux/linux_memory_manage/%E5%9B%9B%E7%89%A9%E7%90%86%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%9E%8B/","title":"四、物理地址空间设计模型","section":"Linux 内存管理","content":"Linux内存管理 | 四、物理地址空间设计模型 #  前面几篇文章，主要讲解了虚拟内存空间的布局和管理，下面同步来聊聊物理内存空间的布局和管理。\n 1、物理内存 #   什么是物理内存？\n 我们平时聊的内存，也叫随机访问存储器（random-access memory），也叫RAM。\nRAM分为两类：\n SRAM：静态RAM，其主要用于CPU高速缓存 L1Cache，L2Cache，L3Cache，其特点是访问速度快，访问速度为 1 - 30 个时钟周期，但是容量小，造价高。   DRAM：动态RAM，其主要用于我们常说的主存上，其特点的是访问速度慢（相对高速缓存），访问速度为 50 - 200 个时钟周期，但是容量大，造价便宜些（相对高速缓存）。  DRAM经过组合起来，就作为我们的计算机内存，也是物理内存。\n 2、物理内存访问模型 #  上面介绍了物理内存的基本组成，那么CPU是如何访问物理内存的呢？\n对于CPU访问物理内存，Linux提供了两种架构：UMA(Uniform Memory Access)一致内存访问，NUMA(Non-Uniform Memory Access)非一致内存访问。\n2.1 UMA #  在UMA架构下，多核处理器中的多个CPU，位于总线的一侧，所有的内存条组成的物理内存位于总线的另一侧。\n所有的CPU访问内存都要经过总线，并且距离都是一样的，所以在UMA架构下，所有CPU具有相同的访问特性，即对内存的访问具有相同的速度。\n2.2 NUMA #  这种架构，系统中的各个处理器都有本地内存，处理器与处理器之间也通过总线连接，以便于其他处理器对本地内存的访问。\n与UMA不同的是，处理器访问本地内存的速度要快于对其他处理器本地内存的访问。\n3、物理内存组织模型 #  内存页是物理内存管理中最小单位，有时也成为页帧（Page Frame）。\n内核对物理内存划分为一页一页的连续的内存块，每页大小4KB，并且使用struct page结构体来表示页结构，其中封装了每个页的状态信息，包括：组织结构，使用信息，统计信息等。\n page结构体较为复杂，我们后续再深入了解。\n   更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  3.1 FLATMEM平坦内存模型 #   FLATMEM即：flat memory model。\n 我们把物理内存想象成它是由连续的一页一页的块组成的，我们从0开始对物理页编号，这样每个物理页都会有页号。\n由于物理地址是连续的，页也是连续的，每个页大小也是一样的。因而对于任何一个地址，只要直接除一下每页的大小，很容易直接算出在哪一页。\n如果是这样，整个物理内存的布局就非常简单、易管理，这就是最经典的平坦内存模型（Flat Memory Model）。\n如上图，平坦内存模型中，内核使用一个mem_map的全局数组，来组织所有划分出来的物理内存页，下标由PFN表示。\n在平坦内存模型下 ，page_to_pfn 与 pfn_to_page 的计算逻辑就非常简单，本质就是基于 mem_map 数组进行偏移操作。\n#ifndef ARCH_PFN_OFFSET #define ARCH_PFN_OFFSET\t(0UL) #endif  #if defined(CONFIG_FLATMEM) #define __pfn_to_page(pfn) (mem_map + ((pfn)-ARCH_PFN_OFFSET)) #define __page_to_pfn(page) ((unsigned long)((page)-mem_map) + ARCH_PFN_OFFSET) #endif  ARCH_PFN_OFFSET 是 PFN 的起始偏移量。\n  3.2 DISCONTIGMEM 不连续内存模型 #   DISCONTIGMEM即：discontiguous memory model。\n 我们早期内核使用的是FLATMEM模型，该模型对于较小的，连续的物理空间是方便使用的，但是当物理内存不连续时，使用mem_map管理，就会出现空洞，这会浪费mem_map数组本身占用的内存空间。\n对于NUMA访问内存模型，物理内存分布就是不连续的，为了有效管理，DISCONTIGMEM 不连续内存模型出现了。\n在不连续的物理内存中，DISCONTIGMEM不连续内存模型，将物理内存分成了一个个的node，然后每个node管理一块连续的物理内存，连续的物理内存仍然使用FLATMEM平坦内存模型来管理，从而避免了内存空洞的浪费。\n 我们可以看出 DISCONTIGMEM 非连续内存模型其实就是 FLATMEM 平坦内存模型的一种扩展。\n DISCONTIGMEM是个稍纵即逝的内存模型，在SPARSEMEM出现后即被完全替代。\n 3.3 SPARSEMEM稀疏内存模型 #  随着内存技术的发展，内核可以支持物理内存的热插拔了（像我们的内存条，可以直接插入拔出），这样不连续物理内存已然称为常态。\nSPARSEMEM稀疏内存模型的核心思想就是对粒度更小的连续内存块进行精细的管理，用于管理连续内存块的单元被称作 section 。\n 物理页大小为 4k 的情况下， section 的大小为 128M ，物理页大小为 16k 的情况下， section 的大小为 512M。\n  在内核中，使用struct mem_section结构体表示SPARSEMEM模型中的section\nstruct mem_section { unsigned long section_mem_map; ... }   每个mem_section管理一片小的，物理内存连续的区域，并且支持对该区域的offline/online状态\n  所有的mem_section都保存在一个全局数组中\n   整体的框架如下：\n 在 SPARSEMEM 稀疏内存模型下 page_to_pfn 与 pfn_to_page 的计算逻辑又发生了变化。\n#if defined(CONFIG_SPARSEMEM) /* * Note: section\u0026#39;s mem_map is encoded to reflect its start_pfn. * section[i].section_mem_map == mem_map\u0026#39;s address - start_pfn; */ #define __page_to_pfn(pg)\t\\ ({\tconst struct page *__pg = (pg);\t\\ int __sec = page_to_section(__pg);\t\\ (unsigned long)(__pg - __section_mem_map_addr(__nr_to_section(__sec)));\t\\ })  #define __pfn_to_page(pfn)\t\\ ({\tunsigned long __pfn = (pfn);\t\\ struct mem_section *__sec = __pfn_to_section(__pfn);\t\\ __section_mem_map_addr(__sec) + __pfn;\t\\ }) #endif  在 page_to_pfn 的转换中，首先需要通过 page_to_section 根据 struct page 结构定位到 mem_section 数组中具体的 section 结构。然后在通过 section_mem_map 定位到具体的 PFN。 在 pfn_to_page 的转换中，首先需要通过 __pfn_to_section 根据 PFN 定位到 mem_section 数组中具体的 section 结构。然后在通过 PFN 在 section_mem_map 数组中定位到具体的物理页 Page 。   4、总结 #  以上，我们先对物理内存空间有一个基础的了解，明白物理内存空间的内存访问模型和组织模型，下面我们再详细介绍物理内存空间的布局和管理。\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":40,"href":"/docs/linux/linux_mmc_subsystem/mmc%E5%AD%90%E7%B3%BB%E7%BB%9F%E4%BA%94mmc%E6%A0%B8%E5%BF%83%E5%B1%82/","title":"【MMC子系统】五、MMC核心层","section":"Linux MMC 子系统","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":41,"href":"/docs/linux/bluetooth/bluetooth%E8%93%9D%E7%89%99%E5%BC%80%E5%8F%91%E4%BA%94ble%E5%8D%8F%E8%AE%AE%E4%B9%8B%E9%93%BE%E8%B7%AF%E5%B1%82/","title":"【Bluetooth蓝牙开发】五、BLE协议之链路层","section":"Bluetooth蓝牙开发","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":42,"href":"/docs/linux/linux_led_subsystem/led%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90%E4%BA%94%E6%A0%B8%E5%BF%83%E5%B1%82%E8%AF%A6%E8%A7%A3%E4%BA%8C/","title":"【LED子系统深度剖析】五、核心层详解（二）","section":"Linux LED子系统","content":"【LED子系统深度剖析】五、核心层详解（二） #  1、前言 #   上篇文章我们了解了子系统的核心层led-class.c，下面我们来分析驱动框架中核心层的led-core.c实现以及作用。\n 我们接着从led-core.c文件开始分析\n2、led_init_core分析 #   上一篇文章，我们知道在将leds_classdev注册进入子系统后，会调用led_init_core函数，初始化核心层，下面我们以led_init_core该函数为突破口分析。\n 2.1 相关数据结构 #  2.1.1 work_struct #  struct work_struct { atomic_long_t data; struct list_head entry; work_func_t func; #ifdef CONFIG_LOCKDEP  struct lockdep_map lockdep_map; #endif }; 结构体名称：work_struct\n文件位置：include/linux/workqueue.h.h\n主要作用：定义一个工作队列，包括了工作项的状态和数据，以及处理工作项的函数指针，用于实现异步执行任务的功能。在工作队列中，每个工作项都是一个work_struct结构体的实例，通过将工作项添加到工作队列中，可以实现后台执行任务的功能。\n2.1.2 timer_list #  struct timer_list { /* * All fields that change during normal runtime grouped to the * same cacheline */ struct hlist_node\tentry; unsigned long\texpires; void\t(*function)(struct timer_list *); u32\tflags; #ifdef CONFIG_LOCKDEP  struct lockdep_map\tlockdep_map; #endif }; 结构体名称：work_struct\n文件位置：include/linux/timer.h\n主要作用：表示一个定时器，其中包括定时器到期时间，处理函数，以及一些标志位信息。\n entry：一个hlist_node结构体，用于将定时器添加到哈希表中。 expires：一个unsigned long类型的值，表示定时器何时到期。 function：表示定时器的处理函数 flags：一个u32类型的值，表示定时器的标志位 lockdep_map：如果定义了CONFIG_LOCKDEP，则包含一个lockdep_map结构体，用于跟踪定时器的锁定情况。  2.2 相关实现 #  void led_init_core(struct led_classdev *led_cdev) { INIT_WORK(\u0026amp;led_cdev-\u0026gt;set_brightness_work, set_brightness_delayed); timer_setup(\u0026amp;led_cdev-\u0026gt;blink_timer, led_timer_function, 0); } 函数介绍：这部分代码，用于初始化核心层，此函数通过设置延迟工作队列来设置 LED 亮度，并通过计时器来进行软件闪烁。\n2.2.1 INIT_WORK #  INIT_WORK(\u0026amp;led_cdev-\u0026gt;set_brightness_work, set_brightness_delayed); #define INIT_WORK(_work, _func)\t\\ __INIT_WORK((_work), (_func), 0)  // #define __INIT_WORK(\u0026amp;led_cdev-\u0026gt;set_brightness_work, set_brightness_delayed, 0)  #define __INIT_WORK(_work, _func, _onstack)\t\\ do {\t\\ __init_work((_work), _onstack);\t\\ (_work)-\u0026gt;data = (atomic_long_t) WORK_DATA_INIT();\t\\ INIT_LIST_HEAD(\u0026amp;(_work)-\u0026gt;entry);\t\\ (_work)-\u0026gt;func = (_func);\t\\ } while (0)  /* do { __init_work(\u0026amp;led_cdev-\u0026gt;set_brightness_work, 0); (\u0026amp;led_cdev-\u0026gt;set_brightness_work)-\u0026gt;data = (atomic_long_t) WORK_DATA_INIT(); INIT_LIST_HEAD(\u0026amp;(\u0026amp;led_cdev-\u0026gt;set_brightness_work)-\u0026gt;entry); (\u0026amp;led_cdev-\u0026gt;set_brightness_work)-\u0026gt;func = set_brightness_delayed; } while (0) */ 函数介绍：INIT_WORK该宏定义，两个参数，一个是表示工作队列的指针\u0026amp;led_cdev-\u0026gt;set_brightness_work，一个是工作队列的处理函数set_brightness_delayed\n实现思路：\n 调用INIT_WORK宏定义，将工作队列指针和处理函数作为参数传递 最终通过do while将工作队列初始化函数包括在内，实现工作队列的初始化  2.2.2 timer_setup #  timer_setup(\u0026amp;led_cdev-\u0026gt;blink_timer, led_timer_function, 0); 函数介绍：这里不展开介绍，同上，将定时器指针和处理函数作为参数，直接初始化。定时器的超时时间通过mod_timer来设置。\n 由上文可知，我们led-core.c的核心是初始化了一个工作队列和一个定时器函数，通过延迟工作队列来设置 LED 亮度，并通过计时器来进行软件闪烁。\n我们先来分析led_timer_function函数\n 3、led_timer_function分析 #   由上文可知，led_timer_function主要负责LED的闪烁功能\n static void led_timer_function(struct timer_list *t) { struct led_classdev *led_cdev = from_timer(led_cdev, t, blink_timer);\t//\t获取结构体指针  unsigned long brightness; unsigned long delay; if (!led_cdev-\u0026gt;blink_delay_on || !led_cdev-\u0026gt;blink_delay_off) {\t//\t闪烁延时判断  led_set_brightness_nosleep(led_cdev, LED_OFF); clear_bit(LED_BLINK_SW, \u0026amp;led_cdev-\u0026gt;work_flags); return; } if (test_and_clear_bit(LED_BLINK_ONESHOT_STOP,\t//\t测试是否为闪烁一次  \u0026amp;led_cdev-\u0026gt;work_flags)) { clear_bit(LED_BLINK_SW, \u0026amp;led_cdev-\u0026gt;work_flags); return; } brightness = led_get_brightness(led_cdev);\t//\t获取当前亮度值  if (!brightness) { /* Time to switch the LED on. */ if (test_and_clear_bit(LED_BLINK_BRIGHTNESS_CHANGE, \u0026amp;led_cdev-\u0026gt;work_flags)) brightness = led_cdev-\u0026gt;new_blink_brightness;\t//\t设置新的亮度值  else brightness = led_cdev-\u0026gt;blink_brightness;\t//\t设置默认亮度值  delay = led_cdev-\u0026gt;blink_delay_on; } else { /* Store the current brightness value to be able * to restore it when the delay_off period is over. */ led_cdev-\u0026gt;blink_brightness = brightness;\t//\t存储当前亮度值  brightness = LED_OFF;\t//\t更新亮度值为0  delay = led_cdev-\u0026gt;blink_delay_off; } led_set_brightness_nosleep(led_cdev, brightness);\t//\t设置亮度  /* Return in next iteration if led is in one-shot mode and we are in * the final blink state so that the led is toggled each delay_on + * delay_off milliseconds in worst case. */ if (test_bit(LED_BLINK_ONESHOT, \u0026amp;led_cdev-\u0026gt;work_flags)) {\t//\t一次闪烁判断  if (test_bit(LED_BLINK_INVERT, \u0026amp;led_cdev-\u0026gt;work_flags)) { if (brightness) set_bit(LED_BLINK_ONESHOT_STOP, \u0026amp;led_cdev-\u0026gt;work_flags); } else { if (!brightness) set_bit(LED_BLINK_ONESHOT_STOP, \u0026amp;led_cdev-\u0026gt;work_flags); } } mod_timer(\u0026amp;led_cdev-\u0026gt;blink_timer, jiffies + msecs_to_jiffies(delay));\t//\t更新定时器时间 } 函数介绍：该函数实现了定时触发，每次触发控制LED的亮度情况，实现亮灭的效果。\n实现思路：\n 首先通过from_timer接口，底层还是通过container_of函数，获取led_classdev结构体指针 判断blink_delay_on和blink_delay_off延时是否为0，如果为0，默认为关闭LED 通过led_get_brightness获取亮度值，闪烁逻辑如下：  如果亮度为0，则设置亮度值，更新延时为亮延时：delay = led_cdev-\u0026gt;blink_delay_on 如果亮度不为0，则设置亮度为0，brightness = LED_OFF;然后设置延时为灭延时：delay = led_cdev-\u0026gt;blink_delay_off   调用led_set_brightness_nosleep设置LED亮度 调用mod_timer更新定时器时间  3.1.1 led_get_brightness #  static inline int led_get_brightness(struct led_classdev *led_cdev) { return led_cdev-\u0026gt;brightness; } 函数介绍：该函数比较简单，直接获取led_classdev结构体的亮度值即可！\n3.1.2 led_set_brightness_nosleep #  void led_set_brightness_nosleep(struct led_classdev *led_cdev, enum led_brightness value) { led_cdev-\u0026gt;brightness = min(value, led_cdev-\u0026gt;max_brightness); if (led_cdev-\u0026gt;flags \u0026amp; LED_SUSPENDED) return; led_set_brightness_nopm(led_cdev, led_cdev-\u0026gt;brightness); } EXPORT_SYMBOL_GPL(led_set_brightness_nosleep); 函数介绍：与最大亮度值进行比较，并设置LED亮度，如果LED被挂起（进入休眠），则直接返回。所以，正如其名，nosleep即不休眠时的函数生效。。\n3.1.3 led_set_brightness_nopm #  void led_set_brightness_nopm(struct led_classdev *led_cdev, enum led_brightness value) { /* Use brightness_set op if available, it is guaranteed not to sleep */ if (!__led_set_brightness(led_cdev, value)) return; /* If brightness setting can sleep, delegate it to a work queue task */ led_cdev-\u0026gt;delayed_set_value = value; schedule_work(\u0026amp;led_cdev-\u0026gt;set_brightness_work); } static int __led_set_brightness(struct led_classdev *led_cdev, enum led_brightness value) { if (!led_cdev-\u0026gt;brightness_set) return -ENOTSUPP; led_cdev-\u0026gt;brightness_set(led_cdev, value); return 0; } 函数介绍：设置LED亮度，该函数首先尝试使用__led_set_brightness函数来设置亮度，该函数保证不会使系统进入睡眠状态。如果不成功，则将亮度设置委托给工作队列任务。\n相关实现：\n  尝试调用__led_set_brightness接口，该函数用于未打开休眠功能。\n  如果该__led_set_brightness函数返回错误码，则代表了打开了休眠功能\n  休眠状态下，想要设置LED亮度值，需要将delayed_set_value变量设置为所需的亮度值，然后调度set_brightness_work任务。\n  4、set_brightness_delayed分析 #   由上面可知，如果子系统打开了休眠功能，设置LED亮度时，需要加入工作队列，而工作队列的处理函数即： set_brightness_delayed\n static void set_brightness_delayed(struct work_struct *ws) { struct led_classdev *led_cdev = container_of(ws, struct led_classdev, set_brightness_work); int ret = 0; if (test_and_clear_bit(LED_BLINK_DISABLE, \u0026amp;led_cdev-\u0026gt;work_flags)) { led_cdev-\u0026gt;delayed_set_value = LED_OFF; led_stop_software_blink(led_cdev); } ret = __led_set_brightness(led_cdev, led_cdev-\u0026gt;delayed_set_value); if (ret == -ENOTSUPP) ret = __led_set_brightness_blocking(led_cdev, led_cdev-\u0026gt;delayed_set_value); if (ret \u0026lt; 0 \u0026amp;\u0026amp; /* LED HW might have been unplugged, therefore don\u0026#39;t warn */ !(ret == -ENODEV \u0026amp;\u0026amp; (led_cdev-\u0026gt;flags \u0026amp; LED_UNREGISTERING) \u0026amp;\u0026amp; (led_cdev-\u0026gt;flags \u0026amp; LED_HW_PLUGGABLE))) dev_err(led_cdev-\u0026gt;dev, \u0026#34;Setting an LED\u0026#39;s brightness failed (%d)\\n\u0026#34;, ret); } 函数介绍：set_brightness_delayed正如其名，延时设置LED灯的亮度。\n实现思路：\n 首先通过container_of来获取led_classdev结构体指针 test_and_clear_bit对闪烁标志位进行判断，如果不支持，则关闭 调用__led_set_brightness和__led_set_brightness_blocking来设置LED的亮度  4.1.1 __led_set_brightness #  static int __led_set_brightness(struct led_classdev *led_cdev, enum led_brightness value) { if (!led_cdev-\u0026gt;brightness_set) return -ENOTSUPP; led_cdev-\u0026gt;brightness_set(led_cdev, value); return 0; } 函数介绍：非阻塞设置LED亮度，该函数调用led-gpio.c硬件驱动层分配的函数来操作硬件实现，用于不支持休眠时，不用考虑休眠是否打断该函数执行\n4.1.2 __led_set_brightness_blocking #  static int __led_set_brightness_blocking(struct led_classdev *led_cdev, enum led_brightness value) { if (!led_cdev-\u0026gt;brightness_set_blocking) return -ENOTSUPP; return led_cdev-\u0026gt;brightness_set_blocking(led_cdev, value); } 函数介绍：阻塞设置LED亮度，该函数调用led-gpio.c硬件驱动层分配的函数来操作硬件实现，用于在支持休眠时，避免休眠打断该函数执行。\n 这两个函数，如果看源码的话，还是有点绕的，因为__led_set_brightness_blocking内部竟然也直接调用了__led_set_brightness，所以这里也尽量还原代码原本的意思。\n 这篇就先讲到这里，当然该文件中还有一些xxx_blink相关的函数，主要用于管理闪烁，我们放到后面再了解。\n5、总结 #  上面我们了解到核心层的主要作用：通过延迟工作队列来设置 LED 亮度，并通过计时器来进行软件闪烁。\n5.1 代码实现流程 #  led_timer_function(drivers/leds/led-core.c) |--\u0026gt; led_get_brightness // 获取亮度值  |--\u0026gt; led_set_brightness_nosleep // 设置LED亮度  |--\u0026gt; led_set_brightness_nopm // 在非休眠状态下设置  |--\u0026gt; __led_set_brightness //  |--\u0026gt; led_cdev-\u0026gt;brightness_set// 硬件驱动层实现  set_brightness_delayed(drivers/leds/led-core.c) |--\u0026gt; __led_set_brightness // 非阻塞函数，调用该接口设置LED亮度后立即返回  |--\u0026gt; led_cdev-\u0026gt;brightness_set |--\u0026gt; gpio_led_set(drivers/leds/leds-gpio.c) // 最终调用的函数  |--\u0026gt; __led_set_brightness_blocking // 阻塞函数，调用该接口设置LED亮度后必须等待设置完成，才返回  |--\u0026gt; led_cdev-\u0026gt;brightness_set_blocking |--\u0026gt; gpio_led_set_blocking 核心层的接口，大都是提供给外部使用的，这些函数也都通过EXPORT_SYMBOL_GPL宏定义来导出的，即：向上提供了操作的接口。——乘上\n并且这些函数底层实现，都关联到了led-gpio.c硬件驱动层，即：调用底层的相关接口——启下\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":43,"href":"/docs/linux/linux_kernel_lock/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3linux%E9%94%81%E6%9C%BA%E5%88%B6%E4%BA%94%E8%A1%8D%E7%94%9F%E8%87%AA%E6%97%8B%E9%94%81/","title":"【深入理解Linux锁机制】五、衍生自旋锁","section":"Linux 内核锁详解","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":44,"href":"/docs/linux/linux_memory_manage/%E4%BA%94%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E7%A9%BA%E9%97%B4%E5%B8%83%E5%B1%80%E5%8F%8A%E7%AE%A1%E7%90%86/","title":"五、物理内存空间布局及管理","section":"Linux 内存管理","content":"Linux内存管理 | 五、物理内存空间布局及管理 #  上章，我们介绍了物理内存的访问内存模型和组织内存模型，我们再来回顾一下：\n物理内存的访问内存模型分为：\n UMA：一致内存访问 NUMA：非一致内存访问  物理内存的组织模型：\n FLATMEM：平坦内存模型 DISCONTIGMEM：不连续内存模型 SMARSEMEM：稀疏内存模型  Linux内核为了用统一的代码获取最大程度的兼容性，对物理内存的定义方面，引入了：内存结点（node）、内存区域（zone），内存页（page）的概念，下面我们来一一探究。\n 更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  1、内存节点node #  内存节点的引入，是Linux为了最大程度的提高兼容性，将UMA和NUMA系统统一起来，对于UMA而言是只有一个节点的系统。\n 下面的代码部分，我们尽可能的只保留暂时用的到的部分，不涉及太多的体系架相关的细节。\n 在Linux内核中，我们使用 typedef struct pglist_data pg_data_t表示一个节点\n/* * On NUMA machines, each NUMA node would have a pg_data_t to describe * it\u0026#39;s memory layout. On UMA machines there is a single pglist_data which * describes the whole memory. * * Memory statistics and page replacement data structures are maintained on a * per-zone basis. */ typedef struct pglist_data { ... int node_id; struct page *node_mem_map; unsigned long node_start_pfn; unsigned long node_present_pages; /* total number of physical pages */ unsigned long node_spanned_pages; /* total size of physical page range, including holes */ ... } pg_data_t;   node_id：每个节点都有自己的ID\n  node_mem_map：当前节点的struct page数组，用来管理这个节点的所有的页\n  node_start_pfn：这个节点的起始页号\n  node_present_pages：这个节点的真正可用的物理内存的页面数\n  node_spanned_pages：这个节点所包含的物理内存的页面数，包括不连续的内存空洞\n   例如，64M 物理内存隔着一个 4M 的空洞，然后是另外的 64M 物理内存。\n这样换算成页面数目就是，16K 个页面隔着 1K 个页面，然后是另外 16K 个页面。\n这种情况下，node_spanned_pages 就是 33K 个页面，node_present_pages 就是 32K 个页面。\n  内核使用了一个大小为 MAX_NUMNODES ，类型为 struct pglist_data 的全局数组 node_data[] 来管理所有的 NUMA 节点。\n2、内存区域zone #  2.1 各区域的布局 #  每一个节点，都被分成了一个个区域zone，我们看一下zone的定义：\nenum zone_type { #ifdef CONFIG_ZONE_DMA  ZONE_DMA, #endif #ifdef CONFIG_ZONE_DMA32  ZONE_DMA32, #endif  ZONE_NORMAL, #ifdef CONFIG_HIGHMEM  ZONE_HIGHMEM, #endif  ZONE_MOVABLE, #ifdef CONFIG_ZONE_DEVICE  ZONE_DEVICE, #endif  __MAX_NR_ZONES }; ZONE_DMA：用作DMA的内存。\n DMA 是这样一种机制：要把外设的数据读入内存或把内存的数据传送到外设，原来都要通过 CPU 控制完成，但是这会占用 CPU，影响 CPU 处理其他事情，所以有了 DMA 模式。\nCPU 只需向 DMA 控制器下达指令，让 DMA 控制器来处理数据的传送，数据传送完毕再把信息反馈给 CPU，这样就可以解放 CPU。\n ZONE_DMA32：对于 64 位系统，有两个 DMA 区域。除了上面说的 ZONE_DMA，还有 ZONE_DMA32。\nZONE_NORMAL：直接映射区，也就i是之前讲的从物理内存到虚拟内存的内核区域，通过加上一个常量直接映射。\nZONE_HIGHMEM：高端内存区\nZONE_MOVABLE：可移动区域，通过将物理内存划分为可移动分配区域和不可移动分配区域来避免内存碎片。\n2.2 各区域的管理 #  上面我们大致了解了，每个zone的布局情况，下面我们来看看内核是如何对其进行管理的。\n 接着上面介绍的pglist_data结构体\n /* * On NUMA machines, each NUMA node would have a pg_data_t to describe * it's memory layout. On UMA machines there is a single pglist_data which * describes the whole memory. * * Memory statistics and page replacement data structures are maintained on a * per-zone basis. */ typedef struct pglist_data { ... int node_id; struct page *node_mem_map; unsigned long node_start_pfn; unsigned long node_present_pages; /* total number of physical pages */ unsigned long node_spanned_pages; /* total size of physical page range, including holes */ ... struct zone node_zones[MAX_NR_ZONES]; struct zonelist node_zonelists[MAX_ZONELISTS]; int nr_zones; ... } pg_data_t;  nr_zones：用于统计 NUMA 节点内包含的物理内存区域个数，不是每个 NUMA 节点都会包含以上介绍的所有物理内存区域，NUMA 节点之间所包含的物理内存区域个数是不一样的。   事实上只有第一个 NUMA 节点可以包含所有的物理内存区域，其它的节点并不能包含所有的区域类型，因为有些内存区域比如：ZONE_DMA，ZONE_DMA32 必须从物理内存的起点开始。这些在物理内存开始的区域可能已经被划分到第一个 NUMA 节点了，后面的物理内存才会被依次划分给接下来的 NUMA 节点。因此后面的 NUMA 节点并不会包含 ZONE_DMA，ZONE_DMA32 区域。\nZONE_NORMAL、ZONE_HIGHMEM 和 ZONE_MOVABLE 是可以出现在所有 NUMA 节点上的。\n  node_zones[MAX_NR_ZONES]：node_zones该数组包括了所有的zone物理内存区域 node_zonelists[MAX_ZONELISTS]：是 struct zonelist 类型的数组，它包含了备用 NUMA 节点和这些备用节点中的物理内存区域。    下面我们看一下struct zone结构体\n struct zone { ...... struct pglist_data\t*zone_pgdat; struct per_cpu_pageset __percpu *pageset; unsigned long\tzone_start_pfn; /* * spanned_pages is the total pages spanned by the zone, including * holes, which is calculated as: * spanned_pages = zone_end_pfn - zone_start_pfn; * * present_pages is physical pages existing within the zone, which * is calculated as: *\tpresent_pages = spanned_pages - absent_pages(pages in holes); * * managed_pages is present pages managed by the buddy system, which * is calculated as (reserved_pages includes pages allocated by the * bootmem allocator): *\tmanaged_pages = present_pages - reserved_pages; * */ unsigned long\tmanaged_pages; unsigned long\tspanned_pages; unsigned long\tpresent_pages; const char\t*name; ...... /* free areas of different sizes */ struct free_area\tfree_area[MAX_ORDER]; /* zone flags, see below */ unsigned long\tflags; /* Primarily protects free_area */ spinlock_t\tlock; ...... } ____cacheline_internodealigned_in_  zone_start_pfn：表示属于这个zone的第一个页 spanned_pages：看注释我们可以知道，spanned_pages = zone_end_pfn - zone_start_pfn，表示该区域的所有物理内存的页面数，包括内存空洞 present_pages：看注释我们可以知道，present_pages = spanned_pages - absent_pages(pages in holes)，表示该区域真实存在的物理内存页面数，不包括空洞 managed_pages：看注释我们可以知道，managed_pages = present_pages - reserved_pages，被伙伴系统管理的所有页面数。 per_cpu_pageset：用于区分冷热页，   什么叫冷热页呢？咱们讲 x86 体系结构的时候讲过，为了让 CPU 快速访问段描述符，在 CPU 里面有段描述符缓存。CPU 访问这个缓存的速度比内存快得多。同样对于页面来讲，也是这样的。如果一个页被加载到 CPU 高速缓存里面，这就是一个热页（Hot Page），CPU 读起来速度会快很多，如果没有就是冷页（Cold Page）。由于每个 CPU 都有自己的高速缓存，因而 per_cpu_pageset 也是每个 CPU 一个。\n   更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  3、内存页page #  内存页是物理内存最小单位，有时也叫页帧（page frame），Linux会为系统的物理内存的每一个页都创建了struct page对象，并用全局变量struct page *mem_map来存放所有物理页page对象的指针，页的大小取决于MMU（Memory Management Unit），后者主要用来将虚拟地址空间转换为物理地址空间。\n 看一下page的结构体\n struct page { unsigned long flags;\t/* Atomic flags, some possibly * updated asynchronously */ union { struct {\t/* Page cache and anonymous pages */ /** * @lru: Pageout list, eg. active_list protected by * zone_lru_lock. Sometimes used as a generic list * by the page owner. */ struct list_head lru; /* See page-flags.h for PAGE_MAPPING_FLAGS */ struct address_space *mapping; pgoff_t index;\t/* Our offset within mapping. */ /** * @private: Mapping-private opaque data. * Usually used for buffer_heads if PagePrivate. * Used for swp_entry_t if PageSwapCache. * Indicates order in the buddy system if PageBuddy. */ unsigned long private; }; struct {\t/* slab, slob and slub */ union { struct list_head slab_list;\t/* uses lru */ struct {\t/* Partial pages */ struct page *next; #ifdef CONFIG_64BIT  int pages;\t/* Nr of pages left */ int pobjects;\t/* Approximate count */ #else  short int pages; short int pobjects; #endif  }; }; struct kmem_cache *slab_cache; /* not slob */ /* Double-word boundary */ void *freelist;\t/* first free object */ union { void *s_mem;\t/* slab: first object */ unsigned long counters;\t/* SLUB */ struct {\t/* SLUB */ unsigned inuse:16; unsigned objects:15; unsigned frozen:1; }; }; }; ..... } 我们能够看到struct page有很多union组成，union 结构是在 C 语言中被用于同一块内存根据情况保存不同类型数据的一种方式。这里之所以用了 union，是因为一个物理页面使用模式有多种。\n 第一种模式：直接用一整页，这一整页的物理内存直接与虚拟地址空间建立映射关系，我们把这种称为匿名页（Anonymous Page）。或者用于关联一个文件，然后再和虚拟地址空间建立映射关系，这样的文件，我们称为内存映射文件（Memory-mapped File），这种分配页级别的，Linux采用一种被称为伙伴系统（Buddy System）的技术。 第二种模式：仅需要分配小的内存块。有时候，我们不需要一下子分配这么多的内存，例如分配一个 task_struct 结构，只需要分配小块的内存，去存储这个进程描述结构的对象。为了满足对这种小内存块的需要，Linux 系统采用了一种被称为slab allocator的技术   上面说的两种，都是页的分配方式，也就是物理内存的分配方式，下一章，我们继续深入分析物理内存的这两种分配方式。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":45,"href":"/docs/linux/linux_mmc_subsystem/mmc%E5%AD%90%E7%B3%BB%E7%BB%9F%E5%85%ADmmc%E5%9D%97%E8%AE%BE%E5%A4%87%E5%B1%82/","title":"【MMC子系统】六、MMC块设备层","section":"Linux MMC 子系统","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":46,"href":"/docs/linux/bluetooth/bluetooth%E8%93%9D%E7%89%99%E5%BC%80%E5%8F%91%E5%85%ADble%E5%8D%8F%E8%AE%AE%E4%B9%8B%E4%BC%A0%E8%BE%93%E5%B1%82/","title":"【Bluetooth蓝牙开发】六、BLE协议之传输层","section":"Bluetooth蓝牙开发","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":47,"href":"/docs/linux/linux_led_subsystem/led%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90%E5%85%AD%E6%A0%B8%E5%BF%83%E5%B1%82%E8%AF%A6%E8%A7%A3%E4%B8%89/","title":"【LED子系统深度剖析】六、核心层详解（三）","section":"Linux LED子系统","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":48,"href":"/docs/linux/linux_kernel_lock/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3linux%E9%94%81%E6%9C%BA%E5%88%B6%E5%85%AD%E4%BF%A1%E5%8F%B7%E9%87%8F/","title":"【深入理解Linux锁机制】六、信号量","section":"Linux 内核锁详解","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":49,"href":"/docs/linux/linux_memory_manage/%E5%85%AD%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E4%BC%99%E4%BC%B4%E7%B3%BB%E7%BB%9F/","title":"六、物理内存分配——伙伴系统","section":"Linux 内存管理","content":"Linux内存管理 | 六、物理内存分配——伙伴系统 #  上一章，我们了解了物理内存的布局以及Linux内核对其的管理方式，页（page）也是物理内存的最小单元，Linux内核对物理内存的分配主要分为两种：一种是整页的分配，采用的是伙伴系统，另一种是小内存块的分配，采用的是slab技术。\n下面我们先来看看什么是伙伴系统！\n 1、伙伴系统（Buddy System） #  Linux系统中，对物理内存进行分配的核心是建立在页面级的伙伴系统之上。Linux内存管理的页大小为4KB，把所有的空闲页分组为11个页块链表，每个链表分别包含很多个大小的页块，有 1、2、4、8、16、32、64、128、256、512 和 1024 个连续页的页块，最大可以申请 1024 个连续页，对应 4MB 大小的连续内存。每个页块的第一个页的物理地址是该页块大小的整数倍。\n如下图所示：\n 第 i 个页块链表中，页块中页的数目为 2^i。——仔细理解这个页块的含义。\n  在struct zone结构体中，有下面定义\nstruct free_area\tfree_area[MAX_ORDER]; #define MAX_ORDER 11 free_area：存放不同大小的页块\nMAX_ORDER：就是指数\n 当向内核请求分配 (2^(i-1)，2^i] 数目的页块时，按照 2^i 页块请求处理。如果对应的页块链表中没有空闲页块，那我们就在更大的页块链表中去找。当分配的页块中有多余的页时，伙伴系统会根据多余的页块大小插入到对应的空闲页块链表中。\n举个例子：\n例如，要请求一个 128 个页的页块时，先检查 128 个页的页块链表是否有空闲块。如果没有，则查 256 个页的页块链表；如果有空闲块的话，则将 256 个页的页块分成两份，一份使用，一份插入 128 个页的页块链表中。如果还是没有，就查 512 个页的页块链表；如果有的话，就分裂为 128、128、256 三个页块，一个 128 的使用，剩余两个插入对应页块链表。\n 上面的这套机制就是伙伴系统所做的事情，它主要负责对物理内存页面进行跟踪，记录哪些是被内核使用的页面，哪些是空闲页面。\n 2、页面分配器（Page Allocator） #  由上一章我们知道，物理内存被分为了几个区域：ZONE_DMA、ZONE_NORMAL、ZONE_HIGHMEM，其中前两个区域的物理页面与虚拟地址空间是线性映射的。\n页面分配器主要的工作原理如下：\n 如果页面分配器分配的物理页面在ZONE_DMA、ZONE_NORMAL区域，那么对应的虚拟地址到物理地址映射的页目录已经建立，因为是线性映射，两者之间有一个差值PAGE_OFFSET。 如果页面分配器分配的物理页面在ZONE_HIGHMEM区域，那么内核此时还没有对该页面进行映射，因此页面分配器的调用者，首先在虚拟地址空间的动态映射区或者固定映射区分配一个虚拟地址，然后映射到该物理页面上。   以上就是页面分配器的原理，对于我们只需要调用相关接口函数就可以了。\n页面分配函数主要有两个：alloc_pages和__get_free_pages，而这两个函数最终也会调用到alloc_pages_node，其实现原理完全一样。\n 下面我们从代码层面来看页面分配器的工作原理\n   更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  3、gfp_mask #  我们先来了解一下gfp_mask，它并不是页面分配器函数，而只是这些页面分配函数中一个重要的参数，是个用于控制分配行为的掩码，并可以告诉内核应该到哪个zone中分配物理内存页面。\n/* Plain integer GFP bitmasks. Do not use this directly. */ #define ___GFP_DMA\t0x01u #define ___GFP_HIGHMEM\t0x02u #define ___GFP_DMA32\t0x04u #define ___GFP_MOVABLE\t0x08u #define ___GFP_RECLAIMABLE\t0x10u #define ___GFP_HIGH\t0x20u #define ___GFP_IO\t0x40u #define ___GFP_FS\t0x80u #define ___GFP_WRITE\t0x100u #define ___GFP_NOWARN\t0x200u #define ___GFP_RETRY_MAYFAIL\t0x400u #define ___GFP_NOFAIL\t0x800u #define ___GFP_NORETRY\t0x1000u #define ___GFP_MEMALLOC\t0x2000u #define ___GFP_COMP\t0x4000u #define ___GFP_ZERO\t0x8000u #define ___GFP_NOMEMALLOC\t0x10000u #define ___GFP_HARDWALL\t0x20000u #define ___GFP_THISNODE\t0x40000u #define ___GFP_ATOMIC\t0x80000u #define ___GFP_ACCOUNT\t0x100000u #define ___GFP_DIRECT_RECLAIM\t0x200000u #define ___GFP_KSWAPD_RECLAIM\t0x400000u #ifdef CONFIG_LOCKDEP #define ___GFP_NOLOCKDEP\t0x800000u #else #define ___GFP_NOLOCKDEP\t0 #endif   ___GFP_DMA：在ZONE_DMA标识的内存区域中查找空闲页。\n  ___GFP_HIGHMEM：在ZONE_HIGHMEM标识的内存区域中查找空闲页。\n  ___GFP_MOVABLE：内核将分配的物理页标记为可移动的。\n  ___GFP_HIGH：内核允许使用紧急分配链表中的保留内存页。该请求必须以原子方式完成，意味着请求过程不允许被中断。\n  ___GFP_IO：内核在查找空闲页的过程中可以进行I/O操作，如此内核可以将换出的页写到硬盘。\n  ___GFP_FS：查找空闲页的过程中允许执行文件系统相关操作。\n  ___GFP_ZERO：用0填充成功分配出来的物理页。\n   通常意义上，这些以“__”打头的GFP掩码只限于在内存管理组件内部的代码使用，对于提供给外部的接口，比如驱动程序中所使用的页面分配函数，gfp_mask掩码以“GFP_”的形式出现，而这些掩码基本上就是上面提到的掩码的组合。\n例如内核为外部模块提供的最常使用的几个掩码如下：\n#define GFP_ATOMIC\t(__GFP_HIGH|__GFP_ATOMIC|__GFP_KSWAPD_RECLAIM) #define GFP_KERNEL\t(__GFP_RECLAIM | __GFP_IO | __GFP_FS) #define GFP_KERNEL_ACCOUNT (GFP_KERNEL | __GFP_ACCOUNT) #define GFP_NOWAIT\t(__GFP_KSWAPD_RECLAIM) #define GFP_NOIO\t(__GFP_RECLAIM) #define GFP_NOFS\t(__GFP_RECLAIM | __GFP_IO) #define GFP_USER\t(__GFP_RECLAIM | __GFP_IO | __GFP_FS | __GFP_HARDWALL) #define GFP_DMA\t__GFP_DMA #define GFP_DMA32\t__GFP_DMA32 #define GFP_HIGHUSER\t(GFP_USER | __GFP_HIGHMEM) #define GFP_HIGHUSER_MOVABLE\t(GFP_HIGHUSER | __GFP_MOVABLE) #define GFP_TRANSHUGE_LIGHT\t((GFP_HIGHUSER_MOVABLE | __GFP_COMP | \\ __GFP_NOMEMALLOC | __GFP_NOWARN) \u0026amp; ~__GFP_RECLAIM) #define GFP_TRANSHUGE\t(GFP_TRANSHUGE_LIGHT | __GFP_DIRECT_RECLAIM)  GFP_ATOMIC：内核模块中最常使用的掩码之一，用于原子分配。此掩码告诉页面分配器，在分配内存页时，绝对不能中断当前进程或者把当前进程移出调度器。 GFP_KERNEL：内核模块中最常使用的掩码之一，带有该掩码的内存分配可能导致当前进程进入睡眠状态。 GFP_USER：用于为用户空间分配内存页，可能引起进程的休眠。 GFP_NOIO：在分配过程中禁止I/O操作 GFP_NOFS：禁止文件系统相关的函数调用 GFP_DMA：限制页面分配器只能在ZONE_DMA域中分配空闲物理页面，用于分配适用于DMA缓冲区的内存。   通过gfp_mask掩码，更加方便我们控制页面分配器到哪个区域去分配物理内存，分配内存的优先级如下：\n 指定__GFP_HIGHMEM：先在ZONE_HIGHMEM域中查找空闲页，如果无法满足当前分配，页分配器将回退到ZONE_NORMAL域中继续查找，如果依然无法满足当前分配，分配器将回退到ZONE_DMA域，或者成功或者失败。 指定__GFP_DMA：只能在ZONE_DMA中分配物理页面，如果无法满足，则分配失败。 没有__GFP_NORMAL这样的掩码，但是前面已经提到，如果gfp_mask中没有明确指定__GFP_HIGHMEM或者是__GFP_DMA，默认就相当于__GFP_NORMAL，优先在ZONE_NORMAL域中分配，其次是ZONE_DMA域。   4、alloc_pages #  alloc_pages函数负责分配2^order个连续的物理页面并返回起始页的struct page实例。\nalloc_pages的实现源码如下：\nstatic inline struct page * alloc_pages(gfp_t gfp_mask, unsigned int order) { return alloc_pages_current(gfp_mask, order); } /** * alloc_pages_current - Allocate pages. * *\t@gfp: *\t%GFP_USER user allocation, * %GFP_KERNEL kernel allocation, * %GFP_HIGHMEM highmem allocation, * %GFP_FS don\u0026#39;t call back into a file system. * %GFP_ATOMIC don\u0026#39;t sleep. *\t@order: Power of two of allocation size in pages. 0 is a single page. * *\tAllocate a page from the kernel page pool. When not in *\tinterrupt context and apply the current process NUMA policy. *\tReturns NULL when no page can be allocated. */ struct page *alloc_pages_current(gfp_t gfp, unsigned order) { struct mempolicy *pol = \u0026amp;default_policy; struct page *page; if (!in_interrupt() \u0026amp;\u0026amp; !(gfp \u0026amp; __GFP_THISNODE)) pol = get_task_policy(current); /* * No reference counting needed for current-\u0026gt;mempolicy * nor system default_policy */ if (pol-\u0026gt;mode == MPOL_INTERLEAVE) page = alloc_page_interleave(gfp, order, interleave_nodes(pol)); else page = __alloc_pages_nodemask(gfp, order, policy_node(gfp, pol, numa_node_id()), policy_nodemask(gfp, pol)); return page; } EXPORT_SYMBOL(alloc_pages_current); alloc_pages调用alloc_pages_current，其中\n gfp参数：即上文的gfp_mask，表明我们想要在哪个物理内存区域进行内存分配 order参数：表示分配 2 的 order 次方个页。  __alloc_pages_nodemask为伙伴系统的核心实现，它会调用 get_page_from_freelist。\nstatic struct page * get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags, const struct alloc_context *ac) { ...... for_next_zone_zonelist_nodemask(zone, z, ac-\u0026gt;zonelist, ac-\u0026gt;high_zoneidx, ac-\u0026gt;nodemask) { struct page *page; ...... page = rmqueue(ac-\u0026gt;preferred_zoneref-\u0026gt;zone, zone, order, gfp_mask, alloc_flags, ac-\u0026gt;migratetype); ...... } 这里面的逻辑也很容易理解，就是在一个循环中先看当前节点的 zone。如果找不到空闲页，则再看备用节点的 zone。\n 每一个 zone，都有伙伴系统维护的各种大小的队列，就像上面伙伴系统原理里讲的那样。\n 这里调用 rmqueue 就很好理解了，就是找到合适大小的那个队列，把页面取下来。\n 伙伴系统的实现代码，感兴趣的可以深入探究。\n 在调用这个函数的时候，有几种情况：\n 如果gfp_mask中没有指定__GFP_HIGHMEM，那么分配的物理页面必然来自ZONE_NORMAL或者ZONE_DMA，由于这两个区域内核在初始化的时候就已经建立了映射关系，所以内核很容易就能找到对应的虚拟地址KVA（Kernel Virtual Address） 如果gfp_mask中指定了__GFP_HIGHMEM，那么页分配器将优先在ZONE_HIGHMEM域中分配物理页，但也不排除因为ZONE_HIGHMEM没有足够的空闲页导致页面来自ZONE_NORMAL与ZONE_DMA域的可能性。对于新分配出的高端物理页面，由于内核尚未在页表中为之建立映射关系，所以此时需要：  在内核的动态映射区分配一个KVA 通过操作页表，将第一步中的KVA映射到该物理页面上，通过kmap实现     5、__get_free_pages #  __get_free_pages该函数负责分配2^ordev个连续的物理页面，返回起始页面所在内核线性地址。\n函数的实现如下：\n/* * Common helper functions. Never use with __GFP_HIGHMEM because the returned * address cannot represent highmem pages. Use alloc_pages and then kmap if * you need to access high mem. */ unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order) { struct page *page; page = alloc_pages(gfp_mask \u0026amp; ~__GFP_HIGHMEM, order); if (!page) return 0; return (unsigned long) page_address(page); } EXPORT_SYMBOL(__get_free_pages); 我们可以看到，函数内部调用了alloc_pages函数，并且不能从__GFP_HIGHMEM高端内存分配物理页，最后通过page_address来返回页面的起始页面的内核线性地址。\n 6、get_zeroed_page #  get_zeroed_page用于分配一个物理页同时将页面对应的内容填充为0，函数返回页面所在的内核线性地址。\n可以看下内核代码：\nunsigned long get_zeroed_page(gfp_t gfp_mask) { return __get_free_pages(gfp_mask | __GFP_ZERO, 0); } EXPORT_SYMBOL(get_zeroed_page); 仅仅是在__get_free_pages基础上，使用了 __GFP_ZERO标志，来初始化分配页面的初始内容。\n 7、总结 #  以上，就是建立在伙伴系统之上的页面级分配器，常用的函数有：alloc_pages、__get_free_pages、get_zeroed_page、__get_dma_pages等，其底层实现都是一样的，只是gfp_mask不同。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":50,"href":"/docs/linux/bluetooth/bluetooth%E8%93%9D%E7%89%99%E5%BC%80%E5%8F%91%E4%B8%83ble%E5%8D%8F%E8%AE%AE%E4%B9%8Bl2cap/","title":"【Bluetooth蓝牙开发】七、BLE协议之L2CAP","section":"Bluetooth蓝牙开发","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":51,"href":"/docs/linux/linux_led_subsystem/led%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90%E4%B8%83%E8%A7%A6%E5%8F%91%E5%99%A8%E5%AE%9E%E7%8E%B0/","title":"【LED子系统深度剖析】七、触发器实现","section":"Linux LED子系统","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":52,"href":"/docs/linux/linux_kernel_lock/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3linux%E9%94%81%E6%9C%BA%E5%88%B6%E4%B8%83%E4%BA%92%E6%96%A5%E4%BD%93/","title":"【深入理解Linux锁机制】七、互斥体","section":"Linux 内核锁详解","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":53,"href":"/docs/linux/bluetooth/bluetooth%E8%93%9D%E7%89%99%E5%BC%80%E5%8F%91%E5%85%ABble%E5%8D%8F%E8%AE%AE%E4%B9%8Batt/","title":"【Bluetooth蓝牙开发】八、BLE协议之ATT","section":"Bluetooth蓝牙开发","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":54,"href":"/docs/linux/linux_led_subsystem/led%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90%E5%85%AB%E5%B0%8F%E8%AF%95%E7%89%9B%E5%88%80/","title":"【LED子系统深度剖析】八、小试牛刀","section":"Linux LED子系统","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":55,"href":"/docs/linux/linux_kernel_lock/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3linux%E9%94%81%E6%9C%BA%E5%88%B6%E5%85%AB%E5%AE%8C%E6%88%90%E9%87%8F/","title":"【深入理解Linux锁机制】八、完成量","section":"Linux 内核锁详解","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":56,"href":"/docs/linux/bluetooth/bluetooth%E8%93%9D%E7%89%99%E5%BC%80%E5%8F%91%E4%B9%9Dble%E5%8D%8F%E8%AE%AEgatt/","title":"【Bluetooth蓝牙开发】九、BLE协议——GATT","section":"Bluetooth蓝牙开发","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":57,"href":"/docs/linux/linux_led_subsystem/led%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90%E4%B9%9D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3%E7%95%AA%E5%A4%96%E7%AF%87/","title":"【LED子系统深度剖析】九、数据结构详解（番外篇）","section":"Linux LED子系统","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":58,"href":"/docs/linux/bluetooth/bluetooth%E8%93%9D%E7%89%99%E5%BC%80%E5%8F%91%E5%8D%81ble%E8%93%9D%E7%89%99%E9%80%9A%E4%BF%A1%E6%B5%81%E7%A8%8B%E5%BB%BA%E7%AB%8B%E8%BF%9E%E6%8E%A5%E5%B9%BF%E6%92%AD%E6%89%AB%E6%8F%8F%E6%96%AD%E5%BC%80%E8%BF%9E%E6%8E%A5/","title":"【Bluetooth蓝牙开发】十、BLE蓝牙通信流程（建立连接，广播，扫描，断开连接）","section":"Bluetooth蓝牙开发","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":59,"href":"/docs/linux/linux_led_subsystem/led%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90%E5%8D%81%E8%AF%A6%E7%BB%86%E5%AE%9E%E7%8E%B0%E6%B5%81%E7%A8%8B%E7%95%AA%E5%A4%96%E7%AF%87/","title":"【LED子系统深度剖析】十、详细实现流程（番外篇）","section":"Linux LED子系统","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":60,"href":"/docs/linux/bluetooth/bluetooth%E8%93%9D%E7%89%99%E5%BC%80%E5%8F%91%E5%8D%81%E4%B8%80%E8%B6%85%E8%AF%A6%E7%BB%86%E7%9A%84bluez%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/","title":"【Bluetooth蓝牙开发】十一、超详细的Bluez交叉编译","section":"Bluetooth蓝牙开发","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":61,"href":"/docs/linux/bluetooth/bluetooth%E8%93%9D%E7%89%99%E5%BC%80%E5%8F%91%E5%8D%81%E4%BA%8C%E8%93%9D%E7%89%99%E8%B0%83%E8%AF%95%E5%B7%A5%E5%85%B7%E9%9B%86%E5%90%88%E6%B1%87%E6%80%BB/","title":"【Bluetooth蓝牙开发】十二、蓝牙调试工具【集合汇总】","section":"Bluetooth蓝牙开发","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":62,"href":"/docs/embeded_tech/self_improve/10w+%E9%98%85%E8%AF%BB%E8%80%97%E6%97%B6%E4%B8%80%E5%91%A8%E6%80%BB%E7%BB%93%E7%9A%84%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF%E8%B6%85%E8%AF%A6%E7%BB%86/","title":"【10W+阅读】耗时一周总结的嵌入式学习路线，超详细","section":"嵌入式工程师养成记","content":"【10W+阅读】耗时一周总结的嵌入式学习路线，超详细 #  人们常说：“人生就是一场场游戏，我们要做的，就是打怪，升级，通关”，学习嵌入式的过程也是如此。\n1、前言 #  最近看到知乎上，给我推送了很多类似的回答，借此机会，也想着重新审视一下自己的学习历程，以及结合自身和大牛，分享一些学习经验，希望对大家有所启发和帮助。\n本文主要目的是为了：\n 提供一张嵌入式学习路线地图 提供不同阶段的学习建议 推荐不同阶段的学习资料  3000余字，耗时1周，建议收藏；码字不易，如有帮助，慷慨三连\n 本文将嵌入式学习路线分为几个方面：\n 嵌入式基础必备知识 51单片机 STM32单片机 小而美的RTOS ARM+LINUX   在这个快节奏的时代，能静下心，耐住性子看看文章，实属不易。\n  2、嵌入式基础必备知识 #  老子曰：“合抱之木，生于毫末：九层之台，起于垒土；千里之行，始于足下”，根基的重要性不言而喻。\n那么对于嵌入式这条路线而言，如何建立一个稳固的根基？\n 2.1、学习内容 #   C语言基础  该部分，主要包括几个核心知识点：三大语法结构、常用的数据类型、函数、结构体、指针、文件操作等。\n 硬件基础知识  该部分，核心知识点在于：电路基础知识、数电模电基础知识、常用的电子元器件等。\n 数据结构  核心知识点：数组、队列、链表、堆栈、树、图、散列表等。\n 操作系统  核心知识点：进程管理、内存管理、文件管理、输入输出管理等。\n 计算机原理  核心知识点：数据表示和运算、存储系统、指令系统、总线系统、中央处理器、输入输出系统等。\n 2.2、学习建议 #   对于C语言基础学习，一定要重点熟练掌握，根基的牢固直接决定了我们的代码质量。 对于硬件基础学习，要适当了解，要能够看懂一些简单的电路结构，认识常用的电子元器件。 对于数据结构学习，前五个是必备学习的，可能在刚开始学习的时候，可能会感觉不到作用在哪里，但是随着接触到嵌入式底层设计以及算法设计的时候，才会恍然大悟。 对于操作系统学习，重点学习其思想，对相关知识点有一个大概的了解，后续接触到继续重点学习，这些无论是RTOS，还是Linux，都有涉及到的。 对于计算机原理学习，可以将其看作是嵌入式系统的各个模块的详解，会让你对嵌入式有一个整体的了解，每一个部分都值得深究。   2.3、学习资料 #   C语言基础：推荐经典书籍**《C语言程序设计》（第2版）谭浩强版本**。 硬件基础：大学里面的《数电模电》书籍所涉及的知识即可。 数据结构：推荐经典书籍**《数据结构》——严蔚敏版**。 操作系统、计算机原理：我用的是**《王道》的系列丛书**，个人感觉不错。  计算机组成、数据结构、操作系统、数据库是嵌入式或者说计算机的入门必读书籍，并且也被列入高校教材内，是真正的基础知识。\n 以上，不一定是全部看完才能体验编程的乐趣，这个基础是一个循序渐进的过程，也不是一朝一夕就能完成的，可以先有一个大概，后续做项目时，哪里不懂补哪里！\n这里涉及到一个重要的学习方法：项目导向的学习法。\n 3、嵌入式入门篇——51单片机 #  在上面的基础知识进行熟悉之后（C语言基础、计算机组成、硬件基础必备），我们准备叩开嵌入式世界的大门。\n入门篇，依旧推荐51单片机，当然有人会说，直接上STM32岂不更好？\n我的看法：建议新手还是以51单片机来入门，因为STM32体系架构比51大很多，对于新手刚开始可能会不太容易适应。\n 3.1、学习内容 #  该部分，主要在最小嵌入式系统中，实现各种有趣的实验。通过51单片机的学习，我们要做到：\n 软件类：  主要知识点有：认识单片机、熟悉逻辑运算、点亮一颗LED灯、按键检测、串口通信、定时器、中断等。\n 硬件类：  主要知识点有：电阻元器件了解，基本模块电路了解，时钟电路，尝试绘制51单片机原理图和PCB\n 3.2、学习建议 #   对于软件类，我们主要做到：认识单片机，熟悉单片机的GPIO的输入、输出操作，串口通信协议掌握等，这些部分都是任何一款嵌入式设备的必备技能。 对于硬件类：我们主要做到：能看懂电路图，熟悉一些简单模块的设计电路，了解Altium Designer的使用方法。   3.3、学习资料 #  51单片机：郭天祥的51单片机教程，经典著作，经久不衰，强烈推荐。\n 庄子说：“水之积也不厚，则其负大舟也无力。“\n该部分，是嵌入式领域的基石，只有将基础打牢，才能负得起Linux泰坦号。\n 4、STM32进阶篇 #  STM32是C51的进阶版，拥有C51的基础知识，开发STM32会得心应手。\nSTM32的系统架构以及硬件设计相比于C51来说，都是上升了一个维度的，这也是为什么我推荐入门学习C51的原因。\n 以STM32F407平台为基础，去学习目前嵌入式主流的一些技术，探寻底层的原理，做到不同平台，都能够得心应手。\n 4.1、学习内容 #   基础练习  该部分，主要练习：点亮LED灯、GPIO的输入输出操作、中断操作、UART通信、IIC通信等\n 进阶练习  该部分，主要练习：DMA通信、SPI通信、CAN通信、LCD显示屏，ADC等\n 高阶练习  该部分，主要学习：STM32时钟架构、总线架构、电源管理、代码框架、SDIO通信、USB通信等。\n 4.2、学习建议 #   对于基础练习，主要目的是为了方便让我们从C51到STM32环境的过渡。 对于进阶练习，主要练习一些通信类相关的协议，可以结合一些传感器进行开发。 对于高阶练习，主要目的是为了熟悉单片机的设计架构，编程的框架，以及一些更复杂的通信技术。  另外，STM32会有寄存器和库函数两个版本，建议交叉学习，理解会更加深刻。\n 4.3、学习资料 #  STM32单片机：推荐正点原子、野火的STM32F103或者STM32F407系列。\n两家的学习资料都非常丰富，既有详细的文档说明，也有完整的学习视频教程，非常适合新手入门学习。\n 俗话说：“有道无术，术尚可求，有术无道，止于术”。要明白道和术的区别，不要本末倒置。\n 5、小而美的RTOS #  RTOS，实时操作系统，可以理解为STM32与Linux之间的桥梁，由于其实现思想大都取之于Linux，所以也称之为精简版的Linux。\n我们常用的有实时操作系统有：UCOS，VxWork，FreeRtos，近些年RT-Thread也异军突起。\n学习这些简单的嵌入式系统，一来能够帮助我们为学习Linux操作系统打下基础，二来也能够扩宽我们的职业道路。\n前面也说过了，无论是UCOS、FreeRtos、Rt-thread，其内部的设计思想大同小异，下面主要以Ucos为例。\n5.1、学习内容 #   实时系统学习  该部分，主要学习：移植Ucos系统、多任务管理、调度算法、消息队列、信号量互斥量、事件、内存管理等。\n 5.2、学习建议 #   对于实时系统学习，除了上述的那些核心知识点外，还要结合2.1 基础必备知识的操作系统书籍加深理解。   5.3、学习资料 #  RTOS的学习：依旧推荐正点原子，野火，因为这些实时操作系统开发，可以基于STM32开发板，同时也有非常详细的文档和视频教学。\n 6、ARM+Linux篇 #  学习完RTOS后，基本嵌入式所涉及的技术已经掌握一半了，你也可以独立完成一些小的项目，也可以找到一个不错的工作，但是一定不要自我满足，有机会一定要接触Linux。\n还是那句话：ARM+Linux，也是最为复杂的东西，如果你不去接触Linux，你永远不知道嵌入式的魅力。\n Linux开发又分为驱动开发，内核开发，应用开发，每一个方向都需要几年甚至几十年的积累。\n 作为初学者，我们要做的就是宏观了解，扩大我们的知识面，然后去选择自己感兴趣的方面。\n 6.1、学习内容 #   Linux基础篇  该部分主要学习：Linux常用命令、VIM学习、Linux的Shell编程、Gcc编译、Makefile等。\n 驱动篇  该部分主要学习：内核模块编译原理、字符设备驱动框架、平台设备驱动、设备树、Pinctrl子系统、I2C子系统、中断子系统、块设备驱动框架、Bootloader等\n 内核篇  该部分主要学习：系统调用、存储管理、进程管理、内存管理、文件管理等。\n 应用篇  该部分主要学习：QT编程、TCP/IP协议、HTTP协议等。\n 6.2、学习建议 #   对于基础学习，刚接触到Linux，一般比较难上手，与之前的单片机完全不同，需要一个熟悉环境的过程。 对于驱动学习，重要在于明白“如何在Linux环境下编写驱动程序”，驱动的底层原理还是那样，加了一层层的框架，需要我们去熟悉。 对于内核学习，上述也是系统的几大核心特色，重点在于\u0026quot;如何使Linux性能最优\u0026quot; 对于应用学习，上述的几个方面也是基础，重点还在于开发什么应用，去学习哪方面的知识，没有定论。  对于Linux，有句老话“学习Linux，3年才算入门，5年才勉强算Linux工程师，对于不太熟悉的领域，博主也不敢妄加断言。”\n 6.3、学习资料 #   对于基础学习，推荐**《鸟哥的Linux私房菜》，《Unix环境高级编程》**等入门书籍。 对于驱动开发，推荐**《Linux设备驱动开发详解》**，Linux内核源码详解等。 对于内核学习，推荐**《Linux Shell脚本攻略》、《深入理解Linux内核》**等。 对于应用开发，推荐**《嵌入式Linux应用开发完全手册》、《Unix网络编程》**等。 另外，推荐正点原子，野火，韦东山三个Linux开发教程，韦老师的课程好评居多，但还是看哪个更适合自己。   7、总结 #  全文整体的学习路线：嵌入式基础学习 -\u0026gt; 51单片机 -\u0026gt; STM32单片机 -\u0026gt; RTOS篇 -\u0026gt; ARM+Linux\n每一个部分，也都从学习内容，学习建议，学习资料三个方面来展开，层层深入，步步指引。\n文章既是我的学习历程，又结合了一些大佬的学习分享，不断调整总结出来的，如有异同，可以讨论。\n全文3000余字，耗时1周，如有帮助，望不吝点赞关注。\n最后，文章所涉及的学习资料以及整理的思维导图，全部会在我的星球【嵌入式艺术】分享！\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":63,"href":"/about/index_zh/","title":"About","section":"Abouts","content":"1、个人介绍 #  🙍🏻‍♂️ 大家好，我是董哥，一名工作多年的嵌入式Linux开发工程师。以下是我的基本信息介绍：\n 参加全国机器人大赛（Robocon），两次获得全国一等奖 毕业后斩获科沃斯，石头，格力等多家头部机器人公司的offer，后入职世界五百强格力电器担任嵌入式开发工程师 现今就职于独角兽芯片企业，担任嵌入式Linux驱动开发工程师 熟练使用C/C++语言开发，熟悉各类MCU开发，如STM32，ARM，SOC等，熟悉Ucos，RT-thread实时操作系统等 目前主要负责Linux驱动，系统开发，WiFi\u0026amp;BT开发等相关工作，同时跟进并参与多款百万级量产项目的研发。 荣获优质嵌入式领域创作者称号，拿下2022年度博客之星嵌入式领域TOP 5，全网收获超百万读者。  2、技术与分享 #  记录Blog是一项值得挑战的事情，一方面是对自我技术的沉淀，另一方面也是四万万嵌入式开发者前行路上的加速剂；并且网上大多数文章七零八落，每个人对技术的理解程度不同，因此好的文章，永不过时！\n我的一些自媒体平台：\n CSDN：卍一十二画卍 知乎：嵌入式艺术 公众号：嵌入式艺术 知识星球：嵌入式艺术  3、我的星球 #  🚩 【嵌入式艺术】星球，目前是处于起步阶段，我们的目标是：携手共创高质量的嵌入式基地，兼收并蓄，群英荟萃，实现升职加薪创业梦！\n🛎️ 我们提供的服务有：\n 提供一个高级嵌入式工程师聚集地，聚焦嵌入式工程师成长与发展。 高质量嵌入式项目、技术的拆解与分析 高效率的嵌入式开发工具分享 AIGC + 嵌入式 应用，跟上时代的脚步 嵌入式的行业趋势与热点分析  🛎️ 我们后续要做的事情：\n 引入更多嵌入式领域大咖加入我们的星球，为大家提供更好的服务！ 引入更多优质公司的内推岗位，以便大家走内部推荐通道，加入头部企业！ 拆解更多嵌入式项目，为大家提供实战经验，以目标为导向，实现更好的学习效果！ 星球不定期举办激励活动，有实物激励以及现金激励两种，希望大家踊跃参加！  "}]