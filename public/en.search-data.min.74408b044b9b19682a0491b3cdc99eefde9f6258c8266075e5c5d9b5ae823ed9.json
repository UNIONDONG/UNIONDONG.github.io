[{"id":0,"href":"/docs/linux/linux_kernel_lock/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3linux%E9%94%81%E6%9C%BA%E5%88%B6%E4%B8%80%E5%86%85%E6%A0%B8%E9%94%81%E7%9A%84%E7%94%B1%E6%9D%A5/","title":"【深入理解Linux锁机制】一、内核锁的由来","section":"Linux 内核锁详解","content":"【深入理解Linux锁机制】一、内核锁的由来 #  在Linux设备驱动中，我们必须要解决的一个问题是：多个进程对共享资源的并发访问，并发的访问会导致竞态。\n1、并发和竞态 #  并发（Concurrency）：指的是多个执行单元同时、并行的被执行。\n竞态（RaceConditions）：并发执行的单元对共享资源的访问，容易导致竞态。\n共享资源：硬件资源和软件上的全局变量、静态变量等。\n解决竞态的途径是：保证对共享资源的互斥访问。\n互斥访问：一个执行单元在访问共享资源的时候，其他执行单元被禁止访问。\n临界区（Critical Sections）：访问共享资源的代码区域成为临界区。临界区需要以某种互斥机制加以保护。\n常见的互斥机制包括：中断屏蔽，原子操作，自旋锁，信号量，互斥体等。\n 2、竞态发生的场合 #  2.1 多对称处理器（SMP）的多个CPU之间 #  多个CPU使用共同的系统总线，可以访问共同的外设和存储器。在SMP的情况下，多核（CPU0、CPU1）的竞态可能发生于：\n CPU0的进程和CPU1的进程之间 CPU0的进程和CPU1的中断之间 CPU0的中断和CPU1的中断之间   2.2 单CPU内，该进程与抢占它的进程之间 #  在单CPU内，多个进程并发执行，当一个进程执行的时间片耗尽，也有可能被另一个高优先级进程打断，会发生竞态，即所谓的调度引发竞态。\n 2.3 中断（软中断、硬中断、Tasklet、底半部）与进程之间 #  当一个进程正在执行，一个外部/内部中断（软中断、硬中断、Tasklet等）将其打断，会导致竞态发生。\n 3、编译乱序和执行乱序 #  除了并发访问导致的竞态外，还需要了解编译器和处理器的一些特点所引发的一些问题。\n3.1 编译乱序 #   现代的高性能编译器，为了提高Cache命中率以及CPU的Load/Store工作效率，会对目标代码进行乱序优化，减少逻辑上不必要的访存！\n因此，在打开编译器优化后，生成的汇编码并没有严格按照代码的逻辑顺序执行，这是正常的。\n 为了解决编译乱序的问题，可以加入barrier()编译屏障。\n顾名思义，编译屏障，也就是为了阻挡编译器的编译优化，加入barrier()编译屏障，即可保证正确的执行顺序。\n编译屏障代码实现如下：\n#define barrier() __asm__ __volatile__(\u0026#34;\u0026#34;: : :\u0026#34;memory\u0026#34;) 这里详细解释一下barrier的汇编实现：\n __asm__：向编译器说明在此插入汇编代码 __volatile__：用于告诉编译器，严禁将此处的汇编语句与其它的语句重组合优化。 (\u0026quot;\u0026quot;: : :\u0026quot;memory\u0026quot;)：一条汇编语句，第一个:前为汇编指令，这里是空操作；第二个:前表示输出操作数，为空；第三个冒号前为输入操作数，也是要修改的寄存器；最后memory表示该指令对内存进行访问，该指令确保了命令之前的内存操作需要完全执行，不被优化。   使用案例：\nint main(int argc,char *argv[]) { int a = 0,b,c,d[4096],e; e = d[4095]; barrier(); b = a; c = a; return 0; }  3.2 执行乱序 #  编译乱序是编译器的行为，而执行乱序就是处理器运行时的行为。\n**高级的CPU往往会根据自身的缓存特性，将访存指令重新排序执行！**这样就导致了多个顺序的指令，后发的指令仍有可能先执行完毕。\n 这种执行乱序，在多个CPU之间，以及单个CPU内部，都是非常常见的。\n  3.2.1 多CPU之间 #  处理器为了解决多核之间执行乱序的问题，一个CPU的行为对另一个CPU可见的情况，ARM处理器引入了内存屏障指令：\n DMB（数据内存屏障），保证在该指令前的所有指令，内存访问完成，再去访问该指令之后的访存动作 DSB（数据同步屏障），保证在该指令前的所有访存指令执行完毕（访存，缓存，跳转预测，TLB维护等）完成 ISB（指令同步屏障），Flush流水线，保证所有在ISB之后执行的指令都是从缓存或者内存中获得。   3.2.2 单CPU内部 #   在单CPU中，我们常遇到访问外设寄存器时，某些外设寄存器就对读写顺序有很高的要求，为了避免执行乱序的发生，这时候就需要CPU的一些内存屏障指令了。\n CPU内部，为了解决这种问题，CPU提供了一些内存屏障指令：\n 可以参考Documentation/memory-devices.txt和Documentation/io_ordering.txt\n  读写屏障：mb() 读屏障：rmb() 写屏障：wmb() 寄存器读屏障__iormb()__ 寄存器写屏障__iowmb()__  #define writeb_relaxed(v,c)\t__raw_writeb(v,c) #define writew_relaxed(v,c)\t__raw_writew((__force u16) cpu_to_le16(v),c) #define writel_relaxed(v,c)\t__raw_writel((__force u32) cpu_to_le32(v),c)  #define readb(c)\t({ u8 __v = readb_relaxed(c); __iormb(); __v; }) #define readw(c)\t({ u16 __v = readw_relaxed(c); __iormb(); __v; }) #define readl(c)\t({ u32 __v = readl_relaxed(c); __iormb(); __v; })  #define writeb(v,c)\t({ __iowmb(); writeb_relaxed(v,c); }) #define writew(v,c)\t({ __iowmb(); writew_relaxed(v,c); }) #define writel(v,c)\t({ __iowmb(); writel_relaxed(v,c); })  writel与writel_relaxed的区别就在于有无屏障。\n  4、总结 #  由上文可知，发生竞态的场合，主要发生在\n 多对称处理器的多CPU之间 单CPU的进程调度、抢占引发的竞态 单CPU的中断与进程之间引发的竞态 高性能的编译器编译乱序问题 高性能的CPU带来的执行乱序问题  为了解决竞态的发生，CPU和ARM处理器提供的内存屏障指令等，同时也提供了中断屏蔽、原子操作、自旋锁、互斥锁、信号量等机制，下面我们来深入了解这些机制吧。\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":1,"href":"/docs/linux/linux_nvmem_subsystem/nvmem%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E4%B8%80efuse%E4%BB%8B%E7%BB%8D%E5%8F%8A%E5%AE%89%E5%85%A8%E5%90%AF%E5%8A%A8%E6%B5%85%E6%9E%90/","title":"【NVMEM子系统深入剖析】一、Efuse介绍及安全启动浅析","section":"Linux NVMEM 子系统","content":"【NVMEM子系统深入剖析】一、Efuse介绍及安全启动浅析 #  1、Efuse是什么 #  eFuse(electronic fuse)：电子保险丝，熔丝性的一种器件，属于一次性可编程存储器。\n之所以成为eFuse，因为其原理像电子保险丝一样，CPU出厂后，这片eFuse空间内所有比特全为1，如果向一位比特写入0，那么就彻底烧死这个比特了，再也无法改变它的值，也就是再也回不去 1 了。\n 一般OEM从CPU厂商购买芯片后，一般都要烧写eFuse，用于标识自己公司的版本信息，运行模式等相关信息。\n同时，由于其一次性编程的特性，我们又将其用在Secure Boot安全启动中。\n  2、OTP是什么 #   了解完eFuse后，我们就顺便了解一下OTP\n OTP(One Time Programmable)是反熔丝的一种器件，就是说，当OTP存储单元未击穿时，它的逻辑状态为0；当击穿时，它的逻辑状态为1，也属于一次性可编程存储器。\n它的物理状态和逻辑状态正好和eFuse相反！\n两者区别如下：\n  从成本上讲，eFuse器件基本上是各个Foundry厂自己提供，因此通常意味着免费或者很少的费用，而OTP器件则通常是第三方IP厂家提供，这就要收费。\n  从器件面积上讲，eFuse的cell的面积更大，所以仅仅有小容量的器件可以考虑。当然如果需要大容量的，也可以多个eFuse Macro拼接，但是这意味着芯片面积的增加，成本也会增加；OTP的cell面积很小，所有相对来讲，可以提供更大容量的Macro可供使用。\n  OTP 比 eFuse 安全性更好，eFuse的编程位可以通过电子显微镜看到，因此其存储的内容可以被轻易破解，但OTP在显微镜下无法区分编程位和未编程位，因此无法读取数据。\n  eFuse默认导通，存储的是\u0026quot;1\u0026quot;，而OTP默认是断开，存储的是\u0026quot;0\u0026quot;，因此OTP的功耗也较eFuse小，面积也较eFuse小。\n   3、什么是Secure Boot #   上面我们也了解过了，efuse主要用于记录一些OEM的产品信息，并且也会用于安全启动，那么安全启动是什么，为什么要做安全启动？\n 安全启动Secure Boot，其主要目的是：以限制消费者能力，防止消费者从软硬件层面，对产品的部分关键系统进行读写，调试等高级权限，达到对产品的商业保密，知识产权的保护。\n安全启动的安全模型是建立在消费者是攻击者的假设之上，一般常见的操作有：\n 刷机安装自定义的操作系统 绕过厂家封闭的支付平台 绕过系统保护，复制厂家保护的数字产品。  除此之外呢，有的比较专业的消费者，还可以：\n 使用数字示波器监听 CPU 和 RAM 、eMMC 之间的数据传输来读取非常底层的数据传输 而且像 eMMC 这种芯片通常都是业界标准化的，攻击者甚至可以把芯片拆下来，然后用市面上现成的通用 eMMC 编程工具来读写上面的内容。  安全启动等级也有一个上限：这个上限通常是认为攻击者不至于能够剥离芯片的封装，然后用电子显微镜等纳米级别精度的显像设备来逆向芯片的内部结构。\n简单来说：能成功攻破芯片安全机制的一次性投资成本至少需要在十万美元以上才可以认为是安全的。\n 4、CPU内部安全机制 #  4.1 bootROM #  BootROM是集成在CPU芯片的一个ROM空间，其主要用于存放一小段可执行程序，出厂的时候被烧录进去写死，不可修改。\nCPU在通电之后，执行的第一条程序就在BootROM，用于初始化Secure Boot安全机制，加载Secure Boot Key密钥，从 存储介质中加载并验证 First Stage Bootloader（FSBL）；最后跳转进 FSBL 中。\n 4.2 iRAM #  为了避免使用外部的RAM，支持Secure Boot的CPU都会内置一块很小的RAM，通常只有 16KB 到 64KB ，我们称之为 iRAM。\n这块 iRAM 上的空间非常宝贵，bootROM 一般会用 4KB 的 iRAM 作为它的堆栈。FSBL 也会被直接加载到 iRAM 上执行。\n 4.3 eFUSE #  如上面所述，在Secure Boot中存放的是根密钥，用于安全启动的验证。\n  一般有两种根密钥：一个是加密解密用的对称密钥 Secure Boot Key，一般是 AES 128 的，每台设备都是随机生成不一样的；\n  另一个是一个 Secure Boot Signing Key 公钥，一般用的 RSA 或 ECC，这个是每个 OEM 自己生成的，每台设备用的都一样，有些芯片会存公钥的 Hash 来减少 eFUSE 的空间使用。\n   4.5 Security Engine #  有些 CPU 中还会有一个专门负责加密解密的模块，我们称为 Security Engine。这个模块通常会有若干个密钥槽（Keyslots），可以通过寄存器将密钥加载到任意一个 Keyslot 当中，通过寄存器操作 DMA 读写，可以使用 Keyslot 中的密钥对数据进行加密、解密、签名、HMAC、随机数生成等操作.\n 4.6 First Stage Bootloader（FSBL） #  FSBL 的作用是初始化 PCB 板上的其他硬件设备，给外部 RAM 映射内存空间，从 外部存储介质中加载验证并执行接下来的启动程序。\n 4.7 根信任建立 #    CPU上电后执行Boot ROM的程序，其这一小段程序用于初始化RAM，并加载Efuse上的内容，判断其所处的运行模式是不是生产模式。\n  如果在生产模式，开启Secure Boot功能，把Efuse上保存的Secure Boot Key加载到Security Engine加密模块中处理。\n  从外部存储介质中加载FSBL，FSBL里面会有一个数字签名和公钥证书，bootROM 会验证这个签名的合法性，以及根证书的 Hash 是否和 eFUSE 中的 Signing Key 的 Hash 相同。\n  如果验证通过，说明 FSBL 的的确确是 OEM 正式发布的，没有受到过篡改。\n  然后bootROM 就会跳转到 FSBL 执行接下来的启动程序。\n   5、参考文章 #  [1]：https://zhuanlan.zhihu.com/p/540171344\n[2]：https://blog.csdn.net/phenixyf/article/details/125675637\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":2,"href":"/docs/linux/linux_memory_manage/%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%9A%84%E7%94%B1%E6%9D%A5%E5%8F%8A%E6%80%9D%E6%83%B3/","title":"一、内存管理的由来及思想","section":"Linux 内存管理","content":"Linux内存管理 | 一、内存管理的由来及思想 #  1、前言 #  《中庸》有：“九层之台，起于垒土” 之说，那么对于我们搞技术的人，同样如此！\n对于Linux内存管理，你可以说没有留意过，但是它存在于我们日常开发的方方面面，你所打开的文件，你所创建的变量，你所运行的程序，无不以此为基础，它可以说是操作系统的基石；只是底层被封装的太好了，以至于我们在做开发的过程中，不需要关心的太多，哪有什么岁月静好，只是有人在负重前行罢了。\n 虽然日常开发中涉及的比较少，但是作为一个合格的Linux开发者，搞懂内存管理，又显得至关重要，同时也会对嵌入式开发大有脾益，今天我们就来详细聊聊内存管理的那点事。\n 该方面的文章，网上也有很多写的非常不错，但是100个人有100种理解方式，并且不同的人，基础不同，理解能力也不同，所以我写这系列的文章，也更有了意义。\n 2、内存管理的由来 #   为什么要有这个概念呢？\n  首先，内存管理，管理的是个什么东西？  管理的其实是我们的物理内存，也就是我们的RAM空间，在电脑上，表现为我们安装的内存条，有的人装个4G的、8G的、甚至64G的，这些就是实打实的物理空间大小，也就是我们的实际的硬件资源。\n 为什么要进行管理？  做嵌入式的都知道，像我们刚开始玩的C51单片机、STM32单片机，我们将程序烧录到Flash中后，开机启动后，然后CPU会将Flash程序加载到RAM中，也就是我们的物理内存，随后我们的所有操作都是基于这一个物理内存所进行的。\n那么此时：\n 我们想再次运行一个一模一样的程序怎么办？ 即使运行了，那两个程序同时操作了同一个变量，值被错误修改了怎么办？  这些就是Linux内存管理要做的事情。\n  顺便介绍一下 我的圈子：高级工程师聚集地，期待大家的加入。\n 3、Linux内存管理思想 #  为了解决上面的一些问题，Linux采用虚拟内存管理技术。\n Linux操作系统抽象出来一个虚拟地址空间的概念，供上层用户使用，这么做的目的是为了让多个用户进程，都以为自己独享了内存空间。 而虚拟地址空间与物理地址空间的对应关系，就交给了一个MMU(Memory Managerment Unit)的家伙来管理，其主要负责将虚拟内存空间映射到真实的物理地址空间。  这么做的主要目的在于：\n 让每个进程都拥有相同大小的虚拟地址空间 避免用户直接访问物理内存，导致系统崩溃  这样，我们同时执行多个进程，虽然看起来虚拟地址操作都是相同的，但是通过MMU之后，就被映射到了不同的物理地址空间，这样就解决了以上的问题。\n 4、总结 #  熟悉了内存管理由来以及其思想，我们可以看出，操作系统的内存管理，主要分为以下几个方面：\n 虚拟内存空间管理：我们抽象出来的虚拟地址空间，该怎么使用，该怎么管理？ 物理内存空间管理：虚拟地址映射到物理内存空间后，该如何管理，如何分配？ 如何映射：虚拟内存如何映射到物理内存，是怎么操作的，映射方法有哪些？  下面我们来一一详细探究。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":3,"href":"/docs/linux/linux_api/","title":"Linux API 揭秘","section":"Linux开发","content":"Ubi loqui #  Mentem genus facietque salire tempus bracchia #  Lorem markdownum partu paterno Achillem. Habent amne generosi aderant ad pellem nec erat sustinet merces columque haec et, dixit minus nutrit accipiam subibis subdidit. Temeraria servatum agros qui sed fulva facta. Primum ultima, dedit, suo quisque linguae medentes fixo: tum petis.\nRapit vocant si hunc siste adspice #  Ora precari Patraeque Neptunia, dixit Danae Cithaeron armaque maxima in nati Coniugis templis fluidove. Effugit usus nec ingreditur agmen ac manus conlato. Nullis vagis nequiquam vultibus aliquos altera suum venis teneas fretum. Armos remotis hoc sine ferrea iuncta quam!\nLocus fuit caecis #  Nefas discordemque domino montes numen tum humili nexilibusque exit, Iove. Quae miror esse, scelerisque Melaneus viribus. Miseri laurus. Hoc est proposita me ante aliquid, aura inponere candidioribus quidque accendit bella, sumpta. Intravit quam erat figentem hunc, motus de fontes parvo tempestate.\niscsi_virus = pitch(json_in_on(eupViral), northbridge_services_troubleshooting, personal( firmware_rw.trash_rw_crm.device(interactive_gopher_personal, software, -1), megabit, ergonomicsSoftware(cmyk_usb_panel, mips_whitelist_duplex, cpa))); if (5) { managementNetwork += dma - boolean; kilohertz_token = 2; honeypot_affiliate_ergonomics = fiber; } mouseNorthbridge = byte(nybble_xmp_modem.horse_subnet( analogThroughputService * graphicPoint, drop(daw_bit, dnsIntranet), gateway_ospf), repository.domain_key.mouse(serverData(fileNetwork, trim_duplex_file), cellTapeDirect, token_tooltip_mashup( ripcordingMashup))); module_it = honeypot_driver(client_cold_dvr(593902, ripping_frequency) + coreLog.joystick(componentUdpLink), windows_expansion_touchscreen); bashGigabit.external.reality(2, server_hardware_codec.flops.ebookSampling( ciscNavigationBacklink, table + cleanDriver), indexProtocolIsp);  Placabilis coactis nega ingemuit ignoscat nimia non #  Frontis turba. Oculi gravis est Delphice; inque praedaque sanguine manu non.\nif (ad_api) { zif += usb.tiffAvatarRate(subnet, digital_rt) + exploitDrive; gigaflops(2 - bluetooth, edi_asp_memory.gopher(queryCursor, laptop), panel_point_firmware); spyware_bash.statePopApplet = express_netbios_digital( insertion_troubleshooting.brouter(recordFolderUs), 65); } recursionCoreRay = -5; if (hub == non) { portBoxVirus = soundWeb(recursive_card(rwTechnologyLeopard), font_radcab, guidCmsScalable + reciprocalMatrixPim); left.bug = screenshot; } else { tooltipOpacity = raw_process_permalink(webcamFontUser, -1); executable_router += tape; } if (tft) { bandwidthWeb *= social_page; } else { regular += 611883; thumbnail /= system_lag_keyboard; }  Caesorum illa tu sentit micat vestes papyriferi #  Inde aderam facti; Theseus vis de tauri illa peream. Oculos uberaque non regisque vobis cursuque, opus venit quam vulnera. Et maiora necemque, lege modo; gestanda nitidi, vero? Dum ne pectoraque testantur.\nVenasque repulsa Samos qui, exspectatum eram animosque hinc, aut manes, Assyrii. Cupiens auctoribus pariter rubet, profana magni super nocens. Vos ius sibilat inpar turba visae iusto! Sedes ante dum superest extrema.\n"},{"id":4,"href":"/docs/linux/linux_debug/%E4%B8%80%E6%96%87%E7%A7%92%E6%87%82ftrace%E7%B3%BB%E7%BB%9F%E8%B0%83%E8%AF%95%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E7%BB%88%E6%9E%81%E6%8C%87%E5%8D%97/","title":"【一文秒懂】Ftrace系统调试工具使用终极指南","section":"Linux 调试工具","content":"【一文秒懂】Ftrace系统调试工具使用终极指南 #  1、Ftrace是什么 #  Ftrace是Function Trace的简写，由 Steven Rostedt 开发的，从 2008 年发布的内核 2.6.27 中开始就内置了。\nFtrace是一个系统内部提供的追踪工具，旨在帮助内核设计和开发人员去追踪系统内部的函数调用流程。\n随着Ftrace的不断完善，除了追踪函数调用流程外，还可以用来调试和分析系统的延迟和性能问题，并发展成为一个追踪类调试工具的框架。\n除了Ftrace外，追踪类调试工具还包括：\n2、Ftrace的实现原理 #  为了帮助我们更好的使用Ftrace，我们有必要简单了解Ftrace的实现原理。\n2.1 Ftrace框架图 #  Ftrace的框架图如下：\n由框架图我们可以知道：\n ftrace包括多种类型的tracers，每个tracer完成不同的功能 将这些不同类型的tracers注册进入ftrace framework 各类tracers收集不同的信息，并放入到Ring buffer缓冲区以供调用。   2.2 Ftrace是如何记录信息的 #  Ftrace采用了静态插桩和动态插桩两种方式来实现。\n静态插桩：\n我们在Kernel中打开了CONFIG_FUNCTION_TRACER功能后，会增加一个-pg的一个编译选项，这个编译选项的作用就是为每个函数入口处，都会插入bl mcount跳转指令，使得每个函数运行时都会进入mcount函数。\n Ftrace一旦使能，对kernel中所有的函数插桩，这带来的性能开销是惊人的，有可能导致人们弃用Ftrace功能。\n 为了解决这个问题，开发者推出了Dynamic ftrace，以此来优化整体的性能。\n动态插桩：\n 这里的动态，是指的动态修改函数指令。\n  编译时，记录所有被添加跳转指令的函数，这里表示所有支持追踪的函数。 内核将所有跳转指令替换为nop指令，以实现非调试状态性能零损失。 根据 function tracer 设置，动态将被调试函数的nop指令，替换为跳转指令，以实现追踪。   总而言之，Ftrace记录数据可以总结为以下几个步骤：\n 打开编译选项-pg，为每个函数都增加跳转指令 记录这些可追踪的函数，并为了减少性能消耗，将跳转函数替换为nop指令 通过flag标志位来动态管理，将需要追踪的函数预留的nop指令替换回追踪指令，记录调试信息。   3、如何使用Ftrace #  3.1 配置详解 #  CONFIG_FTRACE=y # 启用了 Ftrace CONFIG_FUNCTION_TRACER=y\t# 启用函数级别的追踪器 CONFIG_HAVE_FUNCTION_GRAPH_TRACER=y\t# 表示内核支持图形显示 CONFIG_FUNCTION_GRAPH_TRACER=y\t# 以图形的方式显示函数追踪过程 CONFIG_STACK_TRACER=y\t# 启用堆栈追踪器，用于跟踪内核函数调用的堆栈信息。 CONFIG_DYNAMIC_FTRACE=y\t# 启用动态 Ftrace，允许在运行时启用和禁用 Ftrace 功能。 CONFIG_HAVE_FTRACE_NMI_ENTER=y\t# 表示内核支持非屏蔽中断（NMI）时进入 Ftrace 的功能 CONFIG_HAVE_FTRACE_MCOUNT_RECORD=y\t# 表示内核支持通过 mcount 记录函数调用关系。 CONFIG_FTRACE_NMI_ENTER=y # 表示内核支持通过 mcount 记录函数调用关系。  CONFIG_FTRACE_SYSCALLS=y\t# 系统调用的追踪 CONFIG_FTRACE_MCOUNT_RECORD=y\t# 启用 mcount 记录函数调用关系。 CONFIG_SCHED_TRACER=y\t# 支持调度追踪 CONFIG_FUNCTION_PROFILER=y\t# 启用函数分析器，主要用于记录函数的执行时间和调用次数 CONFIG_DEBUG_FS=y\t# 启用 Debug 文件系统支持  上面只是介绍了部分配置，更多详细配置可自行了解。\n并且上述配置不一定全部打开，勾选自己需要的即可，通常我们选择CONFIG_FUNCTION_TRACER和CONFIG_HAVE_FUNCTION_GRAPH_TRACER即可，然后编译烧录到开发板。\n  3.2 挂载debugfs文件系统 #  Ftrace是基于debugfs调试文件系统的，所以我们的第一步就是先挂载debugfs。\nmount -t debugfs none /sys/kernel/debug 此时我们能够在/sys/kernel/debug下看到内核支持的所有的调试信息了。\n# cd /sys/kernel/debug/ # ls asoc gpio regmap bdi ieee80211 sched_debug block memblock sched_features clk mmc0 sleep_time device_component mmc1 suspend_stats devices_deferred mtd tracing dma_buf opp ubi extfrag pinctrl ubifs fault_around_bytes pm_qos wakeup_sources  3.3 traceing目录介绍 #  在/sys/kernel/debug目录下，包含的是kernel所有的调试信息，本章只关注与tracing目录，下面挑选一些比较重要的属性文件来分析。\n  万变不离其宗，如此复杂的框架，设计人员已经提供了README文件，里面详解了各个属性文件的含义，我建议抛弃本文，看README吧:)\n 3.3.1 trace #  trace ：包含当前追踪的内容，以人类可读的格式展现，通过echo \u0026gt; trace来清除。\n 3.3.2 trace_pipe #  trace_pipe 和 trace 一样，都是记录当前的追踪内容，但它和 trace 不一样的是：\n 对 trace_pipe 的读操作将会阻塞，直到有新的追踪数据进来为止； 当前从trace_pipe 读取的内容将被消耗掉，再次读 trace_pipe 又会阻塞到新数据进来为止。   简单的来说，cat trace_pipe是堵塞读取，有数据就读，没数据就等待；而cat trace有没有数据都是直接返回的\n  3.3.3 tracing_on #  tracing_on：向 tracing_on 写入 1，启用追踪；向 tracing_on 写入 0，停止追踪。\n 追踪使用 ring buffer 记录追踪数据。修改 tracing_on 不会影响 ring buffer 当前记录的内容。\n  3.3.4 current_tracer #  current_tracer 表示当前启用的 tracer ，默认为 nop ，即不做任何追踪工作：\n# cat current_tracer nop  3.3.5 available_filter_functions #  available_filter_functions：可以被追踪的函数列表，即可以写到 set_ftrace_filter，set_ftrace_notrace，set_graph_function，set_graph_notrace 文件的函数列表。\n 3.3.6 available_tracers #  available_tracers 文件中包含的是当前编译到内核的 tracer 列表，也表示当前内核支持的tracer列表。\n该列表的内容，就是可以写到 current_tracer 的 tracer 名。\n# cat available_tracers function_graph function nop  nop：表示为空，不追踪 function：追踪函数调用 function_graph：以图形形式追踪函数调用   3.3.7 buffer_size_kb #  buffer_size_kb 记录 CPU buffer 的大小，单位为 KB 。\nper_cpu/cpuX/buffer_size_kb 记录 每个CPU buffer 大小，单位为 KB 。可通过写 buffer_size_kb 来改变 CPU buffer 的大小。\n 3.3.8 buffer_total_size_kb #  buffer_total_size_kb 记录所有 CPU buffer 的总大小，即所有 CPU buffer 大小总和。\n 如有 128 个 CPU buffer ，每个大小 7KB，则 buffer_total_size_kb 记录的总大小为 128 * 7KB = 896。\n buffer_total_size_kb 文件是只读的。\n 3.3.9 set_ftrace_filter #  set_ftrace_filter ：过滤函数追踪，仅仅追踪写入该文件的函数名。\n可填入的参数，可以通过available_filter_functions文件查看当前支持的函数名。\n该过滤功能，也有很多其他变体，如追踪某个模块的函数调用等。\n 官方给的示例：\n Format: :mod:\u0026lt;module-name\u0026gt; example: echo :mod:ext3 \u0026gt; set_ftrace_filter\t# 该模块必须是已经加载进去的模块  3.3.10 set_ftrace_notrace #  set_ftrace_notrace：和 set_ftrace_filter 刚好相反，系统禁用对其中列举函数的追踪。\n 3.3.11 set_ftrace_pid #  系统对 set_ftrace_pid 文件中指定的 PID进程进行追踪。\n如果开启了 options/function-fork 选项，fork 的子进程的 PID 也会自动加入文件，同时该选项也会引起系统自动将退出进程的 PID 从文件中移除。\n 3.3.12 set_graph_function #  此文件中列出的函数将导致函数图跟踪器仅跟踪这些函数以及它们调用的函数。\n但是该跟踪的记录，仍然受set_ftrace_filter 和 set_ftrace_notrace 的影响。\n 3.3.12 set_graph_notrace #  与 set_graph_function 类似，但当函数被命中时，将禁用函数图跟踪，直到退出函数。\n 3.4 简单使用示例 #   一般我们挂载上debugfs后，tracing_on是处于打开状态的。\n 3.4.1 函数追踪 #   3.4.2 追踪图形显示 #   3.4.3 动态过滤追踪 #   3.4.4 重置追踪 #  echo 0 \u0026gt; tracing_on\t# 关闭trace echo \u0026gt; trace\t# 清空当前trace记录 cat available_tracers # 查看当前支持的追踪类型 echo function_graph \u0026gt; current_tracer # 设置当前的追踪类型 echo 1 \u0026gt; tracing_on\t# 开启追踪 cat trace\t# 查看追踪结果  4、进阶用法 #  上述章节，只是介绍了Ftrace最基本的命令，下面来看一下Ftrace在具体问题中的用法！\n4.1 追踪任意命令 #   如何追踪我们执行的命令呢？\n Ftrace支持追踪特定进程，通过set_ftrace_pid属性来设置指定进程。然后在该进程中，执行特定的命令。\n首先我们需要设置好我们的追踪器\nmount -t debugfs none /sys/kernel/debug cd /sys/kernel/debug/tracing echo 0 \u0026gt; tracing_on\t# 关闭追踪器 echo function \u0026gt; current_tracer\t# 设置当前追踪类别 在我们设置好追踪器后，使用如下命令，即可追踪我们执行的命令your_command\necho \u0026gt; trace; echo $$ \u0026gt; set_ftrace_pid; echo 1 \u0026gt; tracing_on; your_command; echo 0 \u0026gt; tracing_on  4.2 追踪指定函数的调用流程 #  跟踪函数的时候，设置 echo 1 \u0026gt; options/func_stack_trace 即可在 trace 结果中获取追踪函数的调用栈。\nmount -t debugfs none /sys/kernel/debug cd /sys/kernel/debug/tracing echo 0 \u0026gt; tracing_on\t# 关闭追踪器 cat available_filter_functions | grep \u0026#34;xxxxxx\u0026#34;\t# 搜索函数是否存在 echo xxxxxx \u0026gt; set_ftrace_filter\t# 设定追踪的函数 echo function \u0026gt; current_tracer\t# 设置当前追踪类别 echo 1 \u0026gt; options/func_stack_trace\t# 记录堆栈信息 echo \u0026gt; trace\t# 清空缓存 echo 1 \u0026gt; tracing_on\t# 开始追踪 效果如下：\n# cat trace # tracer: function # # entries-in-buffer/entries-written: 2/2 #P:3 # # _-----=\u0026gt; irqs-off # / _----=\u0026gt; need-resched # | / _---=\u0026gt; hardirq/softirq # || / _--=\u0026gt; preempt-depth # ||| / delay # TASK-PID CPU# |||| TIMESTAMP FUNCTION # | | | |||| | | kworker/1:1-59 [001] .... 168.954199: mmc_rescan \u0026lt;-process_one_work kworker/1:1-59 [001] .... 168.954248: \u0026lt;stack trace\u0026gt; =\u0026gt; mmc_rescan =\u0026gt; process_one_work =\u0026gt; worker_thread =\u0026gt; kthread =\u0026gt; ret_from_fork =\u0026gt; 0  4.3 追踪指定模块的所有函数 #  要想我们的ko文件能够被Ftrace记录到，我们需要在编译模块的时候，加上编译参数-pg，这点很重要，否则你在available_filter_functions列表中，查找不到你想要的函数。\n然后，需要我们设置过滤器，设置方法有以下几种：\n 按模块直接过滤：  # 示例 Format: :mod:\u0026lt;module-name\u0026gt; example: echo :mod:ext3 \u0026gt; set_ftrace_filter  追踪ext3模块内的所有函数\n   按函数直接过滤   如果该模块内的函数，命名都有一定的规则，可以按照正则表达式来过滤\n # 示例 echo \u0026#34;mmc*\u0026#34; \u0026gt; set_ftrace_filter  过滤包含mmc字符的所有函数\n   按照函数差异来过滤  如果函数命名没有规律，又想过滤该模块所有函数，该怎么办？\n按照加载模块前后的函数差异，写入到文件中来过滤\ncat available_filter_functions \u0026gt; /tmp/1.txt cat available_filter_functions \u0026gt; /tmp/2.txt diff /tmp/1.txt /tmp/2.txt \u0026gt; /tmp/3.txt cat /tmp/3.txt | sed \u0026#39;s/^+//\u0026#39; | awk \u0026#39;{print $1}\u0026#39;\t# 如果diff出来格式前带有+-号，需要手动去掉 cat /tmp/3.txt \u0026gt; set_ftrace_filter  5、自动化管理 #  Ftrace功能很强大，在内核层面我们通过echo和cat即可获取我们想要的所有信息，但是通过一次一次敲命令显得有些繁琐，自己也对常用的功能整合了一个自动化脚本，能够通过命令行，直接追踪特定模块、函数、命令，极大提高了调试效率。\n自动化脚本获取路径：common_trace.sh\n# /root/common_trace.sh  Usage: /root/common_trace.sh {module|funcs|funcs_stack|command|clear} /root/common_trace.sh module ext4 /root/common_trace.sh funcs sysfs /root/common_trace.sh funcs_stack sysfs /root/common_trace.sh command sysfs [functions] /root/common_trace.sh clear 脚本主要实现的功能有：\n 追踪指定模块，查看所有调用流程 追踪指定函数，查看该函数的调用链 追踪指定函数，获取堆栈信息 追踪用户命令，查看所有调用流程，并可选择指定函数来查看调用流程。   脚本除了command功能外，其他功能都需要手动调用common_trace.sh clear来停止追踪。\n  6、总结 #  以上，介绍了Ftrace的由来，实现原理，以及如何使用Ftrace，并最终提供了自动化测试脚本，希望对大家有所帮助。\n 欢迎关注【嵌入式艺术】，董哥原创！\r\r"},{"id":5,"href":"/docs/linux/linux_api/linux-api-%E6%8F%AD%E7%A7%98module_init%E4%B8%8Emodule_exit/","title":"【Linux API 揭秘】module_init与module_exit","section":"Linux API 揭秘","content":" Linux Version：6.6\nAuthor：Donge\nGithub：linux-api-insides\n  1、函数作用 #  module_init和module_exit是驱动中最常用的两个接口，主要用来注册、注销设备驱动程序。\n并且这两个接口的实现机制是一样的，我们先以module_init为切入点分析。\n 2、module_init函数解析 #  2.1 module_init #  #ifndef MODULE /** * module_init() - driver initialization entry point * @x: function to be run at kernel boot time or module insertion * * module_init() will either be called during do_initcalls() (if * builtin) or at module insertion time (if a module). There can only * be one per module. */ #define module_init(x)\t__initcall(x);  ...... #else /* MODULE */ ...... /* Each module must use one module_init(). */ #define module_init(initfn)\t\\ static inline initcall_t __maybe_unused __inittest(void)\t\\ { return initfn; }\t\\ int init_module(void) __copy(initfn)\t\\ __attribute__((alias(#initfn)));\t\\ ___ADDRESSABLE(init_module, __initdata);  ...... #endif 函数名称：module_init\n文件位置：include/linux/module.h\n函数解析：\n 在Linux内核中，驱动程序可以以两种方式存在：内建(Builtin)和模块(Module)。内建驱动就是在编译时，直接编译进内核镜像中；而模块驱动则是在内核运行过程中动态加载卸载的。\n module_init函数的定义位置有两处，使用MODULE宏作为判断依据。MODULE是一个预处理器宏，仅当该驱动作为模块驱动时，编译的时候会加入MODULE的定义。\n 这里难免会有疑问：为什么会有两套实现呢？\n 其实，当模块被编译进内核时，代码是存放在内存的.init字段，该字段在内核代码初始化后，就会被释放掉了，所以当可动态加载模块需要加载时，就需要重新定义了。\n 2.1.1 模块方式 #  当驱动作为可加载模块时，MODULE宏被定义，我们简单分析一下相关代码\n#define module_init(initfn)\t\\ static inline initcall_t __maybe_unused __inittest(void)\t\\ { return initfn; }\t\\ int init_module(void) __copy(initfn)\t\\ __attribute__((alias(#initfn)));\t\\ ___ADDRESSABLE(init_module, __initdata);  static inline initcall_t __maybe_unused __inittest(void) { return initfn; }：一个内联函数，返回传入的initfn函数。  __maybe_unused ：编译器指令，用于告诉编译器，该函数可能不会使用，以避免编译器产生警告信息。   int init_module(void) __copy(initfn) __attribute__((alias(#initfn)));：init_module函数的声明  __copy(initfn)：编译器指令，也就是将我们的initfn函数代码复制到init_module中， __attribute__((alias(#initfn)))：编译器指令，将init_module函数符号的别名设置为initfn。   ___ADDRESSABLE(init_module, __initdata);：一个宏定义，主要用于将init_module函数的地址放入__initdata段，这样，当模块被加载时，init_module函数的地址就可以被找到并调用。  总的来说，如果是可加载的ko模块，module_init宏主要定义了init_module函数，并且将该函数与initfn函数关联起来，使得当模块被加载时，初始化函数可以被正确地调用。\n 2.1.2 内建方式 #  当模块编译进内核时，MODULE宏未被定义，所以走下面流程\n#define module_init(x)\t__initcall(x);  2.2 __initcall #  #define __initcall(fn) device_initcall(fn)  #define device_initcall(fn)\t__define_initcall(fn, 6)  #define __define_initcall(fn, id) ___define_initcall(fn, id, .initcall##id)  #define ___define_initcall(fn, id, __sec)\t\\ __unique_initcall(fn, id, __sec, __initcall_id(fn))  #define __unique_initcall(fn, id, __sec, __iid)\t\\ ____define_initcall(fn,\t\\ __initcall_stub(fn, __iid, id),\t\\ __initcall_name(initcall, __iid, id),\t\\ __initcall_section(__sec, __iid))  #define ____define_initcall(fn, __unused, __name, __sec)\t\\ static initcall_t __name __used \\ __attribute__((__section__(__sec))) = fn;  #define __initcall_stub(fn, __iid, id)\tfn  /* Format: \u0026lt;modname\u0026gt;__\u0026lt;counter\u0026gt;_\u0026lt;line\u0026gt;_\u0026lt;fn\u0026gt; */ #define __initcall_id(fn)\t\\ __PASTE(__KBUILD_MODNAME,\t\\ __PASTE(__,\t\\ __PASTE(__COUNTER__,\t\\ __PASTE(_,\t\\ __PASTE(__LINE__,\t\\ __PASTE(_, fn))))))  /* Format: __\u0026lt;prefix\u0026gt;__\u0026lt;iid\u0026gt;\u0026lt;id\u0026gt; */ #define __initcall_name(prefix, __iid, id)\t\\ __PASTE(__,\t\\ __PASTE(prefix,\t\\ __PASTE(__,\t\\ __PASTE(__iid, id))))  #define __initcall_section(__sec, __iid)\t\\ #__sec \u0026#34;.init\u0026#34;  /* Indirect macros required for expanded argument pasting, eg. __LINE__. */ #define ___PASTE(a,b) a##b #define __PASTE(a,b) ___PASTE(a,b) 函数名称：__initcall\n文件位置：include/linux/init.h\n函数解析：设备驱动初始化函数\n 2.2.1 代码调用流程 #  module_init(fn) |--\u0026gt; __initcall(fn) |--\u0026gt; device_initcall(fn) |--\u0026gt; __define_initcall(fn, 6) |--\u0026gt; ___define_initcall(fn, id, __sec) |--\u0026gt; __initcall_id(fn) |--\u0026gt; __unique_initcall(fn, id, __sec, __iid) |--\u0026gt; ____define_initcall(fn, __unused, __name, __sec) |--\u0026gt; __initcall_stub(fn, __iid, id) |--\u0026gt; __initcall_name(prefix, __iid, id) |--\u0026gt; __initcall_section(__sec, __iid) |--\u0026gt; ____define_initcall(fn, __unused, __name, __sec)   进行函数分析前，我们先要明白#和##的概念\n 2.2.2 #和##的作用 #     符号 作用 举例     ## ##符号 可以是连接的意思 例如 __initcall_##fn##id 为__initcall_fnid那么，fn = test_init，id = 6时，__initcall##fn##id 为 __initcall_test_init6   # #符号 可以是字符串化的意思 例如 #id 为 \u0026quot;id\u0026quot;，id=6 时，#id 为\u0026quot;6\u0026quot;      更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  2.2.3 函数解析 #   下面分析理解比较有难度的函数\n #define device_initcall(fn)\t__define_initcall(fn, 6) #define __define_initcall(fn, id) ___define_initcall(fn, id, .initcall##id)  .initcall##id：通过##来拼接两个字符串：.initcall6  #define ___define_initcall(fn, id, __sec)\t\\ __unique_initcall(fn, id, __sec, __initcall_id(fn))  /* Format: \u0026lt;modname\u0026gt;__\u0026lt;counter\u0026gt;_\u0026lt;line\u0026gt;_\u0026lt;fn\u0026gt; */ #define __initcall_id(fn)\t\\ __PASTE(__KBUILD_MODNAME,\t\\ __PASTE(__,\t\\ __PASTE(__COUNTER__,\t\\ __PASTE(_,\t\\ __PASTE(__LINE__,\t\\ __PASTE(_, fn))))))  /* Indirect macros required for expanded argument pasting, eg. __LINE__. */ #define ___PASTE(a,b) a##b #define __PASTE(a,b) ___PASTE(a,b)  ___PASTE：拼接两个字符串 __initcall_id：它用于生成一个唯一的标识符，这个标识符用于标记初始化函数。  __KBUILD_MODNAME：当前正在编译的模块的名称 __COUNTER__：一个每次使用都会递增计数器，用于确保生成名称的唯一性 __LINE__：当前代码的行号     #define __unique_initcall(fn, id, __sec, __iid)\t\\ ____define_initcall(fn,\t\\ __initcall_stub(fn, __iid, id),\t\\ __initcall_name(initcall, __iid, id),\t\\ __initcall_section(__sec, __iid))  #define ____define_initcall(fn, __unused, __name, __sec)\t\\ static initcall_t __name __used \\ __attribute__((__section__(__sec))) = fn;  #define __initcall_stub(fn, __iid, id)\tfn  /* Format: __\u0026lt;prefix\u0026gt;__\u0026lt;iid\u0026gt;\u0026lt;id\u0026gt; */ #define __initcall_name(prefix, __iid, id)\t\\ __PASTE(__,\t\\ __PASTE(prefix,\t\\ __PASTE(__,\t\\ __PASTE(__iid, id))))  #define __initcall_section(__sec, __iid)\t\\ #__sec \u0026#34;.init\u0026#34; __unique_initcall：调用____define_initcall，关键实现部分\n____define_initcall：定义一个名为 __name 的 initcall_t 类型的静态变量，并将其初始化为 fn，并放入特定的__sec段中。\n __initcall_stub：表示唯一的函数名fn __initcall_name：表示一个唯一的变量名 __initcall_section： 生成一个唯一的段名。 #__sec \u0026quot;.init\u0026quot;：将两个字符串拼接起来，比如：__sec=.initcall6，拼接后的段为：.initcall6.init，该段为最终存储的段。   字段通过链接器链接起来，形成一个列表进行统一管理。\n 这些字段我们可以在arch/arm/kernel/vmlinux.lds中查看。\n ...... __initcall6_start = .; KEEP(*(.initcall6.init)) KEEP(*(.initcall6s.init)) ......  3、module_exit函数解析 #   module_exit和module_init的实现机制几乎没有差别，下面就简单介绍一下。\n 3.1 module_exit #  #ifndef MODULE  /** * module_exit() - driver exit entry point * @x: function to be run when driver is removed * * module_exit() will wrap the driver clean-up code * with cleanup_module() when used with rmmod when * the driver is a module. If the driver is statically * compiled into the kernel, module_exit() has no effect. * There can only be one per module. */ #define module_exit(x)\t__exitcall(x);  ...... #else /* MODULE */ ...... /* This is only required if you want to be unloadable. */ #define module_exit(exitfn)\t\\ static inline exitcall_t __maybe_unused __exittest(void)\t\\ { return exitfn; }\t\\ void cleanup_module(void) __copy(exitfn)\t\\ __attribute__((alias(#exitfn)));\t\\ ___ADDRESSABLE(cleanup_module, __exitdata);  ...... #endif 函数名称：module_exit\n文件位置：include/linux/module.h\n3.1.1 模块方式 #  作为模块方式，与module_init的实现方式一样，定义cleanup_module与exitfn函数相关联，存放在__exitdata段内。\n 3.1.2 内建方式 #  当模块编译进内核时，MODULE宏未被定义，所以走下面流程\n#define module_exit(x)\t__exitcall(x);  3.2 __exitcall #  #define __exitcall(fn)\t\\ static exitcall_t __exitcall_##fn __exit_call = fn  #define __exit_call\t__used __section(\u0026#34;.exitcall.exit\u0026#34;) 函数名称：__initcall\n文件位置：include/linux/init.h\n函数解析：设备驱动卸载函数\n__exitcall_##fn：定义一个新的 exitcall_t 类型的静态变量，并赋值为fn\n__exit_call：__used __section(\u0026quot;.exitcall.exit\u0026quot;)，定义该函数存储的段\n 4、扩展 #   还记得__define_initcall的定义吗？\n #define pure_initcall(fn) __define_initcall(fn, 0)  #define core_initcall(fn) __define_initcall(fn, 1) #define core_initcall_sync(fn) __define_initcall(fn, 1s) #define postcore_initcall(fn) __define_initcall(fn, 2) #define postcore_initcall_sync(fn) __define_initcall(fn, 2s) #define arch_initcall(fn) __define_initcall(fn, 3) #define arch_initcall_sync(fn) __define_initcall(fn, 3s) #define subsys_initcall(fn) __define_initcall(fn, 4) #define subsys_initcall_sync(fn) __define_initcall(fn, 4s) #define fs_initcall(fn) __define_initcall(fn, 5) #define fs_initcall_sync(fn) __define_initcall(fn, 5s) #define rootfs_initcall(fn) __define_initcall(fn, rootfs) #define device_initcall(fn) __define_initcall(fn, 6) #define device_initcall_sync(fn) __define_initcall(fn, 6s) #define late_initcall(fn) __define_initcall(fn, 7) #define late_initcall_sync(fn) __define_initcall(fn, 7s)  #define __initcall(fn) device_initcall(fn) 不同的宏定义，被赋予了不同的调用等级，最后将不同的驱动初始化函数统一汇总到__initcallx_start字段统一管理，形成一个有序的列表。\n这样，我们在内核中，按照顺序遍历这个列表，最后执行对应的模块初始化函数fn即可实现驱动的初始化。\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":6,"href":"/docs/uboot/%E4%B8%80uboot%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3/","title":"一、uboot基础了解","section":"Uboot开发","content":"一、uboot基础了解 #  1. U-boot是什么 #  U-Boot，全称 Universal Boot Loader，是遵循GPL条款的从FADSROM、8xxROM、PPCBOOT逐步发展演化而来的 开放源码项目。\nU-boot，是一个主要用于嵌入式系统的引导加载程序，可以支持多种不同的计算机系统结构，其主要作用为：==引导系统的启动！==目前，U-Boot不仅支持Linux系统的引导，还支持NetBSD, VxWorks, QNX, RTEMS, ARTOS, LynxOS, android等多种嵌入式操作系统。\n2. U-boot主要特性及功能 #   开放：开放的源代码 多平台：支持多种嵌入式操作系统，如Linux、NetBSD、android等 生态：有丰富的设备驱动源码，如以太网、SDRAM、LCD等，同时也具有丰富的开发文档。  3. U-boot下载地址 #  Uboot开发源码：\n  https://source.denx.de/u-boot/u-boot\n  https://ftp.denx.de/pub/u-boot/\n  其他厂商定制的uboot源码：\n 野火  4. U-boot目录结构 #     目录 含义     arch 各个厂商的硬件信息，目录下包括支持的处理器类型   arch/arm/cpu/xxx **每一个子文件夹，包含一种cpu系列。**每个子文件夹下包含cpu.c（CPU初始化），interrupts.c（设置中断和异常），start.S（U-boot的启动文件，早期的初始化）。   board 与开发板有关，每一个子文件夹代表一个芯片厂家，芯片厂家下，每一个子文件夹，表示一个开发板   common 存放与处理器体系无关的通用代码，可以说为通用核心代码！   cmd 存放uboot的相关命令实现部分   drivers 存放外围芯片驱动，网卡，USB等   disk 存放驱动磁盘的分区处理代码   fs 本目录下存放文件系统相关代码，每一个子文件夹表示文件系统   net 网络协议相关代码   doc uboot说明文档   include 各种头文件   post 上电自检代码   api 外部扩展程序的API和示例   tools 编译S-Record或者U-boot镜像的相关工具    5. 如何编译Uboot #  make ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- distclean make ARCH=arm CORSS_COMPILE=arm-linux-gnueabihf- colibri-imx6ull_defconfig make V=1 ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- -j8 ARCH=arm：arm架构\nCROSS_COMPILE：使用的交叉编译器\n 如果编译出错，your compile older 6.0，可以参考【1】\n colibri-imx6ull_defconfig：指定一个config文件，作为相关版型的配置信息\nV=1：这个选项能显示出编译过程中的详细信息，即是verbose编译模式\n-j8：多核并行编译，可以提高编译速度，受硬件限制\n6. U-boot工作模式 #   U-boot的工作模式有：启动加载模式和下载模式\n  启动加载模式：  启动加载模式，为Bootloader正常工作模式，一款开发板，正常上电后，Bootloader将嵌入式操作系统==从FLASH中加载到SDRAM中==运行。\n 下载模式：  下载模式，就是Bootloader通过通信，将内核镜像、根文件系统镜像从PC机直接下载到目标板的FLASH中。\n7. U-boot的存放位置 #  嵌入式系统，一般使用Flash来作为启动设备，Flash上存储着U-boot、环境变量、内核映像、文件系统等。U-boot存放于Flash的起始地址，所在扇区由Soc规定。\n8. U-boot系列文章汇总 #   下面是进行U-boot开发期间，感觉比较不错的资料，总结分享一下！\n [1] : Uboot官网、Uboot官方指南、官方指南2\n[2] : https://blog.51cto.com/u_9291927/category5\n[3] : https://blog.csdn.net/ooonebook/category_6484145.html\n[4]：https://blog.csdn.net/qq_36310253/category_9332618.html\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":7,"href":"/docs/embeded_tech/embeded_interview/soc%E7%9A%84bringup%E6%B5%81%E7%A8%8B/","title":"Soc的Bring Up流程","section":"嵌入式面经","content":"1、Bring Up流程 #  SOC (System on a Chip) bring-up是一个复杂的过程，涉及到硬件、固件和软件的集成和验证，以下是一个基于BROM，SPL，UBOOT和Linux的启动流程的概述：\n BROM (Boot Read-Only Memory)启动：启动的最初阶段，在这个阶段，系统会执行芯片ROM里面的代码，这部分代码主要用来检查启动模式，包括NOR、Nand、Emmc等，然后从对应的存储介质中加载SPL(Secondary Program Loader)代码。 SPL (Secondary Program Loader)启动：SPL属于Uboot的一部分，它的主要作用就是：初始化硬件并加载完整的U-boot，主要体现在初始化时钟、看门狗、DDR、GPIO以及存储外设，最后将U-boot代码加载到DDR中执行。 U-Boot启动：U-boot的主要作用是：引导加载Kernel和DTS。U-boot在启动之后，同样初始化Soc硬件资源，然后会计时等待，并执行默认的启动命令，将Kernel和DTS信息从存储介质中读取出来并加载到内存中执行。 Kernel启动：在U-Boot加载了内核映像和设备树之后，系统会启动Linux。在这个阶段，系统会初始化各种硬件设备，加载驱动程序并启动用户空间应用程序。   更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  2、常见问题 #  Q：为什么上一个阶段已经初始化了硬件资源，下一个阶段为何重复初始化？\nA：\n  每个阶段的硬件初始化，其目标和需求都不同，硬件配置也会不一样，因此在不同阶段进行不同的初始化。\n  硬件状态可能会改变，在SOC启动过程中，硬件状态可能会因为电源管理、时钟管理等原因而改变，这可能需要在每个阶段都重新初始化以确保其正确工作\n  为了保证硬件资源的可靠性，最好每个阶段都重新初始化一次\n   Q：U-boot加载内核时，会进行重定位的操作，这一操作有何意义？\nA：\n U-boot的重定位，主要作用是为了 给内核提供一个连续的、大的内存空间，供内核和其他应用程序使用 U-boot的加载过程分两个阶段，即：SPL和U-boot，   在SPL阶段，主要将U-boot代码从Flash中加载到RAM指定位置 在U-boot阶段，U-boot会将自身从RAM的开始部分移动到RAM的末尾，占用高地址空间，从而让低地址空间可以作为一个连续的，大的内存空间供内核和其他应用程序使用。   Q：在Bring Up中，为了保证启动时间，如何裁剪？\nA：\n 启动时间的裁剪是一个重要的步骤，其主要目标是缩短从电源打开到操作系统完全启动的时间。\n  优化Bootloader：减小Bootloader的代码大小，减少硬件初始化（只初始化必要硬件设备）等 优化Kernel：减少启动服务数量，优化服务的启动顺序，使用预加载技术等方法来实现。 使用快速启动模式：一些SOC支持快速启动模式，这种模式下，SOC会跳过一些不必要的硬件初始化和自检过程，从而更快地启动。 使用休眠和唤醒技术：一些SOC还支持休眠和唤醒技术，这种技术可以将系统的状态保存到非易失性存储器中，然后关闭系统。当系统再次启动时，可以直接从非易失性存储器中恢复系统的状态，从而更快地启动。  \r\u0026nbsp;\r"},{"id":8,"href":"/docs/embeded_tech/self_improve/","title":"嵌入式工程师养成记","section":"嵌入式","content":"Ubi loqui #  Mentem genus facietque salire tempus bracchia #  Lorem markdownum partu paterno Achillem. Habent amne generosi aderant ad pellem nec erat sustinet merces columque haec et, dixit minus nutrit accipiam subibis subdidit. Temeraria servatum agros qui sed fulva facta. Primum ultima, dedit, suo quisque linguae medentes fixo: tum petis.\nRapit vocant si hunc siste adspice #  Ora precari Patraeque Neptunia, dixit Danae Cithaeron armaque maxima in nati Coniugis templis fluidove. Effugit usus nec ingreditur agmen ac manus conlato. Nullis vagis nequiquam vultibus aliquos altera suum venis teneas fretum. Armos remotis hoc sine ferrea iuncta quam!\nLocus fuit caecis #  Nefas discordemque domino montes numen tum humili nexilibusque exit, Iove. Quae miror esse, scelerisque Melaneus viribus. Miseri laurus. Hoc est proposita me ante aliquid, aura inponere candidioribus quidque accendit bella, sumpta. Intravit quam erat figentem hunc, motus de fontes parvo tempestate.\niscsi_virus = pitch(json_in_on(eupViral), northbridge_services_troubleshooting, personal( firmware_rw.trash_rw_crm.device(interactive_gopher_personal, software, -1), megabit, ergonomicsSoftware(cmyk_usb_panel, mips_whitelist_duplex, cpa))); if (5) { managementNetwork += dma - boolean; kilohertz_token = 2; honeypot_affiliate_ergonomics = fiber; } mouseNorthbridge = byte(nybble_xmp_modem.horse_subnet( analogThroughputService * graphicPoint, drop(daw_bit, dnsIntranet), gateway_ospf), repository.domain_key.mouse(serverData(fileNetwork, trim_duplex_file), cellTapeDirect, token_tooltip_mashup( ripcordingMashup))); module_it = honeypot_driver(client_cold_dvr(593902, ripping_frequency) + coreLog.joystick(componentUdpLink), windows_expansion_touchscreen); bashGigabit.external.reality(2, server_hardware_codec.flops.ebookSampling( ciscNavigationBacklink, table + cleanDriver), indexProtocolIsp);  Placabilis coactis nega ingemuit ignoscat nimia non #  Frontis turba. Oculi gravis est Delphice; inque praedaque sanguine manu non.\nif (ad_api) { zif += usb.tiffAvatarRate(subnet, digital_rt) + exploitDrive; gigaflops(2 - bluetooth, edi_asp_memory.gopher(queryCursor, laptop), panel_point_firmware); spyware_bash.statePopApplet = express_netbios_digital( insertion_troubleshooting.brouter(recordFolderUs), 65); } recursionCoreRay = -5; if (hub == non) { portBoxVirus = soundWeb(recursive_card(rwTechnologyLeopard), font_radcab, guidCmsScalable + reciprocalMatrixPim); left.bug = screenshot; } else { tooltipOpacity = raw_process_permalink(webcamFontUser, -1); executable_router += tape; } if (tft) { bandwidthWeb *= social_page; } else { regular += 611883; thumbnail /= system_lag_keyboard; }  Caesorum illa tu sentit micat vestes papyriferi #  Inde aderam facti; Theseus vis de tauri illa peream. Oculos uberaque non regisque vobis cursuque, opus venit quam vulnera. Et maiora necemque, lege modo; gestanda nitidi, vero? Dum ne pectoraque testantur.\nVenasque repulsa Samos qui, exspectatum eram animosque hinc, aut manes, Assyrii. Cupiens auctoribus pariter rubet, profana magni super nocens. Vos ius sibilat inpar turba visae iusto! Sedes ante dum superest extrema.\n"},{"id":9,"href":"/docs/linux/linux_kernel_lock/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3linux%E9%94%81%E6%9C%BA%E5%88%B6%E4%BA%8C%E4%B8%AD%E6%96%AD%E5%B1%8F%E8%94%BD/","title":"【深入理解Linux锁机制】二、中断屏蔽","section":"Linux 内核锁详解","content":"【深入理解Linux内核锁】二、中断屏蔽 #   上一篇了解了内核锁的由来，本篇文章主要来讲一下中断屏蔽的底层实现以及原理。\n  1、中断屏蔽思想 #  中断屏蔽，正如其名，屏蔽掉CPU的中断响应功能，解决并发引起的竞态问题。\n 在进入临界区前屏蔽中断，这么做有什么好处，以及有什么弊端？\n 好处在于：\n 解决了进程与中断之间的并发：保证在执行临界区代码时，不被中断所打断。 解决了进程与进程之间调度的并发：系统的进程调度与中断息息相关，同时也限制了系统进程的并发，解决了系统进程并发带来的竞态问题。  弊端在于：\n 各类中断类型较多，一棒子打死影响大：Linux内核中，除了系统进程调度依赖中断，还有一些异步I/O等众多操作都依赖中断，因此长时间屏蔽中断是很危险的，会对系统造成严重影响，因此也要求临界区代码要简短。 解决的不够完善：关闭中断能够解决进程调度、中断引发的竞态，但是这些都是单CPU内部的，对于SMP对称多处理器，仍然不可避免的会收到其他CPU的中断。因此，并不能解决SMP多CPU引发的竞态。  因此，单独使用中断屏蔽通常不是一种值得推荐的避免竞态的方法\n 2、Linux内核中断屏蔽的实现 #  2.1 Linux内核提供的API接口 #   关于中断屏蔽，Linux内核所提供的接口如下：\n local_irq_enable()\t//\t使能本CPU的中断 local_irq_disable()\t//\t禁止本CPU的中断 local_irq_save(flags)\t//\t禁止本CPU的中断，并保存CPU中断位的信息 local_irq_restore(flags)\t//\t使能本CPU的中断，并恢复CPU中断位的信息 local_bh_disable(void)\t//\t禁止本CPU底半部中断 local_bh_enable(void) //\t使能本CPU底半部中断 文件位置：kernel/include/linux/irqflags.h\n local_irq_enable与local_irq_disable：直接打开/关闭本CPU内的中断，包括了顶半部和底半部中断的打开和关闭。 local_irq_save与local_irq_restore：直接打开/关闭本CPU中断，并且保存中断屏蔽前的状态，便于后续恢复 local_bh_enable与local_bh_disable：直接打开/关闭本CPU内的底半部中断   2.2 API接口实现分析 #   因为中断屏蔽与底层芯片架构有关，不同架构处理方式不同，我们以ARM为例\n 2.2.1 local_irq_enable #  #define local_irq_enable()\tdo { raw_local_irq_enable(); } while (0)  #define raw_local_irq_enable()\tarch_local_irq_enable()  #define arch_local_irq_enable arch_local_irq_enable static inline void arch_local_irq_enable(void) { asm volatile( \u0026#34;\tcpsie i\t@ arch_local_irq_enable\u0026#34; : : : \u0026#34;memory\u0026#34;, \u0026#34;cc\u0026#34;); } 函数介绍：local_irq_enable函数用于将CPSR寄存器中的中断使能位设为1，从而使得CPU能够响应中断。\n文件位置：kernel/arch/arm/include/asm/irqflags.h\n相关实现：\nasm：声明一个内联汇编表达式\ncpsie i：全称Change Processor State, Interrupts Enabled，主要用来设置CPSR寄存器的I位，来允许本CPU响应中断。\nmemory：向汇编说明，此处内存发生了更改，类似于内存屏障的作用\ncc：表示可能会修改条件码的标志\n 汇编语言的格式，大家可以自行简单了解\n  2.2.2 local_irq_disable #  #define local_irq_disable() \\ do { raw_local_irq_disable(); trace_hardirqs_off(); } while (0)  #define raw_local_irq_disable()\tarch_local_irq_disable()  #define arch_local_irq_disable arch_local_irq_disable static inline void arch_local_irq_disable(void) { asm volatile( \u0026#34;\tcpsid i\t@ arch_local_irq_disable\u0026#34; : : : \u0026#34;memory\u0026#34;, \u0026#34;cc\u0026#34;); } 函数介绍：local_irq_disable函数用于将CPSR寄存器中的中断使能位设为0，从而禁止CPU响应中断。\n文件位置：kernel/arch/arm/include/asm/irqflags.h\n相关实现：同上\n cpsid：全称Change Processor State, Interrupts Disabled，用于清除CPSR寄存器的中断标志，以禁止中断！   这里顺便提及一下，CPSR寄存器为Current Program Status Register，用于存储当前程序的状态信息，包括中断使能状态、处理器模式、条件标志等。\n大多数的ARM处理器，都采用CPSR寄存器来管理装填信息，所以ARM处理器可以直接进行操作。\n 2.2.3 local_irq_save #  #define IRQMASK_REG_NAME_R \u0026#34;primask\u0026#34;  #define local_irq_save(flags)\t\\ do {\t\\ raw_local_irq_save(flags);\t\\ trace_hardirqs_off();\t\\ } while (0)  #define raw_local_irq_save(flags)\t\\ do {\t\\ typecheck(unsigned long, flags);\t\\ flags = arch_local_irq_save();\t\\ } while (0)  static inline unsigned long arch_local_irq_save(void) { unsigned long flags; asm volatile( \u0026#34;\tmrs\t%0, \u0026#34; IRQMASK_REG_NAME_R \u0026#34;\t@ arch_local_irq_save\\n\u0026#34; \u0026#34;\tcpsid\ti\u0026#34; : \u0026#34;=r\u0026#34; (flags) : : \u0026#34;memory\u0026#34;, \u0026#34;cc\u0026#34;); return flags; } 函数介绍：arch_local_irq_save函数，用于保存当前中断状态并禁用中断。\n文件位置：kernel/arch/arm/include/asm/irqflags.h\n相关实现：\nmrs %0 IRQMASK_REG_NAME_R：这行代码使用mrs指令将中断屏蔽寄存器的值读取到通用寄存器%0中。IRQMASK_REG_NAME_R是一个占位符，表示要读取的中断屏蔽寄存器的名称，实际的中断屏蔽寄存器为primask。通过这行代码，中断屏蔽寄存器的值被保存到了%0寄存器中。\n: \u0026quot;=r\u0026quot; (flags) : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;: 这是一个约束部分，用于指定寄存器和内存的使用约束。\u0026quot;=r\u0026quot; (flags)表示将%0寄存器的值保存到flags变量中。\u0026quot;memory\u0026quot;和\u0026quot;cc\u0026quot;表示这段代码可能会修改内存和条件码寄存器。\n总的来说，这段代码主要实现了：\n 保存中断屏蔽寄存器的值到flags变量中，并返回 关闭本CPU的中断   2.2.4 local_irq_restore #  #define IRQMASK_REG_NAME_W \u0026#34;primask\u0026#34;  #define local_irq_restore(flags)\t\\ do {\t\\ if (raw_irqs_disabled_flags(flags)) {\t\\ raw_local_irq_restore(flags);\t\\ trace_hardirqs_off();\t\\ } else {\t\\ trace_hardirqs_on();\t\\ raw_local_irq_restore(flags);\t\\ }\t\\ } while (0)  #define raw_local_irq_restore(flags)\t\\ do {\t\\ typecheck(unsigned long, flags);\t\\ arch_local_irq_restore(flags);\t\\ } while (0)  /* * restore saved IRQ \u0026amp; FIQ state */ static inline void arch_local_irq_restore(unsigned long flags) { asm volatile( \u0026#34;\tmsr\t\u0026#34; IRQMASK_REG_NAME_W \u0026#34;, %0\t@ local_irq_restore\u0026#34; : : \u0026#34;r\u0026#34; (flags) : \u0026#34;memory\u0026#34;, \u0026#34;cc\u0026#34;); } 函数介绍：arch_local_irq_restore函数，用于恢复当前中断状态并打开中断。\n相关实现：同上\n 2.2.5 local_bh_enable #  static inline void local_bh_enable(void) { __local_bh_enable_ip(_THIS_IP_, SOFTIRQ_DISABLE_OFFSET); } void __local_bh_enable_ip(unsigned long ip, unsigned int cnt) { WARN_ON_ONCE(in_irq()); lockdep_assert_irqs_enabled(); #ifdef CONFIG_TRACE_IRQFLAGS  local_irq_disable(); #endif  /* * Are softirqs going to be turned on now: */ if (softirq_count() == SOFTIRQ_DISABLE_OFFSET) trace_softirqs_on(ip); /* * Keep preemption disabled until we are done with * softirq processing: */ preempt_count_sub(cnt - 1); if (unlikely(!in_interrupt() \u0026amp;\u0026amp; local_softirq_pending())) { /* * Run softirq if any pending. And do it in its own stack * as we may be calling this deep in a task call stack already. */ do_softirq(); } preempt_count_dec(); #ifdef CONFIG_TRACE_IRQFLAGS  local_irq_enable(); #endif  preempt_check_resched(); } EXPORT_SYMBOL(__local_bh_enable_ip); asmlinkage __visible void do_softirq(void) { __u32 pending; unsigned long flags; if (in_interrupt()) return; local_irq_save(flags); pending = local_softirq_pending(); if (pending \u0026amp;\u0026amp; !ksoftirqd_running(pending)) do_softirq_own_stack(); local_irq_restore(flags); } 函数介绍：local_bh_enable函数，启用本地的底半部bottom half处理，当中断来临时，底半部处理可以被执行。\n文件位置：kernel/include/linux/bottom_half.h\n相关实现：\n 调用__local_bh_enable_ip传入两个参数，这两个参数的作用是：  _THIS_IP_：是一个宏定义，用于获取当前的指令地址，也就是调用 local_bh_enable 函数的地方。 SOFTIRQ_DISABLE_OFFSET：是一个常量，用于指定软中断禁用的偏移量。   __local_bh_enable_ip其主要作用是：处理完软中断softirq后重新启用本地底半部bottom half处理，并检查是否需要进行进程调度。 WARN_ON_ONCE(in_irq())：判断是否处于硬件中断上下文中，如果是，则打印警告信息 lockdep_assert_irqs_enabled()：这是一个锁依赖性检查宏，用于确保在调用此函数时中断是被启用的。 if (softirq_count() == SOFTIRQ_DISABLE_OFFSET) lockdep_softirqs_on(ip)：如果当前软中断计数等于SOFTIRQ_DISABLE_OFFSET，则启用软中断。 __preempt_count_sub(cnt - 1)：减少抢占计数，这是为了防止在处理软中断时发生抢占。 do_softirq：这里表示如果有待处理的软中断，那么就调用do_softirq()函数来处理这些软中断。  in_interrupt()：判断是否处于硬中断中，如果是，则直接返回 local_irq_save：它保存并关闭本地中断，以防止在处理软中断时被其他硬中断打断。 local_softirq_pending：它获取当前待处理的软中断。 如果存在待处理的软中断，并且软中断处理线程（ksoftirqd）没有在运行，那么就在当前的栈上处理软中断。 local_irq_restore：恢复本地中断。      这里有一个疑问，大家不妨思考一下：\n中断上半部和下半部的机制，就是为了让那些紧急处理的事情放在下半部，不那么紧急或者时间较长的任务放到下半部处理，来保证系统的实时性。\n那么在这里，使能中断底半部之后，仍然执行了local_irq_save和local_irq_restore，来关闭本地硬中断，这么做是为了什么？\n 我的猜想：local_bh_disable和local_bh_enable是成对出现的，当我们关闭掉了底半部中断时，也有可能硬中断引发了多个软中断触发，在此打开的时候，此时已经就已经挂起了其他的软中断处理程序；\n如果不关闭硬中断，这时候就有可能发生嵌套，导致堆栈溢出。\n 大家不妨可以一起讨论下。\n  2.2.6 local_bh_disable #  static inline void local_bh_disable(void) { __local_bh_disable_ip(_THIS_IP_, SOFTIRQ_DISABLE_OFFSET); } static __always_inline void __local_bh_disable_ip(unsigned long ip, unsigned int cnt) { preempt_count_add(cnt); barrier(); } 函数介绍：local_bh_disable函数，增加当前进程的抢占计数，从而阻止内核抢占当前进程。\n文件位置：kernel/include/linux/bottom_half.h\n相关实现：\n preempt_count_add：增加当前进程的抢占计数 barrier：执行内存屏障，以确保抢占计数的增加在所有其他内存操作之前完成。   2.2.7 抢占计数机制 #  在local_bh_eable和local_bh_disable中，有一些抢占计数的操作，如：preempt_count_add、preempt_count_dec、preempt_count_dec等，这些作用是什么呢？\n抢占计数（preempt_count）在Linux内核中起着非常重要的作用。它主要用于防止内核抢占。\n 在Linux内核中，当一个进程正在执行内核代码时，如果发生了中断或者有更高优先级的进程需要运行，那么当前进程可能会被抢占，即暂停当前进程，转而去执行中断处理程序或者更高优先级的进程。这种机制被称为内核抢占。\n然而，有些情况下，我们不希望当前进程被抢占。例如，当一个进程正在修改一些全局数据结构时，如果这个进程被抢占，那么其他进程可能会看到这些数据结构处于不一致的状态。为了防止这种情况发生，我们可以通过增加抢占计数来禁止内核抢占。\n 当抢占计数大于0时，内核抢占被禁止。\n当抢占计数等于0时，内核抢占被允许。\n因此，我们可以通过调用preempt_count_add函数来增加抢占计数，从而禁止内核抢占，通过调用preempt_count_dec和preempt_count_dec函数来减少抢占计数，从而允许内核抢占。\n总的来说，抢占计数的作用就是用于控制内核抢占的开启和关闭，以保证内核代码的正确执行。\n  关于中断底半部机制，内容较为复杂，放在后面单独拆解！\n  3、总结 #  该篇文章，主要了解以下几点：\n 中断屏蔽的思想 中断屏蔽的好处与不足 Linux内核提供的中断屏蔽接口 中断屏蔽的底层操作的实现方式    欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":10,"href":"/docs/linux/linux_nvmem_subsystem/nvmem%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E4%BA%8Cnvmem%E9%A9%B1%E5%8A%A8%E6%A1%86%E6%9E%B6/","title":"【NVMEM子系统深入剖析】二、NVMEM驱动框架","section":"Linux NVMEM 子系统","content":"【NVMEM子系统深入剖析】二、NVMEM驱动框架 #  1、前言 #   NVMEM SUBSYSTEM，该子系统整体架构不算太大，还是比较容易去理解的，下面我们一起去一探究竟！\n NVMEM（Non Volatile Memory），该子系统主要用于实现EEPROM、Efuse等非易失存储器的统一管理。\n在早期，像EEPROM驱动是存放于/drivers/misc目录下，由于没有做到好的抽象，每次需要去访问相应内存空间，都需要去复制几乎一样的代码，去注册sysfs，这是一个相当大的抽象泄露。\nNVMEM子系统就是为了解决以往的抽象泄露问题。\n 2、驱动框架 #   该驱动框架较为简单，也适合初学者去熟悉基本的驱动框架。\n 应用层：可以通过用户空间所提供的文件节点，来读取或者修改nvmem存储器的数据。\nNVMEM 核心层：统一管理NVMEM设备，向上实现文件系统接口数据的传递，向下提供统一的注册，注销nvmem设备接口。\nNVMEM 总线驱动：注册NVMEM总线，实现NVMEM控制器的底层代码实现。\nTIP：\nnvmem子系统提供读写存储器的接口有两种，一种是通过文件系统读写，一种是在内核驱动直接读写。\n对于EEPROM，其可以进行读写操作，而对于efuse，更多用于读取密钥信息，进而判断镜像是否被篡改，在用户空间是不允许被更改的。\n这种是通过驱动提供的开放接口，直接获取指定位置的数据，详细的后面展开来说。\n 3、源码目录结构 #  ketnel │ └── driver │ │ └── nvmem │ │ │ ├──\tcore.c\t# NVMEM核心层 │ │ │ ├──\trockchip-efuse.c\t# NVMEM总线驱动  4、用户空间下的目录结构 #  我们可以在用户空间去读取/写入数据，其所在的目录：/sys/bus/nvmem/devices/dev-name/nvmem\nhexdump /sys/bus/nvmem/devices/qfprom0/nvmem 0000000 0000 0000 0000 0000 0000 0000 0000 0000 * 00000a0 db10 2240 0000 e000 0c00 0c00 0000 0c00 0000000 0000 0000 0000 0000 0000 0000 0000 0000 ... * 0001000  5、参考文章 #  [1]：https://blog.csdn.net/qq_33160790/article/details/87836614\n[2]：https://blog.csdn.net/tiantao2012/article/details/72284862\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":11,"href":"/docs/embeded_tech/embeded_interview/","title":"嵌入式面经","section":"嵌入式","content":"Ubi loqui #  Mentem genus facietque salire tempus bracchia #  Lorem markdownum partu paterno Achillem. Habent amne generosi aderant ad pellem nec erat sustinet merces columque haec et, dixit minus nutrit accipiam subibis subdidit. Temeraria servatum agros qui sed fulva facta. Primum ultima, dedit, suo quisque linguae medentes fixo: tum petis.\nRapit vocant si hunc siste adspice #  Ora precari Patraeque Neptunia, dixit Danae Cithaeron armaque maxima in nati Coniugis templis fluidove. Effugit usus nec ingreditur agmen ac manus conlato. Nullis vagis nequiquam vultibus aliquos altera suum venis teneas fretum. Armos remotis hoc sine ferrea iuncta quam!\nLocus fuit caecis #  Nefas discordemque domino montes numen tum humili nexilibusque exit, Iove. Quae miror esse, scelerisque Melaneus viribus. Miseri laurus. Hoc est proposita me ante aliquid, aura inponere candidioribus quidque accendit bella, sumpta. Intravit quam erat figentem hunc, motus de fontes parvo tempestate.\niscsi_virus = pitch(json_in_on(eupViral), northbridge_services_troubleshooting, personal( firmware_rw.trash_rw_crm.device(interactive_gopher_personal, software, -1), megabit, ergonomicsSoftware(cmyk_usb_panel, mips_whitelist_duplex, cpa))); if (5) { managementNetwork += dma - boolean; kilohertz_token = 2; honeypot_affiliate_ergonomics = fiber; } mouseNorthbridge = byte(nybble_xmp_modem.horse_subnet( analogThroughputService * graphicPoint, drop(daw_bit, dnsIntranet), gateway_ospf), repository.domain_key.mouse(serverData(fileNetwork, trim_duplex_file), cellTapeDirect, token_tooltip_mashup( ripcordingMashup))); module_it = honeypot_driver(client_cold_dvr(593902, ripping_frequency) + coreLog.joystick(componentUdpLink), windows_expansion_touchscreen); bashGigabit.external.reality(2, server_hardware_codec.flops.ebookSampling( ciscNavigationBacklink, table + cleanDriver), indexProtocolIsp);  Placabilis coactis nega ingemuit ignoscat nimia non #  Frontis turba. Oculi gravis est Delphice; inque praedaque sanguine manu non.\nif (ad_api) { zif += usb.tiffAvatarRate(subnet, digital_rt) + exploitDrive; gigaflops(2 - bluetooth, edi_asp_memory.gopher(queryCursor, laptop), panel_point_firmware); spyware_bash.statePopApplet = express_netbios_digital( insertion_troubleshooting.brouter(recordFolderUs), 65); } recursionCoreRay = -5; if (hub == non) { portBoxVirus = soundWeb(recursive_card(rwTechnologyLeopard), font_radcab, guidCmsScalable + reciprocalMatrixPim); left.bug = screenshot; } else { tooltipOpacity = raw_process_permalink(webcamFontUser, -1); executable_router += tape; } if (tft) { bandwidthWeb *= social_page; } else { regular += 611883; thumbnail /= system_lag_keyboard; }  Caesorum illa tu sentit micat vestes papyriferi #  Inde aderam facti; Theseus vis de tauri illa peream. Oculos uberaque non regisque vobis cursuque, opus venit quam vulnera. Et maiora necemque, lege modo; gestanda nitidi, vero? Dum ne pectoraque testantur.\nVenasque repulsa Samos qui, exspectatum eram animosque hinc, aut manes, Assyrii. Cupiens auctoribus pariter rubet, profana magni super nocens. Vos ius sibilat inpar turba visae iusto! Sedes ante dum superest extrema.\n"},{"id":12,"href":"/docs/embeded_tech/embeded_interview/cpu%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/","title":"CPU体系架构","section":"嵌入式面经","content":"CPU体系架构 #  2.1 CPU体系架构有哪些？ #   我们常见的CPU架构有哪些呢？\n 如果我们熟悉Linux，那么这个问题肯定不难回答！\n我们查看内核目录下的arch子目录，就可以看到Linux所支持的处理器架构，基本属于我们常见的类型了。\n# ls ./arch alpha arc arm arm64 c6x h8300 hexagon ia64 Kconfig m68k microblaze mips nds32 nios2 openrisc parisc powerpc riscv s390 sh sparc um unicore32 x86 xtensa  准确来说，CPU处理器架构主要有以下几种类型：\n CISC（复杂指令集计算机）：CISC架构的CPU设计理念是尽可能减少程序指令的数量，以降低CPU和内存之间的通信频率。这种架构的一个显著特点是拥有大量的寄存器和复杂的指令集。Intel的x86架构就是一个典型的CISC架构 RISC（精简指令集计算机）：RISC架构的CPU设计理念是通过简化指令集来提高CPU的运行效率。这种架构的一个显著特点是拥有较少的寄存器和简单的指令集。ARM架构就是一个典型的RISC架构 MISC（中间指令集计算机）：MISC架构的CPU设计理念是在CISC和RISC之间寻找一个平衡点，既不过于复杂也不过于简单。这种架构的一个显著特点是指令集的复杂度介于CISC和RISC之间 VLIW（超长指令字计算机）：VLIW架构的CPU设计理念是通过增大指令长度来提高并行执行的可能性。这种架构的一个显著特点是指令长度远大于其他架构的CPU EPIC（显式并行指令计算）：EPIC架构的CPU设计理念是通过显式标记并行指令来提高CPU的运行效率。这种架构的一个显著特点是指令集中包含了并行执行的信息。Intel的Itanium架构就是一个典型的EPIC架构 超标量架构：超标量架构的CPU设计理念是通过在一个时钟周期内执行多条指令来提高CPU的运行效率。这种架构的一个显著特点是CPU内部包含了多个执行单元，可以同时执行多条指令 超线程技术：超线程技术是Intel公司为其部分CPU所采用的一种使单一处理器像多个逻辑处理器那样并行处理多个线程的技术 多核心架构：多核心架构的CPU设计理念是在一个CPU芯片内集成多个处理器核心，以提高并行处理能力。这种架构的一个显著特点是CPU内部包含了多个独立的处理器核心，每个核心可以独立执行指令   这里就有一个疑问，我们什么时候说RISC架构，什么时候说ARM架构，这两个有什么区别呢？\n 以ARM和RISC为例：\n ARM架构和RISC架构的主要区别在于ARM实际上是RISC的一个具体实现，而RISC则是一个更广泛的处理器设计理念。换句话说，ARM是RISC的一个子集。\n同理，X86架构是CISC的一个子集。\n 2.2 常见的问题 #  Q1：你所熟知的处理器架构有哪些？\n我们常见的处理器架构有ARM、X86、mips架构等；\n Q2：STM32属于什么架构的？\nSTM32是ST公司开发的32位微控制器集成电路，基于 ARM 的 Cortex-M 系列内核。因此，STM32 属于 ARM 架构的微控制器。\n Q3：RISC和CISC的区别是什么？\n RISC：精简指令集架构，通过简化指令集，使得大多数的操作都能够在一个指令周期内完成，提高CPU运行效率 CISC：复杂指令集架构，指令集丰富，能够完成一些较为复杂的任务，并且可以降低CPU和内存之间的通信频率，提高性能。    欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":13,"href":"/docs/linux/linux_debug/%E4%B8%80%E6%96%87%E7%A7%92%E6%87%82top%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/","title":"【一文秒懂】TOP命令详解","section":"Linux 调试工具","content":"【一文秒懂】TOP命令详解 #  1、Top命令介绍 #  Linux系统中，Top命令主要用于实时运行系统的监控，包括Linux内核管理的进程或者线程的资源占用情况。\n这个命令对所有正在运行的进程和系统负荷提供不断更新的概览信息，包括系统负载、CPU利用分布情况、内存使用、每个进程的内容使用情况等信息。\n 2、Top命令使用 #  Top的命令介绍如下：\ntop -hv|-bcHiOSs -d secs -n max -u|U user -p pid -o fld -w [cols] 常用的Top指令有：\ntop：启动top命令 top -c：显示完整的命令行 top -b：以批处理模式显示程序信息 top -S：以累积模式显示程序信息 top -n 2：表示更新两次后终止更新显示 top -d 3：设置信息更新周期为3秒 top -p 139：显示进程号为139的进程信息，CPU、内存占用率等 top -n 10：显示更新十次后退出 除此之外，在top进程运行过程中，两个最重要的功能是查看帮助（h 或 ？）和退出（q 或 Ctrl+C）。\n 3、Top信息详解 #  top展示界面由从上到下3部分组成\n 概览区域 表头 任务区域 还有一个输入/消息行，位于概览区域和表头之间。  3.1 概览区详解 #  top - 14:46:08 up 5:46, 1 user, load average: 0.00, 0.00, 0.00  程序或者窗口的名称：top 当前时间和系统的启动时间：14:46:08 up 5:46 总共的用户数量：1 user 过去1、5和15分钟的系统平均负载：load average: 0.00, 0.00, 0.00  Tasks: 290 total, 1 running, 212 sleeping, 0 stopped, 0 zombie %Cpu(s): 0.0 us, 0.1 sy, 0.0 ni, 99.9 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st 这两行显示了任务数量和CPU状态\n 第一行该信息对Task进行分类，包括running、sleeping、stopped、zombie四类，显示了系统中正在运行的任务的状态统计信息。具体来说，这里有291个任务总数，其中有1个任务正在运行，212个任务正在睡眠，0个任务已停止，0个任务为僵尸进程。 第二行显示CPU的状态百分比  %Cpu(s): CPU使用率的统计信息。 us (user): 用户空间进程占用CPU的时间百分比。 sy (system): 内核空间进程占用CPU的时间百分比。 ni (nice): 用户进程以优先级调整过的占用CPU的时间百分比（通常不会有这个值）。 id (idle): CPU空闲的时间百分比。 wa (IO-wait): CPU等待I/O操作的时间百分比。 hi (hardware interrupt): CPU处理硬件中断的时间百分比。 si (software interrupt): CPU处理软件中断的时间百分比。 st: 被虚拟化环境偷取的时间百分比（通常不会有这个值）。    KiB Mem : 3994720 total, 525876 free, 595492 used, 2873352 buff/cache KiB Swap: 2097148 total, 2096624 free, 524 used. 3114400 avail Mem 这两行表示内存的使用情况\n 第一行表示物理内存，分为total、 free、 used 、 buff/cache 第二行表示虚拟内存，分为total、free、used、avail   默认单位是KiB，使用按键E可以切换为MiB、GiB、TiB、PiB、EiB\n KiB = kibibyte = 1024 bytes MiB = mebibyte = 1024 KiB = 1,048,576 bytes GiB = gibibyte = 1024 MiB = 1,073,741,824 bytes TiB = tebibyte = 1024 GiB = 1,099,511,627,776 bytes PiB = pebibyte = 1024 TiB = 1,125,899,906,842,624 bytes EiB = exbibyte = 1024 PiB = 1,152,921,504,606,846,976 bytes   更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  3.2 任务区 #  任务区是按照列的形式来显示的，并且有多个字段可以用来查看进程的状态信息。\n3.2.1 任务字段介绍 #    %CPU： CPU Usage，自上次屏幕更新以来任务占用的CPU时间份额，表示为总CPU时间的百分比。\n  %MEM： Memory Usage，进程使用的物理内存百分比\n  CODE：Code Size，可执行代码占用的物理内存量\n  COMMAND：Command Name or Command Line，用于显示输入的命令行或者程序名称\n  PID：Process Id，任务独立的ID，即进程ID\n  PPID：Parent Process Id，父进程ID\n  UID：User Id，任务所有者的用户ID\n  USER：User Name，用户名\n  RUSER：Real User Name，实际的用户名\n  TTY：Controlling Tty，控制终端名称\n  TIME：CPU TIME，该任务CPU总共运行的时间\n  TIME+：同TIME，其粒度更细\n  OOMa：Out of Memory Adjustment Factor，内存溢出调整机制，这个字段会被增加到当前内存溢出分数中，来决定什么任务会被杀掉，范围是-1000到+1000。\n  OOMs：Out of Memory Score，内存溢出分数，这个字段是用来选择当内存耗尽时杀掉的任务，范围是0到+1000。0的意思是绝不杀掉，1000的意思是总是杀掉。\n  S：Process Status，表示进程状态信息\n D： 不可中断休眠 I：空闲 R：运行中 S：休眠 T：被任务控制信号停止 t：在跟踪期间被调试器停止 Z：僵尸     相关属性有很多，可以使用man top查看，这里先列举这些。\n  3.2.2 字段管理 #  我们输入top后，默认只显示一部分属性信息，我们可以自行管理想要的属性信息。\n我们输入F或者f，进入字段管理功能，用于选择想要的字段信息\n   按键 功能     ↑、↓ 光标上下移动选择   空格、d 切换   s 设置为排序依据字段   a、w 在4种窗口中切换：1.默认，2.任务，3.内存，4.用户   Esc键、q 退出当前窗口     4、交互命令详解 #  top的功能很多，基本能够查看进程的各种状态信息，其中还有一些交互式的命令，方便我们更好的查看系统状态。\n 在top主界面中，我们输入下面的命令\n    命令 功能     h、? 帮助信息查看，涵盖所有的快捷键   空格、回车按键 手动刷新界面信息   q、ESC按键 退出   B 粗体显示功能   d、s 改变间隔时间   E、e 切换内存显示的单位，从KiB到EiB   g 然后输入1-4其中一个数字，选择哪种窗口（1.默认，2.任务，3.内存，4.用户）   H 进程、线程显示切换   k 输入PID信息，杀掉一个任务   Z 改变配色     上面介绍了一些比较常见的交互式命令，还有更多需要你去探索哦！\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":14,"href":"/docs/linux/linux_memory_manage/%E4%BA%8C%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E5%B8%83%E5%B1%80/","title":"二、虚拟地址空间布局","section":"Linux 内存管理","content":"Linux内存管理 | 二、虚拟地址空间布局 #  上一章，我们了解了内存管理的由来以及核心思想，下面我们按照顺序，先来介绍一下Linux虚拟内存空间的管理。\n 同样，我们知道Linux内核抽象出来虚拟内存空间，主要是为了让每个进程都独享该空间，那虚拟内存空间是如何布局的呢？\n 前提：针对于不同位数的CPU，寻址能力不同，抽象出来的虚拟内存空间大小也不同，我们以常见的32位的CPU为例。\n  1、虚拟内存空间布局 #  对于32位的CPU，寻址范围为0~2^32，也就是0x00000000-0xFFFFFFFF，即最多抽象出来4G的虚拟内存空间。\n这4GB的内存空间，在Linux中，又分为用户空间和内核空间，其中0x0000000-0xBFFFFFFF，共3G为用户空间，0xC00000000-0xFFFFFFFF，共1G为内核空间，如下：\n无论内核空间还是用户空间，其仍然是在虚拟内存空间基础之上进行划分的，其直接访问的依旧都是虚拟地址，而非物理地址！\n我们编写代码后，所生成的可执行程序，运行之后就成为一个系统进程，我们在\u0026quot;虚\u0026quot;的角度来看，每个进程都是独享这4G虚拟地址空间的，\n 2、用户态空间布局 #  如上所述，用户空间在虚拟内存中分布在0x0000000-0xBFFFFFFF，大小为3G。\n每一个用户进程，按照访问属性一致的地址空间存放在一起的原则，划分成5个不同的内存区域（访问属性一致指的是：可读，可写，可执行）：\n 代码段：Text Segment，也就是我们的二进制程序，代码段需要防止在运行时被非法修改，所以该段为只读。 数据段：Data Segment，主要存放初始化了的变量，主要包括：静态变量和全局变量，该段为读写。 BSS段：BSS Segment，主要存放未初始化的全局变量，在内存中 bss 段全部置零，该段为读写。 堆段：Heap Segment，主要存放进程运行过程中动态分配的内存段，大小不固定，可动态扩张和缩减，通常使用malloc和free来分配释放，并且堆的增长方向是向上的。 文件映射和匿名映射段：Memory Mapping Segment，主要存放进程使用到的文件或者依赖的动态库，从低地址向上增长。 栈段：Stack Segment，主要存放进程临时创建的局部变量，函数调用上下文信息等，栈向下增长。  一个可执行程序，可以通过size命令，查看编译出来的可执行文件大小，其中包括了代码段，数据段等数据信息，如下:\ndonge@Donge:$ size Donge-Demo text data bss dec hex filename 12538 1916 43632 58086 e2e6 Donge-Demo  text：代码段大小 data：数据段大小 bss：bss段大小 dec：十进制表示的可执行文件大小 hex：十六进制表示的可执行文件大小   运行该程序后，可以通过cat /proc/PID/maps命令，或者pmap PID命令，来查看该进程在虚拟内存空间中的分配情况，其中PID为进程的PID号，如下:\ndonge@Donge:$ cat /proc/16508/maps 55976ff9e000-55976ffa0000 r--p 00000000 08:10 184922 /home/donge/WorkSpace/Program/Donge_Programs/Donge_Demo/build/Donge-Demo 55976ffa0000-55976ffa2000 r-xp 00002000 08:10 184922 /home/donge/WorkSpace/Program/Donge_Programs/Donge_Demo/build/Donge-Demo 55976ffa2000-55976ffa3000 r--p 00004000 08:10 184922 /home/donge/WorkSpace/Program/Donge_Programs/Donge_Demo/build/Donge-Demo 55976ffa3000-55976ffa4000 r--p 00004000 08:10 184922 /home/donge/WorkSpace/Program/Donge_Programs/Donge_Demo/build/Donge-Demo 55976ffa4000-55976ffa5000 rw-p 00005000 08:10 184922 /home/donge/WorkSpace/Program/Donge_Programs/Donge_Demo/build/Donge-Demo 55976ffa5000-55976ffaf000 rw-p 00000000 00:00 0 559771d91000-559771db2000 rw-p 00000000 00:00 0 [heap] 7fec1ad84000-7fec1ad87000 rw-p 00000000 00:00 0 7fec1ad87000-7fec1adaf000 r--p 00000000 08:10 22282 /usr/lib/x86_64-linux-gnu/libc.so.6 7fec1adaf000-7fec1af44000 r-xp 00028000 08:10 22282 /usr/lib/x86_64-linux-gnu/libc.so.6 7fec1af44000-7fec1af9c000 r--p 001bd000 08:10 22282 /usr/lib/x86_64-linux-gnu/libc.so.6 7fec1af9c000-7fec1afa0000 r--p 00214000 08:10 22282 /usr/lib/x86_64-linux-gnu/libc.so.6 7fec1afa0000-7fec1afa2000 rw-p 00218000 08:10 22282 /usr/lib/x86_64-linux-gnu/libc.so.6 7fec1afa2000-7fec1afaf000 rw-p 00000000 00:00 0 7fec1afb5000-7fec1afb7000 rw-p 00000000 00:00 0 7fec1afb7000-7fec1afb9000 r--p 00000000 08:10 22068 /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 7fec1afb9000-7fec1afe3000 r-xp 00002000 08:10 22068 /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 7fec1afe3000-7fec1afee000 r--p 0002c000 08:10 22068 /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 7fec1afef000-7fec1aff1000 r--p 00037000 08:10 22068 /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 7fec1aff1000-7fec1aff3000 rw-p 00039000 08:10 22068 /usr/lib/x86_64-linux-gnu/ld-linux-x86-64.so.2 7ffce385d000-7ffce387e000 rw-p 00000000 00:00 0 [stack] 7ffce394e000-7ffce3952000 r--p 00000000 00:00 0 [vvar] 7ffce3952000-7ffce3953000 r-xp 00000000 00:00 0 [vdso] 上面能大致看出该进程的代码段、堆、文件映射段，栈的内存分布等情况，以上就是我们的可执行程序被加载进入内存之后，在用户态虚拟内存空间的布局情况。\n  顺便介绍一下 我的圈子：高级工程师聚集地，期待大家的加入。\n 3、内核态空间布局 #  下面我们来看一下内核态的虚拟空间布局，首先我们要知道：\n 在Linux系统中，用户进程通常只能访问用户空间的虚拟地址，只有在执行内陷操作或系统调用时才能访问内核空间。 所有的进程通过系统调用进入内核态之后，看到的虚拟地址空间都是一样的，他们是共享内核态虚拟内存空间的。   32位的内核态虚拟空间在虚拟内存中分布在0xC00000000-0xFFFFFFFF上，大小为1G，其要分为以下几个区：\n 直接映射区（Direct Memory Region）：顾名思义，直接映射区就是直接与物理内存建立一一映射关系。从内核空间起始地址开始，到896M的内核空间地址区间，为直接内存映射区，该区域线性地址和分配的物理地址都是连续的。   896M以上的内核地址空间，又称为高端内存区域。\n   安全保护区：也成为内存空洞，大小为8M，其主要目的是为了避免 非连续区的非法访问，\n  动态映射区：也就是vmalloc Region，该区域由Vmalloc函数分配，特点是：虚拟地址空间连续，但是物理地址空间不一定连续。\n  永久映射区（Persistent Kernel Mapping Region）：该区域主要用于访问高端内存，通过alloc_page (_GFP_HIGHMEM)接口分配高端内存页，可以使用kmap函数将分配到的高端内存映射到该区域。\n  固定映射区（Fixing kernel Mapping Region）：该区域虚拟内存地址可以自由映射到物理内存的高端地址上，“固定”表现在“虚拟内存空间地址是固定的”，被映射的物理地址是可变的。\n   为什么会有固定映射这个概念呢 ?\n比如：在内核的启动过程中，有些模块需要使用虚拟内存并映射到指定的物理地址上，而且这些模块也没有办法等待完整的内存管理模块初始化之后再进行地址映射。因此，内核固定分配了一些虚拟地址，这些地址有固定的用途，使用该地址的模块在初始化的时候，将这些固定分配的虚拟地址映射到指定的物理地址上去。\n 4、总结 #  以上就是整个虚拟地址空间的划分，总结如下：\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":15,"href":"/docs/linux/linux_api/linux-api-%E6%8F%AD%E7%A7%98container_of%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/","title":"【Linux API 揭秘】container_of函数详解","section":"Linux API 揭秘","content":" Linux Version：6.6\nAuthor：Donge\nGithub：linux-api-insides\n  1、container_of函数介绍 #  container_of可以说是内核中使用最为频繁的一个函数了，简单来说，它的主要作用就是根据我们结构体中的已知的成员变量的地址，来寻求该结构体的首地址，直接看图，更容易理解。\n 下面我们看看linux是如何实现的吧\n 2、container_of函数实现 #  /** * container_of - cast a member of a structure out to the containing structure * @ptr:\tthe pointer to the member. * @type:\tthe type of the container struct this is embedded in. * @member:\tthe name of the member within the struct. * * WARNING: any const qualifier of @ptr is lost. */ #define container_of(ptr, type, member) ({\t\\ void *__mptr = (void *)(ptr);\t\\ static_assert(__same_type(*(ptr), ((type *)0)-\u0026gt;member) ||\t\\ __same_type(*(ptr), void),\t\\ \u0026#34;pointer type mismatch in container_of()\u0026#34;);\t\\ ((type *)(__mptr - offsetof(type, member))); })  函数名称：container_of\n文件位置：include/linux/container_of.h\n该函数里面包括了一些封装好的宏定义以及函数，比如：static_assert、__same_type、offsetof，以及一些指针的特殊用法，比如：(type *)0)，下面我们一一拆解来看。\n2.1 static_assert #  /** * static_assert - check integer constant expression at build time * * static_assert() is a wrapper for the C11 _Static_assert, with a * little macro magic to make the message optional (defaulting to the * stringification of the tested expression). * * Contrary to BUILD_BUG_ON(), static_assert() can be used at global * scope, but requires the expression to be an integer constant * expression (i.e., it is not enough that __builtin_constant_p() is * true for expr). * * Also note that BUILD_BUG_ON() fails the build if the condition is * true, while static_assert() fails the build if the expression is * false. */ #define static_assert(expr, ...) __static_assert(expr, ##__VA_ARGS__, #expr) #define __static_assert(expr, msg, ...) _Static_assert(expr, msg) 函数名称：static_assert\n文件位置：include/linux/build_bug.h\n函数解析：该宏定义主要用来 在编译时检查常量表达式，如果表达式为假，编译将失败，并打印传入的报错信息\n expr：该参数表示传入进来的常量表达式 ...：表示编译失败后，要打印的错误信息 _Static_assert：C11中引入的关键字，用于判断表达式expr并打印错误信息msg。  在container_of函数中，主要用来断言判断\nstatic_assert( __same_type(*(ptr), ((type *)0)-\u0026gt;member) || __same_type(*(ptr), void) , \u0026#34;pointer type mismatch in container_of()\u0026#34; );  2.2 __same_type #  /* Are two types/vars the same type (ignoring qualifiers)? */ #ifndef __same_type # define __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b)) #endif 函数名称：__same_type\n文件位置：include/linux/compiler.h\n函数解析：该宏定义用于检查两个变量是否是同种类型\n __builtin_types_compatible_p：gcc的内建函数，判断两个参数的类型是否一致，如果是则返回1 typeof：gcc的关键字，用于获取变量的类型信息  了解完__same_type，想要理解__same_type(*(ptr), ((type *)0)-\u0026gt;member)，需要先弄明白(type *)0的含义。\n  更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  2.3 (type *)0 #  (type *)0，该如何理解这个表达式呢？\n 首先，type是我们传入进来的结构体类型，比如上面讲到的struct test，而这里所做的可以理解为强制类型转换：(struct test *)addr。 addr可以表示内存空间的任意的地址，我们在强制转换后，默认后面一片的内存空间存储的是该数据结构。   而(type *)0的作用，也就是默认将0地址处的内存空间，转换为该数据类型。   我们就把0，当作我们正常的addr地址变量来操作，((type *)0)-\u0026gt;member，就是获取我们结构体的成员对象。 ((type *)0)-\u0026gt;member：是一种常见的技巧，用于直接获取结构体type的成员member的类型，而不需要定义一个type类型的对象。   2.4 offsetof #  #ifndef offsetof #define offsetof(TYPE, MEMBER) ((size_t) \u0026amp;((TYPE *)0)-\u0026gt;MEMBER) #endif 函数名称：offsetof\n文件位置：include/linux/stddef.h\n函数解析：该宏定义用于获取结构体中指定的成员，距离该结构体偏移量。\n TYPE：表示结构体的类型 MEMBER：表示指定的结构体成员 __builtin_offsetof：gcc内置函数，直接返回偏移量。   在新的linux源码中，直接引用了gcc内置的函数，而在老的内核源码中，该偏移量的实现方式如下：\n#define offsetof(TYPE, MEMBER) ((size_t) \u0026amp;((TYPE *)0)-\u0026gt;MEMBER) 同样用到了((TYPE *)addr)，上面我们知道\n ((TYPE *)addr)-\u0026gt;MEMBER：表示获取该结构体的成员 \u0026amp;((TYPE *)addr)-\u0026gt;MEMBER)：加了一个\u0026amp;，表示地址，取该成员的内存地址。  比如我们addr=0x00000010，那么\u0026amp;((TYPE *)0x00000010)-\u0026gt;MEMBER)就相当于0x00000010+size 比如我们addr=0，那么\u0026amp;((TYPE *)0)-\u0026gt;MEMBER)就相当于size     到这里，我们对container_of函数内部涉及的相关知识了然于胸，下面我们再来看container_of，简直容易到起飞。\n 2.5 container_of #  #define container_of(ptr, type, member) ({\t\\ void *__mptr = (void *)(ptr);\t\\ static_assert(__same_type(*(ptr), ((type *)0)-\u0026gt;member) ||\t\\ __same_type(*(ptr), void),\t\\ \u0026#34;pointer type mismatch in container_of()\u0026#34;);\t\\ ((type *)(__mptr - offsetof(type, member))); })  static_assert：断言信息，避免我们传入的参数类型不对，而做的编译检查处理，直接忽略。  #define container_of(ptr, type, member) ({\t\\ void *__mptr = (void *)(ptr);\t\\ ((type *)(__mptr - offsetof(type, member))); })   offsetof(type, member)：计算的是结构体中的成员的偏移量，这里称为size\n  (__mptr - offsetof(type, member))：也就是根据我们已知的成员变量地址，计算出来结构体的首地址\n  ((type *)(__mptr - offsetof(type, member)))：最后强制转换为(type *)，结构体指针。\n   比如，我们已知的结构体成员的地址为0xffff0000，计算之后如下：\n 3、总结 #  linux内核中，小小的一个函数，内部包括的技巧如此之多：static_assert、__same_type、(type *)0、offsetof。\n了解完内部完整的实现手法之后，我们也可以手码一个container_of了 :)\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":16,"href":"/docs/uboot/%E4%BA%8Cuboot%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/","title":"二、uboot启动流程分析","section":"Uboot开发","content":"二、uboot启动流程分析 #   上一篇文章：（一）uboot基础了解 下一篇文章：（三）Uboot驱动模型\n 同大多数的Bootloader一样，uboot的启动过程也分为BL1、BL2两个阶段，分别对应着SPL和Uboot。\nSPL（BL1阶段）：负责开发板的基础配置和设备初始化，并且搬运Uboot到内存中，由汇编代码和少量的C语言实现\nUboot（BL2阶段）：主要负责初始化外部设备，引导Kernel启动，由纯C语言实现。\n 我们这篇文章，主要介绍Uboot（BL2阶段）的启动流程，BL1阶段启动流程的详细分析，可以见我的后续文章。想要深入了解的，可以好好研究下！\n 2.1、程序执行流程图 #  我们先总体来看一下Uboot的执行步骤，这里以EMMC作为启动介质，进行分析！\n无论是哪种启动介质，基本流程都相似，我们这就往下看！\n==打开图片，结合文档、图片、代码进行理解！==\n 2.2、u-boot.lds——Uboot的入口函数 #  u-boot.lds：是uboot工程的链接脚本文件，对于工程的编译和链接有非常重要的作用，决定了uboot的组装，并且u-boot.lds链接文件中的ENTRY(_start)指定了uboot程序的入口地址。\n 如果不知道u-boot.lds放到在哪里，可以通过find -name u-boot.lds查找，根目录要进入到uboot的源码的位置哦！\n如果查找结果有很多，结合自己的板子信息，确定自己使用的u-boot.lds。\n当然，准确的方法是查看Makefile文件，分析出来u-boot.lds所生成的位置。\n 在u-boot.lds的文件中，可以看到.text段，存放的就是执行的文本段。截取部分代码段如下：\nOUTPUT_FORMAT(\u0026#34;elf32-littlearm\u0026#34;, \u0026#34;elf32-littlearm\u0026#34;, \u0026#34;elf32-littlearm\u0026#34;) OUTPUT_ARCH(arm) ENTRY(_start) SECTIONS { . = 0x00000000;\t@起始地址  . = ALIGN(4);\t@四字节对齐  .text :\t{\t*(.__image_copy_start)\t@映像文件复制起始地址 *(.vectors)\t@异常向量表 arch/arm/cpu/armv7/start.o (.text*)\t@启动函数 } ...... }   ENTRY(_start)：程序的入口函数，_start在arch/arm/lib/vectors.S中定义.globl _start\n  SECTIONS定义了段，包括text文本段、data数据段、bss段等。\n  __image_copy_start在System.map和u-boot.map中均有定义\n  arch/arm/cpu/armv7/start.o对应文件arch/arm/cpu/armv7/start.S，该文件中定义了main函数的入口。\n   Tip：上面只进行大概分析，有汇编经验的朋友，可以详细进行分析！\n 2.3、board_init_f——板级前置初始化 #  跟随上文的程序执行流程图，我们看board_init_f这个函数。其位于common/board_f.c。\nvoid board_init_f(ulong boot_flags) { gd-\u0026gt;flags = boot_flags; gd-\u0026gt;have_console = 0; if (initcall_run_list(init_sequence_f)) hang(); } static const init_fnc_t init_sequence_f[] = { setup_mon_len, ... log_init, arch_cpu_init,\t/* basic arch cpu dependent setup */ env_init,\t/* initialize environment */ ... reloc_fdt, reloc_bootstage, reloc_bloblist, setup_reloc, ... } board_init_f()，其最核心的内容就是调用了init_sequence_f初始化序列，进行了一系列初始化的工作。\n主要包括：串口、定时器、设备树、DM驱动模型等，另外还包括global_data结构体相关对象的变量。\n 详细分析，可以看文末的参考文章[1]\n 我们需要注意的一点就是，在初始化队列末尾，执行了几个reloc_xxx的函数，这几个函数实现了Uboot的重定向功能。\n2.4、relocate_code重定向 #   重定向技术，可以说也算是Uboot的一个重点了，也就是将uboot自身镜像拷贝到ddr上的另外一个位置的动作。\n 2.4.1 为什么需要重定向呢？ #   一般需要重定向的条件如下：\n  uboot存储在只读存储器上，比如ROM、Nor flash，需要将代码拷贝到DDR上，才能完整运行Uboot。 为Kernel腾空间，Kernel一般会放在DDR的地段地址上，所以要把Uboot重定向到顶端地址，避免冲突。  2.4.2 Uboot是如何重定向的？ #  Uboot的重定向有如下几个步骤：\n 对relocate进行空间划分 计算uboot代码空间到relocate的位置的偏移 relocate旧的global_data到新的global_data空间上 relocate Uboot 修改relocate后的全局变量的label relocate中断向量表  运行大致流程：\narch/arm/lib/crt0.S文件内，主要实现了：\nENTRY(_main) bl board_init_f @@ 在board_init_f里面实现了 @@ （1）对relocate进行空间规划 @@ （2）计算uboot代码空间到relocation的位置的偏移 @@ （3）relocate旧的global_data到新的global_data的空间上 ldr sp, [r9, #GD_START_ADDR_SP] /* sp = gd-\u0026gt;start_addr_sp */  bic sp, sp, #7 /* 8-byte alignment for ABI compliance */  ldr r9, [r9, #GD_BD] /* r9 = gd-\u0026gt;bd */  sub r9, r9, #GD_SIZE /* new GD is below bd */ @@ 把新的global_data地址放在r9寄存器中 adr lr, here ldr r0, [r9, #GD_RELOC_OFF] /* r0 = gd-\u0026gt;reloc_off */  add lr, lr, r0 @@ 计算返回地址在新的uboot空间中的地址。b调用函数返回之后，就跳到了新的uboot代码空间中。 ldr r0, [r9, #GD_RELOCADDR] /* r0 = gd-\u0026gt;relocaddr */ @@ 把uboot的新的地址空间放到r0寄存器中，作为relocate_code的参数 b relocate_code @@ 跳转到relocate_code中，在这里面实现了 @@ （1）relocate旧的uboot代码空间到新的空间上去 @@ （2）修改relocate之后全局变量的label @@ 注意，由于上述已经把lr寄存器重定义到uboot新的代码空间中了，所以返回之后，就已经跳到了新的代码空间了！！！！！！ bl relocate_vectors @@ relocate中断向量表  setup_reloc——重定向地址查看（仿真有关）  在这里我们说明一下board_init_f里面的setup_reloc初始化函数\nstatic int setup_reloc(void) { if (gd-\u0026gt;flags \u0026amp; GD_FLG_SKIP_RELOC) { debug(\u0026#34;Skipping relocation due to flag\\n\u0026#34;); return 0; } #ifdef CONFIG_SYS_TEXT_BASE #ifdef ARM  gd-\u0026gt;reloc_off = gd-\u0026gt;relocaddr - (unsigned long)__image_copy_start; #elif defined(CONFIG_M68K)  /* * On all ColdFire arch cpu, monitor code starts always * just after the default vector table location, so at 0x400 */ gd-\u0026gt;reloc_off = gd-\u0026gt;relocaddr - (CONFIG_SYS_TEXT_BASE + 0x400); #elif !defined(CONFIG_SANDBOX)  gd-\u0026gt;reloc_off = gd-\u0026gt;relocaddr - CONFIG_SYS_TEXT_BASE; #endif #endif  memcpy(gd-\u0026gt;new_gd, (char *)gd, sizeof(gd_t)); debug(\u0026#34;Relocation Offset is: %08lx\\n\u0026#34;, gd-\u0026gt;reloc_off); if (is_debug_open()) { printf(\u0026#34;Relocating to %08lx, new gd at %08lx, sp at %08lx\\n\u0026#34;, gd-\u0026gt;relocaddr, (ulong)map_to_sysmem(gd-\u0026gt;new_gd), gd-\u0026gt;start_addr_sp); } return 0; } 由于，Uboot进行了重定向，所以按照常规的地址仿真的话，我们可能访问到错误的内存空间，通过setup_reloc的Relocating to %08lx打印，我们可以得到重定向后的地址，方便我们仿真。\nUboot的重定向也有相当大的一部分知识点，上面也仅仅是简单介绍了relocate的基本步骤和流程，后续看大家需要，如果大家想了解，我再补上这一部分。\n2.4.3 Uboot重定向作用 #  总之，Uboot重定向之后，把Uboot整体搬运到了高端内存区，为Kernel的加载提供空间，避免内存践踏。\n2.5、board_init_r——板级后置初始化 #   我们接着跟着流程图往下看，重定向之后，Uboot运行于新的地址空间，接着我们执行board_init_r，主要作为Uboot运行的最后初始化步骤。\n board_init_r这个函数，同样位于common/board_f.c，主要用于初始化各类外设信息\nvoid board_init_r(gd_t *new_gd, ulong dest_addr) {\tif (initcall_run_list(init_sequence_r)) hang(); /* NOTREACHED - run_main_loop() does not return */ hang(); } static init_fnc_t init_sequence_r[] = { initr_reloc, initr_reloc_global_data, board_init,\t/* Setup chipselects */ initr_dm, initr_mmc, ... run_main_loop } 与board_init_f相同，同样有一个init_sequence_r初始化列表，包括：initr_dmDM模型初始化，initr_mmcMMC驱动初始化，等等。\n最终，uboot就运行到了run_main_loop，进而执行main_loop这个函数。\n2.6、main_loop——Uboot主循环 #   该函数为Uboot的最终执行函数，无论是加载kernel还是uboot的命令行体系，均由此实现。\n void main_loop(void) { const char *s; bootstage_mark_name(BOOTSTAGE_ID_MAIN_LOOP, \u0026#34;main_loop\u0026#34;); if (IS_ENABLED(CONFIG_VERSION_VARIABLE)) env_set(\u0026#34;ver\u0026#34;, version_string); /* set version variable */ cli_init(); if (IS_ENABLED(CONFIG_USE_PREBOOT)) run_preboot_environment_command(); if (IS_ENABLED(CONFIG_UPDATE_TFTP)) update_tftp(0UL, NULL, NULL); s = bootdelay_process(); if (cli_process_fdt(\u0026amp;s)) cli_secure_boot_cmd(s); autoboot_command(s); cli_loop(); panic(\u0026#34;No CLI available\u0026#34;); } env_set：设置环境变量，两个参数分别为name和value\ncli_init：用于初始化hash shell的一些变量\nrun_preboot_environment_command：执行预定义的环境变量的命令\nbootdelay_process：加载延时处理，一般用于Uboot启动后，有几秒的倒计时，用于进入命令行模式。\ncli_loop：命令行模式，主要作用于Uboot的命令行交互。\n2.6.1 bootdelay_process #   记得对照文章开始的执行流程图哦！\n 详细解释标注于代码中\u0026hellip;\u0026hellip;\nconst char *bootdelay_process(void) { char *s; int bootdelay; bootcount_inc(); s = env_get(\u0026#34;bootdelay\u0026#34;);\t//先判断是否有bootdelay环境变量，如果没有，就使用menuconfig中配置的CONFIG_BOOTDELAY时间  bootdelay = s ? (int)simple_strtol(s, NULL, 10) : CONFIG_BOOTDELAY; if (IS_ENABLED(CONFIG_OF_CONTROL))\t//是否使用设备树进行配置  bootdelay = fdtdec_get_config_int(gd-\u0026gt;fdt_blob, \u0026#34;bootdelay\u0026#34;, bootdelay); debug(\u0026#34;### main_loop entered: bootdelay=%d\\n\\n\u0026#34;, bootdelay); if (IS_ENABLED(CONFIG_AUTOBOOT_MENU_SHOW)) bootdelay = menu_show(bootdelay); bootretry_init_cmd_timeout(); #ifdef CONFIG_POST  if (gd-\u0026gt;flags \u0026amp; GD_FLG_POSTFAIL) { s = env_get(\u0026#34;failbootcmd\u0026#34;); } else #endif /* CONFIG_POST */ if (bootcount_error()) s = env_get(\u0026#34;altbootcmd\u0026#34;); else s = env_get(\u0026#34;bootcmd\u0026#34;);\t//获取bootcmd环境变量，用于后续的命令执行  if (IS_ENABLED(CONFIG_OF_CONTROL)) process_fdt_options(gd-\u0026gt;fdt_blob); stored_bootdelay = bootdelay; return s; } 2.6.2 autoboot_command #  详细解释标注于代码中\u0026hellip;\u0026hellip;\nvoid autoboot_command(const char *s) { debug(\u0026#34;### main_loop: bootcmd=\\\u0026#34;%s\\\u0026#34;\\n\u0026#34;, s ? s : \u0026#34;\u0026lt;UNDEFINED\u0026gt;\u0026#34;); if (stored_bootdelay != -1 \u0026amp;\u0026amp; s \u0026amp;\u0026amp; !abortboot(stored_bootdelay)) { bool lock; int prev; lock = IS_ENABLED(CONFIG_AUTOBOOT_KEYED) \u0026amp;\u0026amp; !IS_ENABLED(CONFIG_AUTOBOOT_KEYED_CTRLC); if (lock) prev = disable_ctrlc(1); /* disable Ctrl-C checking */ run_command_list(s, -1, 0); if (lock) disable_ctrlc(prev);\t/* restore Ctrl-C checking */ } if (IS_ENABLED(CONFIG_USE_AUTOBOOT_MENUKEY) \u0026amp;\u0026amp; menukey == AUTOBOOT_MENUKEY) { s = env_get(\u0026#34;menucmd\u0026#34;); if (s) run_command_list(s, -1, 0); } } 我们看一下判断条件stored_bootdelay != -1 \u0026amp;\u0026amp; s \u0026amp;\u0026amp; !abortboot(stored_bootdelay\n stored_bootdelay：为环境变量的值，或者menuconfig设置的值 s：为环境变量bootcmd的值，为后续运行的指令 abortboot(stored_bootdelay)：主要用于判断是否有按键按下。如果按下，则不执行bootcmd命令，进入cli_loop 命令行模式；如果不按下，则执行bootcmd命令，跳转到加载Linux启动。  2.6.3 cli_loop #  void cli_loop(void) { bootstage_mark(BOOTSTAGE_ID_ENTER_CLI_LOOP); #ifdef CONFIG_HUSH_PARSER  parse_file_outer(); /* This point is never reached */ for (;;);\t//死循环 #elif defined(CONFIG_CMDLINE)  cli_simple_loop(); #else  printf(\u0026#34;## U-Boot command line is disabled. Please enable CONFIG_CMDLINE\\n\u0026#34;); #endif /*CONFIG_HUSH_PARSER*/} 如上代码，程序只执行parse_file_outer来处理用户的输入、输出信息。\n 好啦，基本到这里，我们已经对Uboot的启动流程了然于胸了吧！\n当然，更深层次的不建议去深入了解，有时间可以慢慢去研究。\n 大家有疑问，可以评论区交流\u0026hellip;\u0026hellip;\n参考文章：\n[1]：boadr_init_f介绍\n[2]：启动流程参考\n[3]：main_loop相关\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":17,"href":"/docs/linux/linux_kernel_lock/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3linux%E9%94%81%E6%9C%BA%E5%88%B6%E4%B8%89%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/","title":"【深入理解Linux锁机制】三、原子操作","section":"Linux 内核锁详解","content":"【深入理解Linux内核锁】三、原子操作 #  1、原子操作思想 #  原子操作（atomic operation），不可分割的操作。其通过原子变量来实现，以保证单个CPU周期内，读写该变量不能被打断，进而判断该变量的值，来解决并发引起的互斥。\nAtomic类型的变量可以在执行期间禁止中断，并保证在访问变量时的原子性。\n 简单来说，我们可以把原子变量看作为一个标志位，然后再来检测该标志位的值。\n其原子性表现在：操作该标志位的值，不可被打断。\n 在Linux内核中，提供了两类原子操作的接口，分别是针对位和整型变量的原子操作。\n2、整型变量原子操作 #  2.1 API接口 #   对于整形变量的原子操作，内核提供了一系列的 API接口\n /*设置原子变量的值*/ atomic_t v = ATOMIC_INIT(0); /* 定义原子变量v并初始化为0 */ void atomic_set(atomic_t *v, int i); /* 设置原子变量的值为i */ /*获取原子变量的值*/ atomic_read(atomic_t *v); /* 返回原子变量的值*/ /*原子变量的加减*/ void atomic_add(int i, atomic_t *v); /* 原子变量增加i */ void atomic_sub(int i, atomic_t *v); /* 原子变量减少i */ /*原子变量的自增，自减*/ void atomic_inc(atomic_t *v);\t/* 原子变量增加1 */ void atomic_dec(atomic_t *v); /* 原子变量减少1 */ /*原子变量的操作并测试*/ int atomic_inc_and_test(atomic_t *v);\t/*进行对应操作后，测试原子变量值是否为0*/ int atomic_dec_and_test(atomic_t *v); int atomic_sub_and_test(int i, atomic_t *v); /*原子变量的操作并返回*/ int atomic_add_return(int i, atomic_t *v);\t/*进行对应操作后，返回新的值*/ int atomic_sub_return(int i, atomic_t *v); int atomic_inc_return(atomic_t *v); int atomic_dec_return(atomic_t *v); 2.2 API实现 #   我们下面就介绍几个稍微有代表性的接口实现\n以下基于Linux内核源码4.19，刚看是看的时候，有点摸不着头脑，因为定义的地方和引用的地方较多，不太容易找到，后来才慢慢得窥门径。\n 2.2.1 原子变量结构体 #  typedef struct { int counter; } atomic_t; 结构体名称：atomic_t\n文件位置：include/linux/types.h\n主要作用：原子变量结构体，该结构体只包含一个整型成员变量counter，用于存储原子变量的值。\n2.2.2 设置原子变量操作 #   设置原子变量的值的方式有两种：\n 通过ATOMIC_INIT宏定义来设置 通过atomic_set函数来定义   2.2.2.1 ATOMIC_INIT #  #define ATOMIC_INIT(i)\t{ (i) } 函数介绍：定义了一个ATOMIC类型的变量，并初始化为给定的值。\n文件位置：arch/arm/include/asm/atomic.h\n实现方法：这个宏定义比较简单，通过大括号将值包裹起来作为一个结构体，结构体的第一个成员就是给定的该值。\n2.2.2.2 atomic_set #  // arch/arm/include/asm/atomic.h #define atomic_set(v,i)\tWRITE_ONCE(((v)-\u0026gt;counter), (i))  // include/linux/compiler.h #define WRITE_ONCE(x, val) \\ ({\t\\ union { typeof(x) __val; char __c[1]; } __u =\t\\ { .__val = (__force typeof(x)) (val) }; \\ __write_once_size(\u0026amp;(x), __u.__c, sizeof(x));\t\\ __u.__val;\t\\ })  static __always_inline void __write_once_size(volatile void *p, void *res, int size) { switch (size) { case 1: *(volatile __u8 *)p = *(__u8 *)res; break; case 2: *(volatile __u16 *)p = *(__u16 *)res; break; case 4: *(volatile __u32 *)p = *(__u32 *)res; break; case 8: *(volatile __u64 *)p = *(__u64 *)res; break; default: barrier(); __builtin_memcpy((void *)p, (const void *)res, size); barrier(); } } 函数介绍：该函数也用作初始化原子变量\n文件位置：arch/arm/include/asm/atomic.h\n实现方式：通过调用WRITE_ONCE来实现，其中WRITE_ONCE宏实现了一些屏蔽编译器优化的技巧，确保写入操作是原子的。\n atomic_set调用WRITE_ONCE将i的值写入原子变量(v)-\u0026gt;counter中，WRITE_ONCE以保证操作的原子性 WRITE_ONCE用来保证操作的原子性，它是怎么实现的呢？  创建union联合体，包括__val和__C成员变量 定义一个__U变量，使用强制转换将参数__val转换为typeof(x)类型，传递给联合体变量__u.__val 调用__write_once_size函数，将__u.__c指向的内存块的内容写入到变量x的内存空间中，大小为sizeof(x)。 函数返回__u.__val，也就是写入的值   union联合体  它的特点是存储多种数据类型的值，但是所有成员共享同一个内存空间，这样可以节省内存空间。 主要作用是将一个非字符类型的数据x强制转换为一个字符类型的数据，以字符类型数据来访问该区块的内存单元。   __write_once_size函数实现了操作的原子性，核心有以下几点：  该函数在向内存写入数据时使用了volatile关键字，告诉编译器不要进行优化，每次操作都从内存中读取最新的值。 函数中的switch语句保证了对不同大小的数据类型使用不同的存储方式，可以保证内存访问的原子性。 对于默认情况，则使用了__builtin_memcpy函数进行复制，而这个函数具有原子性。 barrier()函数指示CPU要完成所有之前的内存操作，以及确保执行顺序与其他指令不发生重排。    2.2.3 原子变量的加减 #  2.2.3.1 ATOMIC_OPS #  /* * ARMv6 UP and SMP safe atomic ops. We use load exclusive and * store exclusive to ensure that these are atomic. We may loop * to ensure that the update happens. */ #define ATOMIC_OP(op, c_op, asm_op)\t\\ static inline void atomic_##op(int i, atomic_t *v)\t\\ {\t\\ unsigned long tmp;\t\\ int result;\t\\ \\ prefetchw(\u0026amp;v-\u0026gt;counter);\t\\ __asm__ __volatile__(\u0026#34;@ atomic_\u0026#34; #op \u0026#34;\\n\u0026#34;\t\\ \u0026#34;1:\tldrex\t%0, [%3]\\n\u0026#34;\t\\ \u0026#34;\t\u0026#34; #asm_op \u0026#34;\t%0, %0, %4\\n\u0026#34;\t\\ \u0026#34;\tstrex\t%1, %0, [%3]\\n\u0026#34;\t\\ \u0026#34;\tteq\t%1, #0\\n\u0026#34;\t\\ \u0026#34;\tbne\t1b\u0026#34;\t\\ : \u0026#34;=\u0026amp;r\u0026#34; (result), \u0026#34;=\u0026amp;r\u0026#34; (tmp), \u0026#34;+Qo\u0026#34; (v-\u0026gt;counter)\t\\ : \u0026#34;r\u0026#34; (\u0026amp;v-\u0026gt;counter), \u0026#34;Ir\u0026#34; (i)\t\\ : \u0026#34;cc\u0026#34;);\t\\ }\t\\ #define ATOMIC_OP_RETURN(op, c_op, asm_op)\t\\ static inline int atomic_##op##_return_relaxed(int i, atomic_t *v)\t\\ {\t\\ unsigned long tmp;\t\\ int result;\t\\ \\ prefetchw(\u0026amp;v-\u0026gt;counter);\t\\ \\ __asm__ __volatile__(\u0026#34;@ atomic_\u0026#34; #op \u0026#34;_return\\n\u0026#34;\t\\ \u0026#34;1:\tldrex\t%0, [%3]\\n\u0026#34;\t\\ \u0026#34;\t\u0026#34; #asm_op \u0026#34;\t%0, %0, %4\\n\u0026#34;\t\\ \u0026#34;\tstrex\t%1, %0, [%3]\\n\u0026#34;\t\\ \u0026#34;\tteq\t%1, #0\\n\u0026#34;\t\\ \u0026#34;\tbne\t1b\u0026#34;\t\\ : \u0026#34;=\u0026amp;r\u0026#34; (result), \u0026#34;=\u0026amp;r\u0026#34; (tmp), \u0026#34;+Qo\u0026#34; (v-\u0026gt;counter)\t\\ : \u0026#34;r\u0026#34; (\u0026amp;v-\u0026gt;counter), \u0026#34;Ir\u0026#34; (i)\t\\ : \u0026#34;cc\u0026#34;);\t\\ \\ return result;\t\\ }  #define ATOMIC_FETCH_OP(op, c_op, asm_op)\t\\ static inline int atomic_fetch_##op##_relaxed(int i, atomic_t *v)\t\\ {\t\\ unsigned long tmp;\t\\ int result, val;\t\\ \\ prefetchw(\u0026amp;v-\u0026gt;counter);\t\\ \\ __asm__ __volatile__(\u0026#34;@ atomic_fetch_\u0026#34; #op \u0026#34;\\n\u0026#34;\t\\ \u0026#34;1:\tldrex\t%0, [%4]\\n\u0026#34;\t\\ \u0026#34;\t\u0026#34; #asm_op \u0026#34;\t%1, %0, %5\\n\u0026#34;\t\\ \u0026#34;\tstrex\t%2, %1, [%4]\\n\u0026#34;\t\\ \u0026#34;\tteq\t%2, #0\\n\u0026#34;\t\\ \u0026#34;\tbne\t1b\u0026#34;\t\\ : \u0026#34;=\u0026amp;r\u0026#34; (result), \u0026#34;=\u0026amp;r\u0026#34; (val), \u0026#34;=\u0026amp;r\u0026#34; (tmp), \u0026#34;+Qo\u0026#34; (v-\u0026gt;counter)\t\\ : \u0026#34;r\u0026#34; (\u0026amp;v-\u0026gt;counter), \u0026#34;Ir\u0026#34; (i)\t\\ : \u0026#34;cc\u0026#34;);\t\\ \\ return result;\t\\ }  #define ATOMIC_OPS(op, c_op, asm_op)\t\\ ATOMIC_OP(op, c_op, asm_op)\t\\ ATOMIC_OP_RETURN(op, c_op, asm_op)\t\\ ATOMIC_FETCH_OP(op, c_op, asm_op)  ATOMIC_OPS(add, +=, add) ATOMIC_OPS(sub, -=, sub)  找atomic_add找半天，还找到了不同的架构下面。:(\n原来内核通过各种宏定义将其操作全部管理起来，宏定义在内核中的使用也是非常广泛了。\n 函数作用：通过一些列宏定义，来实现原子变量的add、sub、and、or等原子变量操作\n文件位置：arch/arm/include/asm/atomic.h\n实现方式：\n 我们以atomic_##op为例来介绍，其他大同小异！\n #define ATOMIC_OP(op, c_op, asm_op)\t\\ static inline void atomic_##op(int i, atomic_t *v)\t\\ {\t\\ unsigned long tmp;\t\\ int result;\t\\ \\ prefetchw(\u0026amp;v-\u0026gt;counter);\t\\ __asm__ __volatile__(\u0026#34;@ atomic_\u0026#34; #op \u0026#34;\\n\u0026#34;\t\\ \u0026#34;1:\tldrex\t%0, [%3]\\n\u0026#34;\t\\ \u0026#34;\t\u0026#34; #asm_op \u0026#34;\t%0, %0, %4\\n\u0026#34;\t\\ \u0026#34;\tstrex\t%1, %0, [%3]\\n\u0026#34;\t\\ \u0026#34;\tteq\t%1, #0\\n\u0026#34;\t\\ \u0026#34;\tbne\t1b\u0026#34;\t\\ : \u0026#34;=\u0026amp;r\u0026#34; (result), \u0026#34;=\u0026amp;r\u0026#34; (tmp), \u0026#34;+Qo\u0026#34; (v-\u0026gt;counter)\t\\ : \u0026#34;r\u0026#34; (\u0026amp;v-\u0026gt;counter), \u0026#34;Ir\u0026#34; (i)\t\\ : \u0026#34;cc\u0026#34;);\t\\ }\t 首先是函数名称atomic_##op，通过##来实现字符串的拼接，使函数名称可变，如atomic_add、atomic_sub等 调用prefetchw函数，预取数据到L1缓存，方便操作，提高程序性能，但是不要滥用。 __asm__ __volatile__：表示汇编指令 \u0026quot;@ atomic_\u0026quot; #op \u0026quot;\\n\u0026quot;：添加汇编注释，也就是我们的函数名字，如：atomic_add、atomic_sub \u0026quot;1: ldrex %0, [%3]\\n\u0026quot;：将%3存储地址的数据，读入到%0地址中，ldrex为独占式的读取操作。 \u0026quot; \u0026quot; #asm_op \u0026quot; %0, %0, %4\\n\u0026quot;：\u0026quot; #asm_op \u0026quot;表示作为宏定义传进来的参数，表示不同的操作码add、sub等，操作%0和%4对应的地址的值，并将结果返回到%0地址处 \u0026quot; strex %1, %0, [%3]\\n\u0026quot; ：表示将%0地址处的值写入%3地址处，strex为独占式的写操作，写入的结果会返回到%1地址中 \u0026quot; teq %1, #0\\n\u0026quot;：测试%1寄存器的值是否为0，如果不等于0，则执行下面的\u0026quot; bne 1b\u0026quot; 操作，跳转到1代码标签的位置，也就是ldrex前面的1的位置 : \u0026quot;=\u0026amp;r\u0026quot; (result), \u0026quot;=\u0026amp;r\u0026quot; (tmp), \u0026quot;+Qo\u0026quot; (v-\u0026gt;counter)：根据汇编语法，前两个为输出操作数，第三个为输入输出操作数 : \u0026quot;r\u0026quot; (\u0026amp;v-\u0026gt;counter), \u0026quot;Ir\u0026quot; (i)：根据汇编语法，这两个为输入操作数 : \u0026quot;cc\u0026quot;：表示可能会修改条件码寄存器，编译期间需要优化。  总结：上述原子操作，通过ldrex和strex也就是我们说的load和store指令，来完成数据的读写，同时也保证了其原子性！\n 这一部分，牵涉到汇编的语法，需要提前了解下基础的汇编指令。\n 2.2.3.2 atomic_add和atomic_sub定义 #  ATOMIC_OPS(add, +=, add) ATOMIC_OPS(sub, -=, sub)  通过宏定义来实现atomic_add和atomic_sub的定义，下面我们就不一一分析了，原理都是通过ARM提供的ldrex strex也就是我们常说的Load和Store指令实现读取操作，确保操作的原子性。\n 3、位原子操作 #   对于位原子操作，内核也提供了一系列的 API接口\n 3.1 API接口 #  void set_bit(nr, void *addr);\t//\t设置位：设置addr地址的第nr位，所谓设置位即是将位写为1 void clear_bit(nr, void *addr);\t//\t清除位：清除addr地址的第nr位，所谓清除位即是将位写为0 void change_bit(nr, void *addr);\t//\t改变位：对addr地址的第nr位进行反置。 int test_bit(nr, void *addr);\t//\t测试位：返回addr地址的第nr位。 int test_and_set_bit(nr, void *addr);//\t测试并设置位 int test_and_clear_bit(nr, void *addr);\t//\t测试并清除位 int test_and_change_bit(nr, void *addr);//\t测试并改变位 3.2 API实现 #   同样，我们还是简单介绍几个接口，其他核心实现原理相同\n 3.2.1 set_bit #  #define set_bit(nr,p)\tATOMIC_BITOP(set_bit,nr,p)  #define ATOMIC_BITOP(name,nr,p)\t\\ (__builtin_constant_p(nr) ? ____atomic_##name(nr, p) : _##name(nr,p))  extern void _set_bit(int nr, volatile unsigned long * p); /* * These functions are the basis of our bit ops. * * First, the atomic bitops. These use native endian. */ static inline void ____atomic_set_bit(unsigned int bit, volatile unsigned long *p) { unsigned long flags; unsigned long mask = BIT_MASK(bit); p += BIT_WORD(bit); raw_local_irq_save(flags); *p |= mask; raw_local_irq_restore(flags); } #define BIT_MASK(nr)\t(1UL \u0026lt;\u0026lt; ((nr) % BITS_PER_LONG)) #define BIT_WORD(nr)\t((nr) / BITS_PER_LONG)  #ifdef CONFIG_64BIT #define BITS_PER_LONG 64 #else #define BITS_PER_LONG 32 #endif /* CONFIG_64BIT */函数介绍：该函数用于原子操作某个地址的某一位。\n文件位置：/arch/arm/include/asm/bitops.h\n实现方式：\n __builtin_constant_p：GCC的一个内置函数，用来判断表达式是否为常量，如果为常量，则返回值为1 ____atomic_set_bit函数中BIT_MASK，用于获取操作位的掩码，将要设置的位设置为1，其他为0 BIT_WORD：确定要操作位的偏移，要偏移多少个字 最后通过raw_local_irq_save和raw_local_irq_restore中断屏蔽来保证位操作*p |= mask;的原子性  4、总结 #  该文章主要详细了解了Linux内核锁的原子操作，原子操作分为两种：整型变量的原子操作和位原子操作。\n 整型变量的原子操作：通过ldrex和strex来实现 位原子操作：通过中断屏蔽来实现。   欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":18,"href":"/docs/linux/linux_nvmem_subsystem/nvmem%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E4%B8%89%E6%A0%B8%E5%BF%83%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%8F%8A%E5%86%85%E5%9C%A8%E5%85%B3%E8%81%94/","title":"【NVMEM子系统深入剖析】三、核心数据结构及内在关联","section":"Linux NVMEM 子系统","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":19,"href":"/docs/embeded_tech/embeded_interview/linux%E7%94%A8%E6%88%B7%E6%80%81%E5%92%8C%E5%86%85%E6%A0%B8%E6%80%81%E4%BA%A4%E4%BA%92%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/","title":"Linux用户态和内核态交互的几种方式","section":"嵌入式面经","content":"Linux用户态和内核态交互的几种方式 #  Linux分为内核态Kernel Mode和用户态User Mode，其通信方式主要有：\n 系统调用System Call：最常见的用户态和内核态之间的通信方式。通过系统调用接口（open、read、write、fork等）请求内核执行特定的动作。 中断Interrupts：中断包括软中断和硬中断，每当中断到来的时候，CPU会暂停当前执行的用户态代码，切换到内核态来处理中断。 信号Signal：内核通过Signal通知用户态进程发生了某些事件，用户态注册信号处理函数，来响应特定的信号事件。如 SIGTERM、SIGINT 等。 共享内存Share Memory：允许多个进程在它们的地址空间中共享一块内存区域，从而实现用户态和内核态之间的高效通信。这种方式避免了用户态和内核态之间频繁切换的问题，但是也需要考虑到数据的同步问题，保证数据一致性。   用户态User Mode访问内核态Kernel Mode的数据交互的方式有：\n  procfs进程文件系统：一个伪文件系统，因为其不占用外部存储空间，只占有少量的内存，挂载在/proc目录下\n  sysctl：它也是一个Linux命令，主要用来修改内核的运行时参数，也就是在内核运行时，动态修改内核参数。\n 和 procfs 的区别在于：procfs 主要是输出只读数据，而 sysctl 输出的大部分信息是可写的。\n   sysfs虚拟文件系统：通过/sys来完成用户态和内核的通信，和 procfs 不同的是，sysfs 是将一些原本在 procfs 中的，关于设备和驱动的部分，独立出来，以 “设备树” 的形式呈现给用户。\n  netlink 接口：也是最常用的一种方式，本质是socket接口，使用netlink用于网络相关的内核和用户进程之间的消息传递。\n  共享内存Share Memory：允许多个进程在它们的地址空间中共享一块内存区域，从而实现用户态和内核态之间的高效数据传输。\n   欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":20,"href":"/docs/uboot/%E4%B8%89uboot%E9%A9%B1%E5%8A%A8%E6%A8%A1%E5%9E%8B/","title":"三、Uboot驱动模型","section":"Uboot开发","content":"三、Uboot驱动模型 #   全文耗时一周，精心汇总，希望对大家有所帮助，感觉可以的点赞，关注，不迷路，后续还有更多干货！\n看文章前，答应我，静下心来，慢慢品！\n 3.1、什么是Uboot驱动模型 #  学过Linux的朋友基本都知道Linux的设备驱动模型，Uboot根据Linux的驱动模型架构，也引入了Uboot的驱动模型（driver model ：DM）。\n**这种驱动模型为驱动的定义和访问接口提供了统一的方法。**提高了驱动之间的兼容性以及访问的标准型，uboot驱动模型和kernel中的设备驱动模型类似。\n3.2、为什么要有驱动模型呢 #   无论是Linux还是Uboot，一个新对象的产生必定有其要解决的问题，驱动模型也不例外！\n  提高代码的可重用性：为了能够使代码在不同硬件平台，不同体系架构下运行，必须要最大限度的提高代码的可重用性。 高内聚，低耦合：分层的思想也是为了达到这一目标，低耦合体现在对外提供统一的抽象访问接口，高内聚将相关度紧密的集中抽象实现。 便于管理：在不断发展过程中，硬件设备越来越多，驱动程序也越来越多，为了更好的管理驱动，也需要一套优秀的驱动架构！  3.3、如何使用uboot的DM模型 #   DM模型的使用，可以通过menuconfig来配置。\nmake menuconfig\n ①：menuconfig配置全局DM模型 #  Device Drivers -\u0026gt; Generic Driver Options -\u0026gt; Enable Driver Model 通过上面的路径来打开Driver Model模型，最终配置在.config文件中，CONFIG_DM=y\n②：指定某个驱动的DM模型 #  全局的DM模型打开后，我们对于不通的驱动模块，使能或者失能DM功能。如MMC驱动为例：\nDevice Drivers -\u0026gt; MMC Host controller Support -\u0026gt; Enable MMC controllers using Driver Model 最终反映在.config文件中的CONFIG_DM_MMC=y\n在对应的驱动中，可以看到判断#if !CONFIG_IS_ENABLED(DM_MMC)，来判断是否打开DM驱动模型。\n在管理驱动的Makefile文件中，也能看到obj-$(CONFIG_$(SPL_)DM_MMC) += mmc-uclass.o，来判断是否将驱动模型加入到编译选项中。\n总之，我们要打开DM模型，最后反映在几个配置信息上：\n CONFIG_DM=y，全局DM模型打开 CONFIG_DM_XXX=y，某个驱动的DM模型的打开 可以通过Kconifg、Makefile来查看对应宏的编译情况  3.4、DM模型数据结构 #  要想了解DM模型整套驱动框架，我们必须先了解它的一砖一瓦！也就是组成驱动框架的各个数据结构。\n① global_data #  typedef struct global_data { ... #ifdef CONFIG_DM  struct udevice\t*dm_root;\t/* Root instance for Driver Model */ struct udevice\t*dm_root_f;\t/* Pre-relocation root instance */ struct list_head uclass_root;\t/* Head of core tree */ #endif ... } global_data，管理着整个Uboot的全局变量，其中dm_root，dm_root_f，uclass_root用来管理整个DM模型。这几个变量代表什么意思呢？\n dm_root：DM模型的根设备 dm_root_f：重定向前的根设备 uclass_root：uclass链表的头  这几个变量，最终要的作用就是：管理整个模型中的udevice设备信息和uclass驱动类。\n② uclass #  我们首先看一下uclass这个结构体\n/** * struct uclass - a U-Boot drive class, collecting together similar drivers * * A uclass provides an interface to a particular function, which is * implemented by one or more drivers. Every driver belongs to a uclass even * if it is the only driver in that uclass. An example uclass is GPIO, which * provides the ability to change read inputs, set and clear outputs, etc. * There may be drivers for on-chip SoC GPIO banks, I2C GPIO expanders and * PMIC IO lines, all made available in a unified way through the uclass. * * @priv: Private data for this uclass * @uc_drv: The driver for the uclass itself, not to be confused with a * \u0026#39;struct driver\u0026#39; * @dev_head: List of devices in this uclass (devices are attached to their * uclass when their bind method is called) * @sibling_node: Next uclass in the linked list of uclasses */ struct uclass { void *priv;\t//uclass的私有数据  struct uclass_driver *uc_drv;\t//uclass类的操作函数集合  struct list_head dev_head;\t//该uclass的所有设备  struct list_head sibling_node;\t//下一个uclass的节点 }; 根据注释，我们就可以了解到，uclass相当于老师，管理着==对应某一个类别下==的所有的udevice。\n 例如：一个IIC驱动程序，其驱动程序框架是一致的，只有一种，但是IIC驱动的设备可以有很多，如EEPROM，MCU6050等；\n所有在这里呢，dev_head链表就是用来管理该驱动类下的所有的设备。\n 总结：uclass，来管理该类型下的所有设备，并且有对应的uclass_driver驱动。\n  定义 #    uclass是uboot自动生成的，并且不是所有uclass都会生成，有对应uclass_driver并且有被udevice匹配到的uclass才会生成。\n  存放 #    所有生成的uclass都会被挂载gd-\u0026gt;uclass_root链表上。\n  相关API #     直接遍历链表gd-\u0026gt;uclass_root链表并且根据uclass_id来获取到相应的uclass。\n int uclass_get(enum uclass_id key, struct uclass **ucp); // 从gd-\u0026gt;uclass_root链表获取对应的ucla ss ③ uclass_driver #  正如上面，我们看到了uclass类所包含uclass_driver结构体，uclass_driver正如其名，它就是uclass的驱动程序。其主要作用是：为uclass提供统一管理的接口，结构体如下：\n/** * struct uclass_driver - Driver for the uclass * * A uclass_driver provides a consistent interface to a set of related * drivers. */ struct uclass_driver { const char *name; // 该uclass_driver的命令  enum uclass_id id; // 对应的uclass id /* 以下函数指针主要是调用时机的区别 */ int (*post_bind)(struct udevice *dev); // 在udevice被绑定到该uclass之后调用  int (*pre_unbind)(struct udevice *dev); // 在udevice被解绑出该uclass之前调用  int (*pre_probe)(struct udevice *dev); // 在该uclass的一个udevice进行probe之前调用  int (*post_probe)(struct udevice *dev); // 在该uclass的一个udevice进行probe之后调用  int (*pre_remove)(struct udevice *dev);// 在该uclass的一个udevice进行remove之前调用  int (*child_post_bind)(struct udevice *dev); // 在该uclass的一个udevice的一个子设备被绑定到该udevice之后调用  int (*child_pre_probe)(struct udevice *dev); // 在该uclass的一个udevice的一个子设备进行probe之前调用  int (*init)(struct uclass *class); // 安装该uclass的时候调用  int (*destroy)(struct uclass *class); // 销毁该uclass的时候调用  int priv_auto_alloc_size; // 需要为对应的uclass分配多少私有数据  int per_device_auto_alloc_size; //  int per_device_platdata_auto_alloc_size; //  int per_child_auto_alloc_size; //  int per_child_platdata_auto_alloc_size; //  const void *ops; //操作集合  uint32_t flags; // 标识为 };   定义 #    uclass_driver主要通过UCLASS_DRIVER来定义，这里就简单说明一下底层代码，耐心看哦！\n 下面以pinctrl为例\n UCLASS_DRIVER(pinctrl) = { .id = UCLASS_PINCTRL, .post_bind = pinctrl_post_bind, .flags = DM_UC_FLAG_SEQ_ALIAS, .name = \u0026#34;pinctrl\u0026#34;, }; /* Declare a new uclass_driver */ #define UCLASS_DRIVER(__name)\t\\ ll_entry_declare(struct uclass_driver, __name, uclass)  #define ll_entry_declare(_type, _name, _list)\t\\ _type _u_boot_list_2_##_list##_2_##_name __aligned(4)\t\\ __attribute__((unused,\t\\ section(\u0026#34;.u_boot_list_2_\u0026#34;#_list\u0026#34;_2_\u0026#34;#_name))) 上面基本上就是我们的底层代码了，稍微有点绕，但是也不难！我们只需要将宏进行替换就行了！\n通过上面的定义，我们替换掉宏之后，最终得到的定义如下：\nstruct uclass_driver _u_boot_list_2_uclass_2_pinctrl = { .id = UCLASS_PINCTRL, .post_bind = pinctrl_post_bind, .flags = DM_UC_FLAG_SEQ_ALIAS, .name = \u0026#34;pinctrl\u0026#34;, } //同时存放在段._u_boot_list_2_uclass_2_pinctrl中，也就是section段的内容   存放 #    由上面结构体可得，其定义之后都被存放在了段._u_boot_list_2_uclass_2_pinctrl中，那么去哪里可以看到呢？\n在u-boot.map文件中搜索，._u_boot_list_2_uclass_2_pinctrl，就可以查到程序中定义的所有驱动程序。\n这里相信大家会有疑问，为什么是uclass_2呢？我们大概看一下，也会看到uclass_1和uclass_3，这两个代表什么呢？往下看！\n  相关API #     想要获取uclass_driver需要先获取uclass_driver table。\n struct uclass_driver *uclass = ll_entry_start(struct uclass_driver, uclass); // 会根据.u_boot_list_2_uclass_1的段地址来得到uclass_driver table的地址  const int n_ents = ll_entry_count(struct uclass_driver, uclass); // 获得uclass_driver table的长度  struct uclass_driver *lists_uclass_lookup(enum uclass_id id) // 从uclass_driver table中获取uclass id为id的uclass_driver。 正如注释描述，上文中提到的uclass_1和uclass_3起到定位作用，用于计算uclass_2的长度！\n上述的API，主要用于根据uclass_id来查找到对应的uclass_driver，进而操作对应的uclass下的udevice。\n④ uclass_id #  我们在uclass_driver中，看到一个uclass_id类型，这种类型与uclass有什么关系呢？\n我们知道，uclass代表驱动的一个类别，uclass_driver是uclass的驱动程序，为uclass提供统一操作接口。而对于不同类型的驱动，就需要uclass_id来区分了！\n事实上，每一种类型的设备uclass都有唯一对应的uclass_id，贯穿设备模型，也是udevice与uclass相关联的关键之处。\nenum uclass_id { /* These are used internally by driver model */ UCLASS_ROOT = 0, UCLASS_DEMO, UCLASS_TEST, UCLASS_TEST_FDT, UCLASS_TEST_BUS, UCLASS_TEST_PROBE, ...... /* U-Boot uclasses start here - in alphabetical order */ UCLASS_ACPI_PMC,\t/* (x86) Power-management controller (PMC) */ UCLASS_ADC,\t/* Analog-to-digital converter */ UCLASS_AHCI,\t/* SATA disk controller */ UCLASS_AUDIO_CODEC,\t/* Audio codec with control and data path */ UCLASS_AXI,\t/* AXI bus */ UCLASS_BLK,\t/* Block device */ UCLASS_BOARD,\t/* Device information from hardware */ ...... }; 在这里，我们就把他当作一个设备识别的标志即可！\n 最后，压轴的两个结构体出来了，也是DM模型最终操作的对象。\n ⑤ udevice #  /** * struct udevice - An instance of a driver * * This holds information about a device, which is a driver bound to a * particular port or peripheral (essentially a driver instance). * */ struct udevice { const struct driver *driver;\t//device 对应的driver  const char *name;\t//device 的名称  void *platdata; void *parent_platdata; void *uclass_platdata; ofnode node;\t//设备树节点  ulong driver_data; struct udevice *parent;\t//父设备  void *priv;\t// 私有数据的指针  struct uclass *uclass;\t//驱动所属的uclass  void *uclass_priv; void *parent_priv; struct list_head uclass_node; struct list_head child_head; struct list_head sibling_node; uint32_t flags; int req_seq; int seq; #ifdef CONFIG_DEVRES  struct list_head devres_head; #endif };   定义 #   **硬编码：**代码中调用U_BOOT_DEVICE宏来定义设备资源，实际为一个设备实例。 **设备树：**将设备描述信息写在对应的DTS文件中，然后编译成DTB，最终由uboot解析设备树后动态生成的。 传参方式：通过命令行或者接口将设备资源信息传递进来，非常灵活。    存放 #    udevice是最基础的一个设备单元，我们把它作为一个独立的个体，上层所有的操作，最终都与该结构体有关。\n我们创建一个设备后，为了服从统一的管理，该结构体会被连接到DM模型下，并入到机制中。那么udevice会被连接到哪里呢？\n 将udevice连接到对应的uclass中，uclass主要用来管理着同一类的驱动 除此之外，有父子关系的udevice，还会连接到udevice-\u0026gt;child_head链表下，方便调用  大概可以理解为下面这样：\n  相关API #    #define uclass_foreach_dev(pos, uc) \\ list_for_each_entry(pos, \u0026amp;uc-\u0026gt;dev_head, uclass_node)  #define uclass_foreach_dev_safe(pos, next, uc) \\ list_for_each_entry_safe(pos, next, \u0026amp;uc-\u0026gt;dev_head, uclass_node)  int uclass_get_device(enum uclass_id id, int index, struct udevice **devp); // 通过索引从uclass中获取udevice int uclass_get_device_by_name(enum uclass_id id, const char *name, // 通过设备名从uclass中获取udevice  struct udevice **devp); int uclass_get_device_by_seq(enum uclass_id id, int seq, struct udevice **devp); int uclass_get_device_by_of_offset(enum uclass_id id, int node, struct udevice **devp); int uclass_get_device_by_phandle(enum uclass_id id, struct udevice *parent, const char *name, struct udevice **devp); int uclass_first_device(enum uclass_id id, struct udevice **devp); int uclass_first_device_err(enum uclass_id id, struct udevice **devp); int uclass_next_device(struct udevice **devp); int uclass_resolve_seq(struct udevice *dev); 这些相关的API，主要作用就是根据uclass_id，查找对应的uclass，然后根据索引值或者名称，来查找到对应的udevice\n③ driver #  struct driver { char *name;\t//驱动名称  enum uclass_id id;\t//驱动所对应的uclass_id\t const struct udevice_id *of_match;\t//匹配函数  int (*bind)(struct udevice *dev);\t//绑定函数  int (*probe)(struct udevice *dev);\t//注册函数  int (*remove)(struct udevice *dev); int (*unbind)(struct udevice *dev); int (*ofdata_to_platdata)(struct udevice *dev); int (*child_post_bind)(struct udevice *dev); int (*child_pre_probe)(struct udevice *dev); int (*child_post_remove)(struct udevice *dev); int priv_auto_alloc_size; int platdata_auto_alloc_size; int per_child_auto_alloc_size; int per_child_platdata_auto_alloc_size; const void *ops;\t/* driver-specific operations */ uint32_t flags; #if CONFIG_IS_ENABLED(ACPIGEN)  struct acpi_ops *acpi_ops; #endif };   定义 #    driver对象，主要通过U_BOOT_DRIVER来定义\n 以pinctrl来举例\n U_BOOT_DRIVER(xxx_pinctrl) = { .name\t= \u0026#34;xxx_pinctrl\u0026#34;, .id\t= UCLASS_PINCTRL, .of_match\t= arobot_pinctrl_match, .priv_auto_alloc_size = sizeof(struct xxx_pinctrl), .ops\t= \u0026amp;arobot_pinctrl_ops, .probe\t= arobot_v2s_pinctrl_probe, .remove = arobot_v2s_pinctrl_remove, }; /* Declare a new U-Boot driver */ #define U_BOOT_DRIVER(__name)\t\\ ll_entry_declare(struct driver, __name, driver)  #define ll_entry_declare(_type, _name, _list)\t\\ _type _u_boot_list_2_##_list##_2_##_name __aligned(4)\t\\ __attribute__((unused,\t\\ section(\u0026#34;.u_boot_list_2_\u0026#34;#_list\u0026#34;_2_\u0026#34;#_name))) 通过上面的定义，最终我们定义的结构体如下：\nstruct driver _u_boot_list_2_driver_2_xxx_pinctrl = { .name\t= \u0026#34;xxx_pinctrl\u0026#34;, .id\t= UCLASS_PINCTRL, .of_match\t= arobot_pinctrl_match, .priv_auto_alloc_size = sizeof(struct xxx_pinctrl), .ops\t= \u0026amp;arobot_pinctrl_ops, .probe\t= arobot_v2s_pinctrl_probe, .remove = arobot_v2s_pinctrl_remove, } //同时存放在段._u_boot_list_2_driver_2_xxx_pinctrl中   存放 #    由上面结构体可得，其定义之后都被存放在了段._u_boot_list_2_driver_2_xxx中，那么去哪里可以看到呢？\n在u-boot.map文件中搜索，._u_boot_list_2_driver，就可以查到程序中定义的所有驱动程序。\n最终，所有driver结构体以列表的形式被放在.u_boot_list_2_driver_1和.u_boot_list_2_driver_3的区间中。\n  相关API #    /*先获取driver table 表*/ struct driver *drv = ll_entry_start(struct driver, driver);\t// 会根据.u_boot_list_2_driver_1的段地址来得到uclass_driver table的地址  const int n_ents = ll_entry_count(struct driver, driver);\t// 通过.u_boot_list_2_driver_3的段地址 减去 .u_boot_list_2_driver_1的段地址 获得driver table的长度  /*遍历所有的driver*/ struct driver *lists_driver_lookup_name(const char *name)\t// 从driver table中获取名字为name的driver。 正如注释描述，上文中提到的driver_1和driver_3起到定位作用，用于计算driver_2的长度！\n上述的API，主要用于根据name来查找到对应的driver驱动程序。\n综上，DM模型相关的数据结构介绍完毕，整体设计的架构如下：\n正如红线部分，如何实现driver和udevice的绑定、uclass、uclass_driver的绑定呢？\n要想真正搞懂这些，我们不得不去深入到DM的初始化流程。\n3.5、DM驱动模型之上帝视角 #   对于DM模型，我们站在上帝视角来观察整套模型框架是如何的！\n 从对象设计的角度来看，Uboot的驱动模型可以分为静态形式和动态形式。\n  **静态模式：**对象是离散的，和其他对象分隔开，减小对象复杂度，利于模块化设计。\n  动态模式：运行态表达形式的对象是把所有的静态对象组合成层次视图，有清晰的数据关联视图\n  在静态模式下，驱动模型主要将对象分为udevice和driver，即设备和驱动程序，两个就像火车的两条轨道，永远也不会产生交集，驱动和设备可以想注册多少就注册多少。\n我们看一下udevice的描述：\n/** * struct udevice - An instance of a driver * * This holds information about a device, which is a driver bound to a * particular port or peripheral (essentially a driver instance). * */ udevice是driver的一个实例，两个不相交的铁轨，终归也是想要发生爱情的。那么如何让其产生交集呢？这就是动态模式需要做的工作了！\n**在动态模式下，**引入了uclass和uclass_driver两个数据结构，实现了对udevice和driver的管理。\n看一下uclass和uclass_driver两个结构体的说明：\n/** * struct uclass - a U-Boot drive class, collecting together similar drivers * */ /** * struct uclass_driver - Driver for the uclass * * A uclass_driver provides a consistent interface to a set of related * drivers. * */  **uclass：**设备组公共属性对象，作为udevice的一个属性，主要用来管理某个驱动类的所有的设备。 **uclass_driver：**设备组公共行为对象，uclass的驱动程序，主要将uclass管理的设备和驱动实现绑定、注册，移除等操作。  通过这两个结构体的引入，可以将毫不相关的udevice是driver关联起来！\nudevice与driver的绑定：通过驱动的of_match和compatible属性来配对，绑定。\nudevice与uclass的绑定：udevice内的driver下的uclass_id，来与uclass对应的uclass_driver的uclass_id进行匹配。\nuclass与uclass_driver的绑定：已知udevice内的driver下的uclass_id，创建uclass的同时，通过``uclass_id找到对应的uclass_driver对象，然后将uclass_driver绑定到uclass`上！\n整体结构如下：\n3.6、DM模型——Udevice与driver绑定 #   相信站在上帝视角看完DM的整体架构，大家都对DM框架有一定了解，下面我们来看看具体的实现细节！\n DM的初始化分为两个部分，一个是在relocate重定向之前的初始化：initf_dm，一个是在relocate重定向之后的初始化：initr_dm。\n我们对比这两个函数：\nstatic int initf_dm(void) { #if defined(CONFIG_DM) \u0026amp;\u0026amp; CONFIG_VAL(SYS_MALLOC_F_LEN)  int ret; bootstage_start(BOOTSTAGE_ID_ACCUM_DM_F, \u0026#34;dm_f\u0026#34;); ret = dm_init_and_scan(true);\t//这里为true  bootstage_accum(BOOTSTAGE_ID_ACCUM_DM_F); if (ret) return ret; #endif #ifdef CONFIG_TIMER_EARLY  ret = dm_timer_init(); if (ret) return ret; #endif  return 0; } static int initr_dm(void) { int ret; /* Save the pre-reloc driver model and start a new one */ gd-\u0026gt;dm_root_f = gd-\u0026gt;dm_root; gd-\u0026gt;dm_root = NULL; #ifdef CONFIG_TIMER  gd-\u0026gt;timer = NULL; #endif  bootstage_start(BOOTSTAGE_ID_ACCUM_DM_R, \u0026#34;dm_r\u0026#34;); ret = dm_init_and_scan(false);\t//这里为false  bootstage_accum(BOOTSTAGE_ID_ACCUM_DM_R); if (ret) return ret; return 0; } 两个均调用了dm_init_and_scan这个接口，这两个的关键区别在于参数的不同。\n 首先说明一下dts节点中的“u-boot,dm-pre-reloc”属性，当设置了这个属性时，则表示这个设备在relocate之前就需要使用。 当dm_init_and_scan的参数为true时，只会对带有“u-boot,dm-pre-reloc”属性的节点进行解析。而当参数为false的时候，则会对所有节点都进行解析。  DM初始化的大体步骤如下：\n 如上程序执行流程图，下面我们详细讲解几个函数。\n ① dm_init #  int dm_init(bool of_live) { int ret; if (gd-\u0026gt;dm_root) { dm_warn(\u0026#34;Virtual root driver already exists!\\n\u0026#34;); return -EINVAL; } INIT_LIST_HEAD(\u0026amp;DM_UCLASS_ROOT_NON_CONST); #if defined(CONFIG_NEEDS_MANUAL_RELOC)  fix_drivers(); fix_uclass(); fix_devices(); #endif  ret = device_bind_by_name(NULL, false, \u0026amp;root_info, \u0026amp;DM_ROOT_NON_CONST);\t//查找root_driver驱动，并绑定  if (ret) return ret; #if CONFIG_IS_ENABLED(OF_CONTROL) # if CONFIG_IS_ENABLED(OF_LIVE)  if (of_live) DM_ROOT_NON_CONST-\u0026gt;node = np_to_ofnode(gd-\u0026gt;of_root); else #endif  DM_ROOT_NON_CONST-\u0026gt;node = offset_to_ofnode(0); #endif  ret = device_probe(DM_ROOT_NON_CONST);\t//probe激活root_driver驱动  if (ret) return ret; return 0; } dm_init这个函数，名字起的容易让人误导，这个函数主要做的就是初始化了根设备root_driver，根据这个跟设备，初始化了global_data中的dm_root、uclass_root。\n② lists_bind_fdt #   我们通常会使用设备树来定义各种设备，所以这个函数才是主角。\n 这个函数主要用来查找子设备，并且根据查找到的子设备，进而查找对应驱动进行绑定！即：实现了driver和device的绑定。\nint lists_bind_fdt(struct udevice *parent, ofnode node, struct udevice **devp, bool pre_reloc_only) { struct driver *driver = ll_entry_start(struct driver, driver);\t//获得驱动列表的起始地址  const int n_ents = ll_entry_count(struct driver, driver);\t//获得驱动列表的总数量  const struct udevice_id *id; struct driver *entry; struct udevice *dev; bool found = false; const char *name, *compat_list, *compat; int compat_length, i; int result = 0; int ret = 0; if (devp) *devp = NULL; name = ofnode_get_name(node); log_debug(\u0026#34;bind node %s\\n\u0026#34;, name); compat_list = ofnode_get_property(node, \u0026#34;compatible\u0026#34;, \u0026amp;compat_length);\t//得到compatible属性，用于匹配driver驱动  if (!compat_list) { if (compat_length == -FDT_ERR_NOTFOUND) { log_debug(\u0026#34;Device \u0026#39;%s\u0026#39; has no compatible string\\n\u0026#34;, name); return 0; } dm_warn(\u0026#34;Device tree error at node \u0026#39;%s\u0026#39;\\n\u0026#34;, name); return compat_length; } /* * Walk through the compatible string list, attempting to match each * compatible string in order such that we match in order of priority * from the first string to the last. */ for (i = 0; i \u0026lt; compat_length; i += strlen(compat) + 1) { compat = compat_list + i; log_debug(\u0026#34; - attempt to match compatible string \u0026#39;%s\u0026#39;\\n\u0026#34;, compat); for (entry = driver; entry != driver + n_ents; entry++) {\t//循环判断所有驱动是否匹配\t ret = driver_check_compatible(entry-\u0026gt;of_match, \u0026amp;id, compat); if (!ret) break; } if (entry == driver + n_ents) continue; if (pre_reloc_only) { if (!ofnode_pre_reloc(node) \u0026amp;\u0026amp; !(entry-\u0026gt;flags \u0026amp; DM_FLAG_PRE_RELOC)) { log_debug(\u0026#34;Skipping device pre-relocation\\n\u0026#34;); return 0; } } log_debug(\u0026#34; - found match at \u0026#39;%s\u0026#39;: \u0026#39;%s\u0026#39; matches \u0026#39;%s\u0026#39;\\n\u0026#34;, entry-\u0026gt;name, entry-\u0026gt;of_match-\u0026gt;compatible, id-\u0026gt;compatible); ret = device_bind_with_driver_data(parent, entry, name, id-\u0026gt;data, node, \u0026amp;dev);\t//该函数，用于创建udevice对象，并与查找到的driver绑定  if (ret == -ENODEV) { log_debug(\u0026#34;Driver \u0026#39;%s\u0026#39; refuses to bind\\n\u0026#34;, entry-\u0026gt;name); continue; } if (ret) { dm_warn(\u0026#34;Error binding driver \u0026#39;%s\u0026#39;: %d\\n\u0026#34;, entry-\u0026gt;name, ret); return ret; } else { found = true; if (devp) *devp = dev; } break; } if (!found \u0026amp;\u0026amp; !result \u0026amp;\u0026amp; ret != -ENODEV) log_debug(\u0026#34;No match for node \u0026#39;%s\u0026#39;\\n\u0026#34;, name); return result; } lists_bind_fdt这个函数，主要用来扫描设备树中的各个节点；\n根据扫描到的udevice设备信息，通过compatible来匹配compatible相同的driver，匹配成功后，就会创建对应的struct udevice结构体，它会同时指向设备资源和driver，这样设备资源和driver就绑定在一起了。\n3.7、DM模型——probe探测函数的执行 #   上述，完成了DM模型的初始化，但是我们只是建立了driver和udevice的绑定关系，那么何时调用到我们驱动中的probe探测函数呢？uclass与driver又何时匹配的呢？\n 上文呢，dm_init只是负责初始化并绑定了udevice和driver，那么probe探测函数的执行，当然是在该驱动初始化的时候喽！\n 下文以mmc驱动为例！其初始化流程如下：\n 详细代码在这里就不展开来叙述了！\n在MMC驱动初始化后，有没有注意到mmc_probe这个函数，该函数就是间接调用了我们驱动编写的probe函数。\n执行流程在上面已经很清楚了：根据uclass_id，调用``uclass_get_device_by_seq来得到udevice，进而调用device_probe来找到对应驱动的probe`。\nint device_probe(struct udevice *dev) { const struct driver *drv; int ret; int seq; if (!dev) return -EINVAL; if (dev-\u0026gt;flags \u0026amp; DM_FLAG_ACTIVATED) return 0; drv = dev-\u0026gt;driver;\t//获取driver  assert(drv); ret = device_ofdata_to_platdata(dev); if (ret) goto fail; /* Ensure all parents are probed */ if (dev-\u0026gt;parent) {\t//父设备probe  ret = device_probe(dev-\u0026gt;parent); if (ret) goto fail; /* * The device might have already been probed during * the call to device_probe() on its parent device * (e.g. PCI bridge devices). Test the flags again * so that we don\u0026#39;t mess up the device. */ if (dev-\u0026gt;flags \u0026amp; DM_FLAG_ACTIVATED) return 0; } seq = uclass_resolve_seq(dev); if (seq \u0026lt; 0) { ret = seq; goto fail; } dev-\u0026gt;seq = seq; dev-\u0026gt;flags |= DM_FLAG_ACTIVATED; /* * Process pinctrl for everything except the root device, and * continue regardless of the result of pinctrl. Don\u0026#39;t process pinctrl * settings for pinctrl devices since the device may not yet be * probed. */ if (dev-\u0026gt;parent \u0026amp;\u0026amp; device_get_uclass_id(dev) != UCLASS_PINCTRL) pinctrl_select_state(dev, \u0026#34;default\u0026#34;); if (CONFIG_IS_ENABLED(POWER_DOMAIN) \u0026amp;\u0026amp; dev-\u0026gt;parent \u0026amp;\u0026amp; (device_get_uclass_id(dev) != UCLASS_POWER_DOMAIN) \u0026amp;\u0026amp; !(drv-\u0026gt;flags \u0026amp; DM_FLAG_DEFAULT_PD_CTRL_OFF)) { ret = dev_power_domain_on(dev); if (ret) goto fail; } ret = uclass_pre_probe_device(dev); if (ret) goto fail; if (dev-\u0026gt;parent \u0026amp;\u0026amp; dev-\u0026gt;parent-\u0026gt;driver-\u0026gt;child_pre_probe) { ret = dev-\u0026gt;parent-\u0026gt;driver-\u0026gt;child_pre_probe(dev); if (ret) goto fail; } /* Only handle devices that have a valid ofnode */ if (dev_of_valid(dev)) { /* * Process \u0026#39;assigned-{clocks/clock-parents/clock-rates}\u0026#39; * properties */ ret = clk_set_defaults(dev, 0); if (ret) goto fail; } if (drv-\u0026gt;probe) {\tret = drv-\u0026gt;probe(dev);\t//调用驱动的probe  if (ret) goto fail; } ret = uclass_post_probe_device(dev); if (ret) goto fail_uclass; if (dev-\u0026gt;parent \u0026amp;\u0026amp; device_get_uclass_id(dev) == UCLASS_PINCTRL) pinctrl_select_state(dev, \u0026#34;default\u0026#34;); return 0; fail_uclass: if (device_remove(dev, DM_REMOVE_NORMAL)) { dm_warn(\u0026#34;%s: Device \u0026#39;%s\u0026#39; failed to remove on error path\\n\u0026#34;, __func__, dev-\u0026gt;name); } fail: dev-\u0026gt;flags \u0026amp;= ~DM_FLAG_ACTIVATED; dev-\u0026gt;seq = -1; device_free(dev); return ret; } 主要工作归纳如下：\n 根据udevice获取driver 然后判断是否父设备被probe 对父设备进行probe 调用driver的probe函数  3.8、DM模型——uclass与uclass_driver绑定 #   上述完成了driver的probe函数调用，基本底层都已经准备好了，uclass何时与uclass_driver绑定，给上层提供统一的API呢？\n uclass与uclass_driver绑定，也是在驱动probe之后，确保该驱动存在，设备存在，最后为该驱动绑定uclass与uclass_driver，为上层提供统一接口。\n 以根据MMC驱动为例\n 回到上文的驱动流程图，看到mmc_do_preinit这个函数了嘛？里面调用了ret = uclass_get(UCLASS_MMC, \u0026amp;uc);，该函数才是真正的将uclass与uclass_driver绑定。\nint uclass_get(enum uclass_id id, struct uclass **ucp) { struct uclass *uc; *ucp = NULL; uc = uclass_find(id); if (!uc) return uclass_add(id, ucp); *ucp = uc; return 0; } uclass_get主要实现了：根据uclass_id查找对应的uclass是否被添加到global_data-\u0026gt;uclass_root链表中，如果没有添加到，就调用uclass_add函数，实现uclass与uclass_driver的绑定，并将其添加到global_data-\u0026gt;uclass_root链表中。\nstatic int uclass_add(enum uclass_id id, struct uclass **ucp) { struct uclass_driver *uc_drv; struct uclass *uc; int ret; *ucp = NULL; uc_drv = lists_uclass_lookup(id);\t//根据uclass_id查找到对应的driver  if (!uc_drv) { debug(\u0026#34;Cannot find uclass for id %d: please add the UCLASS_DRIVER() declaration for this UCLASS_... id\\n\u0026#34;, id); /* * Use a strange error to make this case easier to find. When * a uclass is not available it can prevent driver model from * starting up and this failure is otherwise hard to debug. */ return -EPFNOSUPPORT; } uc = calloc(1, sizeof(*uc)); if (!uc) return -ENOMEM; if (uc_drv-\u0026gt;priv_auto_alloc_size) { uc-\u0026gt;priv = calloc(1, uc_drv-\u0026gt;priv_auto_alloc_size); if (!uc-\u0026gt;priv) { ret = -ENOMEM; goto fail_mem; } } uc-\u0026gt;uc_drv = uc_drv;\t//uclass与uclass_driver绑定  INIT_LIST_HEAD(\u0026amp;uc-\u0026gt;sibling_node); INIT_LIST_HEAD(\u0026amp;uc-\u0026gt;dev_head); list_add(\u0026amp;uc-\u0026gt;sibling_node, \u0026amp;DM_UCLASS_ROOT_NON_CONST);\t//添加到global_data-\u0026gt;uclass_root链表中  if (uc_drv-\u0026gt;init) { ret = uc_drv-\u0026gt;init(uc); if (ret) goto fail; } *ucp = uc; return 0; fail: if (uc_drv-\u0026gt;priv_auto_alloc_size) { free(uc-\u0026gt;priv); uc-\u0026gt;priv = NULL; } list_del(\u0026amp;uc-\u0026gt;sibling_node); fail_mem: free(uc); return ret; } 好啦，到这里基本就把Uboot的DM模型全部理清楚啦，耗时一个周，总感觉想要自己去讲明白，真的不是一件容易的事情呢！\n如果对你们有帮助，记得点个赞哦！\n3.9 参考文档 #  [1] : https://www.dazhuanlan.com/archevalier/topics/1323360\n[2] : https://www.cnblogs.com/gs1008612/p/8253213.html\n[3] : https://blog.csdn.net/kunkliu/article/details/103168591\n[4] : https://blog.csdn.net/ooonebook/article/details/53234020\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":21,"href":"/docs/linux/linux_memory_manage/%E4%B8%89%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E7%AE%A1%E7%90%86/","title":"三、虚拟地址空间管理","section":"Linux 内存管理","content":"Linux内存管理 | 三、虚拟地址空间管理 #  上一节，我们主要了解了虚拟内存空间的布局情况，趁热打铁，我们直接从源代码的视角，来看一下Linux内核是如何管理虚拟内存空间的。\n废话不多说，直接开始！\n1、用户态空间管理 #  读完上一节我们知道，用户态的布局情况如下：\n我们运行的可执行程序，被加载进内存后，会作为一个进程存在，这个进程Linux内核会将其抽象成一个结构体。没错，它就是task_struct。\n1.1 task_struct结构体 #  task_struct结构体是进程的抽象，进程所涉及到的内容非常多，下面只列举出一些重要的数据结构，方面理解。\n// include/linux/sched.h struct task_struct { ... pid_t\tpid;\t//\t进程PID  pid_t\ttgid;\t//\t线程PID  struct files_struct\t*files;\t// 进程打开的文件信息  struct mm_struct\t*mm;\t//\t进程虚拟内存空间的内存描述符  ... } 如上，进程抽象为task_struct结构体，通过mm_struct结构体来管理虚拟内存空间。\n1.2 mm_struct结构体 #  每个进程都有唯一的 mm_struct 结构体，也就是前边提到的每个进程的虚拟地址空间都是独立，互不干扰的。\nmm_struct的结构体如下：\n//\tinclude/linux/mm_types.h struct mm_struct { ... struct { ... unsigned long task_size;\t/* size of task vm space */ ... unsigned long mmap_base;\t/* base of mmap area */ unsigned long total_vm;\t/* Total pages mapped */ unsigned long locked_vm;\t/* Pages that have PG_mlocked set */ unsigned long pinned_vm;\t/* Refcount permanently increased */ unsigned long data_vm;\t/* VM_WRITE \u0026amp; ~VM_SHARED \u0026amp; ~VM_STACK */ unsigned long exec_vm;\t/* VM_EXEC \u0026amp; ~VM_WRITE \u0026amp; ~VM_STACK */ unsigned long stack_vm;\t/* VM_STACK */ unsigned long start_code, end_code, start_data, end_data; unsigned long start_brk, brk, start_stack; unsigned long arg_start, arg_end, env_start, env_end; ... struct vm_area_struct *mmap;\t/* list of VMAs */ struct rb_root mm_rb; ... }__randomize_layout; ... }  1.3 内核态和用户态的划分 #  mm_struct里面定义的task_size变量，就是用来划分虚拟内存的用户空间和内核空间的。\nunsigned long task_size; task_size也就是两者的分界线，下面我们看下task_size是如何被赋值的。\n 当我们执行一个新的进程的时候，Linux内核会执行load_elf_binary的API接口，进而调用setup_new_exec函数来实现新进程的创建。\n在setup_new_exec函数中，会执行\ncurrent-\u0026gt;mm-\u0026gt;task_size = TASK_SIZE; 这个TASK_SIZE就是我们设置的内核空间地址和用户空间地址的分界线，由我们自定义配置。\n#ifdef CONFIG_X86_32 /* * User space process size: 3GB (default). */ #define TASK_SIZE\tPAGE_OFFSET #define TASK_SIZE_MAX\tTASK_SIZE /* config PAGE_OFFSET hex default 0xC0000000 depends on X86_32 */ #else /* * User space process size. 47bits minus one guard page. */ #define TASK_SIZE_MAX\t((1UL \u0026lt;\u0026lt; 47) - PAGE_SIZE) #define TASK_SIZE\t(test_thread_flag(TIF_ADDR32) ? \\ IA32_PAGE_OFFSET : TASK_SIZE_MAX) ...... 这里我们只需要知道TASK_SIZE默认值3为PAGE_OFFSET，并且默认为0xC0000000为分界线的，即用户空间3GB，内核空间1GB；当然这个可以由我们动态配置，可以配置PAGE_OFFSET为0x80000000，即用户空间和内核空间均为2GB，取决于我们的应用场合，当你看到与我们讲解不同时，也不用大惊小怪。\n 以上，表达的概念很简单，如下图：\n  1.4 位置信息描述 #  我们知道用户态内存空间分为几个区域：代码段、数据段、BSS段、堆、文件映射和匿名映射区、栈等几个部分，同样在mm_struct中，定义了这些区域的统计信息和位置。\nunsigned long mmap_base;\t/* base of mmap area */ unsigned long total_vm;\t/* Total pages mapped */ unsigned long locked_vm;\t/* Pages that have PG_mlocked set */ unsigned long pinned_vm;\t/* Refcount permanently increased */ unsigned long data_vm;\t/* VM_WRITE \u0026amp; ~VM_SHARED \u0026amp; ~VM_STACK */ unsigned long exec_vm;\t/* VM_EXEC \u0026amp; ~VM_WRITE \u0026amp; ~VM_STACK */ unsigned long stack_vm;\t/* VM_STACK */ unsigned long start_code, end_code, start_data, end_data; unsigned long start_brk, brk, start_stack; unsigned long arg_start, arg_end, env_start, env_end;  total_vm：总映射页面的数目。（这么大的虚拟内存空间，不可能全部映射到真实的物理内存，都是按需映射的，这里表示当前映射的页面总数目）   由于物理内存比较小，当内存吃紧的时候，就会发生换入换出的操作，即将暂时不用的页面换出到硬盘上，有的页面比较重要，不能换出。\n  locked_vm：被锁定不能换出的页面 pinned_vm ：不能换出、也不能移动的页面 data_vm：存放数据页的页的数目 exec_vm：存放可执行文件的页的数目 stack_vm：存放堆栈信息页的数目 start_code、end_code：表示可执行代码开始和结束的位置 start_data、end_data：表示已初始化数据的开始位置和结束位置 start_brk、brk：堆的起始地址，结束地址 start_stack：是栈的起始位置，在 RBP 寄存器中存储，栈的结束位置也就是栈顶指针，在 RSP 寄存器中存储。在栈中内存地址的增长方向也是由高地址向低地址增长。 arg_start、arg_end：参数列表的起始位置和结束位置 env_start、env_end：环境变量的起始位置和结束位置   整体的布局情况如下：\n 1.5 区域属性描述 #  尽管已经有了一些变量来描述每一个段的信息，但是Linux内核在mm_struct结构体里面，还有一个专门的数据结构vm_area_struct来管理每个区域的属性。\nstruct vm_area_struct *mmap;\t/* list of VMAs */ struct rb_root mm_rb; mmap：为一个单链表，将所有的区域串联起来\nmm_rb：为一个红黑树，方便查找和修改内存区域。\n 下面看一下vm_area_struct数据结构：\nstruct vm_area_struct { /* The first cache line has the info for VMA tree walking. */ unsigned long vm_start;\t/* Our start address within vm_mm. */ unsigned long vm_end;\t/* The first byte after our end address within vm_mm. */ /* linked list of VM areas per task, sorted by address */ struct vm_area_struct *vm_next, *vm_prev; struct rb_node vm_rb; struct mm_struct *vm_mm;\t/* The address space we belong to. */ struct list_head anon_vma_chain; /* Serialized by mmap_sem \u0026amp; * page_table_lock */ struct anon_vma *anon_vma;\t/* Serialized by page_table_lock */ /* Function pointers to deal with this struct. */ const struct vm_operations_struct *vm_ops; struct file * vm_file;\t/* File we map to (can be NULL). */ void * vm_private_data;\t/* was vm_pte (shared mem) */ } __randomize_layout;   vm_start、vm_end：为该区域在用户空间的起始和结束地址\n  vm_next、vm_prev：将该区域添加到链表上，便于管理。\n  vm_rb：将这个区域放到红黑树上\n  vm_ops：对该区域可以进行的内存操作\n  anon_vma：匿名映射\n  vm_file：文件映射\n   用户态空间的每个区域都由该结构体来管理，最终形成下面的这个结构：\n  顺便介绍一下 我的圈子：高级工程师聚集地，期待大家的加入。\n 2、内核态空间管理 #  上面，我们从源码角度了解了用户态空间管理，下面我们看内核态空间管理。\n回顾一下，我们内核态的布局情况是怎么样的呢，还记得吗？\n我们要知道：\n 内核态的虚拟空间和任何一个进程都没有关系，所有的进程看到的内核态虚拟空间都是一样的。 在内核态，我们直接操作的依旧是虚拟地址，而非物理地址 不同CPU结构下，内核态空间的布局格式是不变的，但是大小会有所调整，比如ARM和X86的大小空间有所不同。   内核态空间管理并不像用户态那样使用结构体来统一管理，而是直接使用宏来定义每个区域的分界线，\n 下面我们以x86架构来分析内核态空间的管理\n 2.1 分界线定义 #  /* * User space process size: 3GB (default). */ #define TASK_SIZE\tPAGE_OFFSET  /* PAGE_OFFSET - the virtual address of the start of the kernel image */ #define PAGE_OFFSET\t((unsigned long)__PAGE_OFFSET)  #define __PAGE_OFFSET\t__PAGE_OFFSET_BASE  #define __PAGE_OFFSET_BASE\t_AC(CONFIG_PAGE_OFFSET, UL)  config PAGE_OFFSET hex default 0xB0000000 if VMSPLIT_3G_OPT default 0x80000000 if VMSPLIT_2G default 0x78000000 if VMSPLIT_2G_OPT default 0x40000000 if VMSPLIT_1G default 0xC0000000 depends on X86_32 TASK_SIZE：内核态空间与用户态空间的分界线\nPAGE_OFFSET：该宏表示内核镜像起始的虚拟地址。\nCONFIG_PAGE_OFFSET：这个宏定义的值，根据实际情况自行设定，默认为0XC0000000，可以设置为0X80000000等。\n以上，TASK_SIZE就被定义为0XC0000000作为用户态空间和内核态空间的分界线，将4G虚拟内存分配为3G/1G结构。\n2.2 直接映射区定义 #  直接映射区是定义在PAGE_OFFSET和high_memory之间的区域。\n PAGE_OFFSET：表示内核镜像的起始地址，上文已经说明。 high_memory也是表示的就是896M这个值，表示高端内存的分界线。   顺便说明以下，TASK_SIZE和PAGE_OFFSET在不同架构下是不同的，在ARM架构下，两者并不相等，本文以X86架构为例\n 2.3 安全保护区定义 #  系统会在high_memory和VMALLOC_START之间预留8M的安全保护区，防止访问越界。\nVMALLOC_OFFSET表示的是内核动态映射区的偏移，也就是所谓的安全保护区。\n#define VMALLOC_START\t(((unsigned long)high_memory + VMALLOC_OFFSET) \u0026amp; ~(VMALLOC_OFFSET-1))  #define VMALLOC_OFFSET\t(8*1024*1024) 可以很清楚的看到VMALLOC_OFFSET定义了8M的空间，VMALLOC_START在high_memory基础上，偏移了VMALLOC_OFFSET 8M空间大小作为安全保护区，以防越界访问。\n2.3 动态映射区定义 #  VMALLOC_START和VMALLOC_END之间称为内核动态映射区。\n和用户态进程使用 malloc 申请内存一样，在这块动态映射区内核是使用 vmalloc 进行内存分配。\n#define VMALLOC_START\t(((unsigned long)high_memory + VMALLOC_OFFSET) \u0026amp; ~(VMALLOC_OFFSET-1))  #ifdef CONFIG_HIGHMEM # define VMALLOC_END\t(PKMAP_BASE - 2 * PAGE_SIZE) #else # define VMALLOC_END\t(LDT_BASE_ADDR - 2 * PAGE_SIZE) #endif  PKMAP_BASE：是永久映射区的起始地址。\nVMALLOC_END：在永久映射区的起始地址下，偏移2个PAGE_SIZE作为安全保护区。\n2.4 永久映射区定义 #  PKMAP_BASE 到 FIXADDR_START 的空间称为永久内核映射，在内核的这段虚拟地址空间中允许建立与物理高端内存的长期映射关系。\n 比如内核通过 alloc_pages() 函数在物理内存的高端内存中申请获取到的物理内存页，这些物理内存页可以通过调用 kmap 映射到永久映射区中。\n #define PKMAP_BASE\t\\ ((LDT_BASE_ADDR - PAGE_SIZE) \u0026amp; PMD_MASK)  #define LDT_BASE_ADDR\t\\ ((CPU_ENTRY_AREA_BASE - PAGE_SIZE) \u0026amp; PMD_MASK)  #define CPU_ENTRY_AREA_BASE\t\\ ((FIXADDR_TOT_START - PAGE_SIZE * (CPU_ENTRY_AREA_PAGES + 1)) \\ \u0026amp; PMD_MASK)  #define FIXADDR_TOT_START\t(FIXADDR_TOP - FIXADDR_TOT_SIZE)  #define FIXADDR_TOP\t((unsigned long)__FIXADDR_TOP)  #define FIXADDR_TOT_SIZE\t(__end_of_fixed_addresses \u0026lt;\u0026lt; PAGE_SHIFT)  unsigned long __FIXADDR_TOP = 0xfffff000; #define PMD_MASK\t(~(PMD_SIZE - 1))  #define PAGE_SHIFT\t12 #define PAGE_SIZE\t(_AC(1,UL) \u0026lt;\u0026lt; PAGE_SHIFT)  #define CPU_ENTRY_AREA_PAGES\t(NR_CPUS * 40)  #define FIXADDR_START\t(FIXADDR_TOP - FIXADDR_SIZE)   PKMAP_BASE：是永久映射区的起始地址，它经过一系列的计算得到，具体可以看上面的宏定义，我们大概了解就行了，不同体系结构的定义位置还不一样。 FIXADDR_START：是固定映射区的起始地址，也是永久映射区的结束地址。  2.5 固定映射区定义 #  FIXADDR_START到FIXADDR_TOP的空间称为固定映射区，主要用于满足特殊的需求。\n#define FIXADDR_TOP\t((unsigned long)__FIXADDR_TOP)  unsigned long __FIXADDR_TOP = 0xfffff000; 固定映射区中的虚拟地址，可以自由映射到物理内存的高端地址空间上，特点是其映射的虚拟地址是不变的，物理地址是可以改变的。\n2.6 临时映射区定义 #  最后FIXADDR_TOP到0xFFFFFFFF之间的区域称为临时映射区。\n 它主要用来做什么呢，网上举的一个例子，大家参考以下。\n 假设用户态的进程要映射一个文件到内存中，先要映射用户态进程空间的一段虚拟地址到物理内存，然后将文件内容写入这个物理内存供用户态进程访问。\n给用户态进程分配物理内存页可以通过 alloc_pages()，分配完毕后，按说将用户态进程虚拟地址和物理内存的映射关系放在用户态进程的页表中，就完事大吉了。这个时候，用户态进程可以通过用户态的虚拟地址，也即 0 至 3G 的部分，经过页表映射后访问物理内存，并不需要内核态的虚拟地址里面也划出一块来，映射到这个物理内存页。\n但是如果要把文件内容写入物理内存，这件事情要内核来干了，这就只好通过 kmap_atomic 做一个临时映射，写入物理内存完毕后，再 kunmap_atomic 来解映射即可。\n以上，就是内核态空间的布局以及管理。\n3、总结 #  该篇文章，主要从源码角度来了解用户态空间和内核态空间是如何管理的，挪用大佬的一个图片，结合上面所讲的，相信很快就能茅塞顿开。\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":22,"href":"/docs/linux/linux_kernel_lock/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3linux%E9%94%81%E6%9C%BA%E5%88%B6%E5%9B%9B%E8%87%AA%E6%97%8B%E9%94%81/","title":"【深入理解Linux锁机制】四、自旋锁","section":"Linux 内核锁详解","content":"【深入理解Linux内核锁】四、自旋锁 #   上两节主要讲解了中断屏蔽和原子操作，这两个作为最底层的操作，几乎在Linux内核中都不单独使用，下面我们来带大家了解一下常用的自旋锁！\n 1、什么是自旋锁？ #  自旋锁是一种典型的对临界资源进行互斥访问的手段。\n它的底层实现逻辑是：原子变量+判断检测。\n原子变量我们可以理解为一把锁，通过操作原子变量（锁）的状态，并对其进行判断，如果锁未被锁定，我们就继续往下执行；如果锁已经被锁定，我们就原地自旋，直到等到锁被打开。\n在ARM平台下，自旋锁的实现使用了ldrex、strex、以及内存屏障指令dmb、dsb、wfe、sev等。\n2、自旋锁思想 #   自旋锁主要针对于SMP或者单CPU但内核可抢占的情况，对于单CPU内核不可抢占的情况时，自旋锁退化为空操作。 自旋锁实际为忙等锁，当锁不可用时，CPU一直处于等待状态，直到该锁被释放。 自旋锁可能会导致内核死锁，当递归使用自旋锁时，则将该CPU锁死。 在多核SMP的情况下，任何一个核拿到了自旋锁，该核上的抢占调度也暂时禁止了，但是没有禁止另外一个核的抢占调度。 在自旋锁锁定期间，不能调用引起进程调度的函数，如copy_from_user()、copy_to_user()、kmalloc()和msleep()，否则会导致内核崩溃  3、自旋锁的定义及实现 #  3.1 API接口 #  //\t定义自旋锁 spinlock_t lock; //\t初始化自旋锁 spin_lock_init(\u0026amp;lock) //\t获得自旋锁 spin_lock(\u0026amp;lock)\t//\t获取自旋锁，如果立即获得锁，则直接返回，否则，自旋等待，直到锁被释放 spin_trylock(\u0026amp;lock)\t//\t尝试获取自旋锁，如果立即获得锁，返回true，否则直接返回false，不原地等待  //\t释放自旋锁 spin_unlock(\u0026amp;lock) 自旋锁保证了不受其他CPU或者单CPU内的抢占进程的干扰，但是对于临界区代码，仍然有可能会受到中断和底半部的影响。\n为了解决这种问题，我们就要使用自旋锁的衍生。\nspin_lock_irq() = spin_lock() + local_irq_disable()\t//\t获取自旋锁并关中断 spin_unlock_irq() = spin_unlock() + local_irq_enable()\t//\t释放自旋锁并开中断 spin_lock_irqsave() = spin_lock() + local_irq_save()\t//\t获取自旋锁并关中断，保存中断状态 spin_unlock_irqrestore() = spin_unlock() + local_irq_restore()//释放自旋锁，开中断并恢复中断状态 spin_lock_bh() = spin_lock() + local_bh_disable()\t//\t获取自旋锁并关底半部中断 spin_unlock_bh() = spin_unlock() + local_bh_enable()\t//\t释放自旋锁并发开底半部中断 当我们的临界区代码，有可能被进程或者中断访问时，就需要在进程上下文中，调用spin_lock_irqsave()、spin_unlock_irqrestore()，在中断上下文中调用spin_lock()、spin_unlock()，如下图：\n TODO：替换图片\n 3.2 API实现 #  3.2.1 结构体spinlock_t、raw_spinlock、arch_spinlock_t #  typedef struct spinlock { union { struct raw_spinlock rlock; #ifdef CONFIG_DEBUG_LOCK_ALLOC # define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))  struct { u6 __padding[LOCK_PADSIZE]; struct lockdep_map dep_map; }; #endif  }; } spinlock_t; typedef struct raw_spinlock { arch_spinlock_t raw_lock; #ifdef CONFIG_DEBUG_SPINLOCK  unsigned int magic, owner_cpu; void *owner; #endif #ifdef CONFIG_DEBUG_LOCK_ALLOC  struct lockdep_map dep_map; #endif } raw_spinlock_t; typedef struct { union { u32 slock; struct __raw_tickets { #ifdef __ARMEB__  u16 next; u16 owner; #else  u16 owner; u16 next; #endif  } tickets; }; } arch_spinlock_t; 结构体名称：spinlock_t、raw_spinlock、arch_spinlock_t\n文件位置：include/linux/spinlock.h、arch/arm/include/asm/spinlock_types.h\n主要作用：结构体层层嵌套，用于定义一个自旋锁。\n  slock：32位无符号整形数据，用于锁的控制\n  __raw_tickets：union类型，用于基于票证锁算法的自旋锁。\n owner ：表示当前持有自旋锁的线程的索引 next ：表示下一个等待获取自旋锁的线程的索引   每个线程进入代码段时，会尝试获取自旋锁，如果获取失败，它们会在锁的等待队列中排队。然后，等待队列中的线程会按照优先级顺序依次抢占锁的拥有权，直到某个线程成功获取自旋锁并执行完关键代码，释放锁资源为止。\n    这里使用的union联合体，其共享内存空间，其具体区别可看下面：\nstruct与union区别：https://blog.csdn.net/lishuo0204/article/details/118957959\n 3.2.2 spin_lock_init #  #define spin_lock_init(_lock)\t\\ do {\t\\ spinlock_check(_lock);\t\\ raw_spin_lock_init(\u0026amp;(_lock)-\u0026gt;rlock);\t\\ } while (0)  static __always_inline raw_spinlock_t *spinlock_check(spinlock_t *lock) { return \u0026amp;lock-\u0026gt;rlock; }} # define raw_spin_lock_init(lock)\t\\ do { *(lock) = __RAW_SPIN_LOCK_UNLOCKED(lock); } while (0)  #define __RAW_SPIN_LOCK_UNLOCKED(lockname)\t\\ (raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname)  #define __RAW_SPIN_LOCK_INITIALIZER(lockname)\t\\ {\t\\ .raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,\t\\ SPIN_DEBUG_INIT(lockname)\t\\ SPIN_DEP_MAP_INIT(lockname) }  #define __ARCH_SPIN_LOCK_UNLOCKED\t{ { 0 } } 函数名称：spin_lock_init\n文件位置：include/linux/spinlock.h\n主要作用：初始化自旋锁\n函数调用流程：\n// spin_lock_init spin_lock_init(include/linux/spinlock.h) |--\u0026gt; spinlock_check // 对锁进行检查，判断是否存在  |--\u0026gt; raw_spin_lock_init // 初始化锁  |--\u0026gt; __RAW_SPIN_LOCK_UNLOCKED(include/linux/spinlock_types.h) |--\u0026gt; __RAW_SPIN_LOCK_INITIALIZER // 将锁初始为__ARCH_SPIN_LOCK_UNLOCKED未上锁状态 上述函数主要通过宏定义给变量.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED赋值，初始化为0，即为未上锁的状态；并且提供了两个调试接口：CONFIG_DEBUG_SPINLOCK、CONFIG_DEBUG_LOCK_ALLOC，默认为关闭。\n这里面有个关于spinlock_check存在的意义的讨论，感兴趣的可以看一下：https://stackoverflow.com/questions/52551594/spinlock-initialization-function\n3.2.3 spin_lock #  static __always_inline void spin_lock(spinlock_t *lock) { raw_spin_lock(\u0026amp;lock-\u0026gt;rlock); } #define raw_spin_lock(lock)\t_raw_spin_lock(lock)  #ifndef CONFIG_INLINE_SPIN_LOCK void __lockfunc _raw_spin_lock(raw_spinlock_t *lock) { __raw_spin_lock(lock); } EXPORT_SYMBOL(_raw_spin_lock); #ifndef CONFIG_INLINE_SPIN_LOCK void __lockfunc _raw_spin_lock(raw_spinlock_t *lock) { __raw_spin_lock(lock); } EXPORT_SYMBOL(_raw_spin_lock); static inline void __raw_spin_lock(raw_spinlock_t *lock) { preempt_disable(); spin_acquire(\u0026amp;lock-\u0026gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock); } #define preempt_disable() \\ do { \\ preempt_count_inc(); \\ barrier(); \\ } while (0)  #define LOCK_CONTENDED(_lock, try, lock) \\ lock(_lock)  static inline void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock) { __acquire(lock); arch_spin_lock(\u0026amp;lock-\u0026gt;raw_lock); } static inline void arch_spin_lock(arch_spinlock_t *lock) { unsigned long tmp; u32 newval; arch_spinlock_t lockval; prefetchw(\u0026amp;lock-\u0026gt;slock); __asm__ __volatile__( \u0026#34;1:\tldrex\t%0, [%3]\\n\u0026#34; \u0026#34;\tadd\t%1, %0, %4\\n\u0026#34; \u0026#34;\tstrex\t%2, %1, [%3]\\n\u0026#34; \u0026#34;\tteq\t%2, #0\\n\u0026#34; \u0026#34;\tbne\t1b\u0026#34; : \u0026#34;=\u0026amp;r\u0026#34; (lockval), \u0026#34;=\u0026amp;r\u0026#34; (newval), \u0026#34;=\u0026amp;r\u0026#34; (tmp) : \u0026#34;r\u0026#34; (\u0026amp;lock-\u0026gt;slock), \u0026#34;I\u0026#34; (1 \u0026lt;\u0026lt; TICKET_SHIFT) : \u0026#34;cc\u0026#34;); while (lockval.tickets.next != lockval.tickets.owner) { wfe(); lockval.tickets.owner = READ_ONCE(lock-\u0026gt;tickets.owner); } smp_mb(); } 函数名称：spin_lock\n文件位置：include/linux/spinlock.h\n主要作用：用于在进程或线程首次尝试获取锁的时候进行自旋，不停地检查锁的状态，如果锁已经被其他进程或线程占用，则自旋等待，直到锁被释放。\n函数调用流程：\n// spin_lock spin_lock(include/linux/spinlock.h) |--\u0026gt; raw_spin_lock |--\u0026gt; _raw_spin_lock(include/linux/spinlock_api_smp.h) |--\u0026gt; __raw_spin_lock |--\u0026gt; __raw_spin_lock |--\u0026gt; preempt_disable |--\u0026gt; preempt_count_inc |--\u0026gt; barrier |--\u0026gt; spin_acquire |--\u0026gt; LOCK_CONTENDED |--\u0026gt; do_raw_spin_lock |--\u0026gt; arch_spin_lock(arch/arm/include/asm/spinlock.h) 实现流程：\n  preempt_disable(); 禁用内核抢占，确保当前 CPU 执行该代码时不会被其他进程或线程抢占。\n 其通过preempt_count_inc增加抢占计数器的值，通过抢占计数器来实现对任务的执行顺序进行管理。 通过内存屏障barrier来确保前面的操作完成后再继续执行后面的代码。    spin_acquire(\u0026amp;lock-\u0026gt;dep_map, 0, 0, _RET_IP_); 通过调用 spin_acquire() 函数获取自旋锁，用于保护共享资源不被两个、或者多个线程所修改。\n spin_acquire是lockdep工具的一部分，主要用于动态检测死锁。 lock-\u0026gt;dep_map是锁的依赖地图，_RET_IP_是调用者的返回地址。这两个参数都是用于lockdep的调试信息。 lockdep是一个强大的锁调试工具，它可以跟踪锁的所有获取和释放，并动态地检测可能的死锁情况。    LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock); 实际为调用do_raw_spin_lock函数来实现获取锁并自旋的操作。\n  下面为arch_spin_lock汇编代码分析：\n  prefetchw(\u0026amp;lock-\u0026gt;slock) 函数用于提前加载锁的地址到处理器缓存中，从而提高锁的获取效率。\n  ldrex %0, [%3] 用于以原子方式读取锁的值到寄存器 %0 中，%3 为锁的地址。\n  add %1, %0, %4用于将当前获取锁的 CPU 分配的新值加上原锁值 %0 以及%4 固定常量，结果存放在%1新值中。\n  strex %2, %1, [%3] 用于以原子方式将更新的值 %1 写入锁的地址所指定的内存位置，%2 为写入结果。\n 这里解释一下为什么要做add处理：\n由spinlock_t的结构体可知，是由联合体组成，并且可以通过owner和next两个字段访问，next在高16位，owner在低16位。\n这里上锁的操作，是将其值设置为1 \u0026lt;\u0026lt; TICKET_SHIFT，也就是高16位，即next字段设置为1，表示下一个等待获取自旋锁的线程的索引。\n   teq %2, #0：用于测试写入结果 %2 是否为0，如果为0，表示锁获取成功，反之则跳转到标签1b处执行。\n  当获取锁成功之后，程序会执行 while 循环，不断等待锁的所有权被赋予当前 CPU, 直到锁的所有权的拥有者持有锁为止。\n  smp_mb() 函数执行一条内存屏障，确保所有关键数据的顺序性已经刷新到内存中。\n  综上，spin_lock代码的作用是获取自旋锁，让当前线程获得临界资源的控制权，避免多个线程同时修改共享资源而造成数据冲突。同时，通过禁用内核抢占和使用内联函数优化的方式，保证了原子操作的执行效率和可靠性。\n3.2.4 spin_unlock #  static __always_inline void spin_unlock(spinlock_t *lock) { raw_spin_unlock(\u0026amp;lock-\u0026gt;rlock); } #define raw_spin_unlock(lock)\t_raw_spin_unlock(lock)  void __lockfunc _raw_spin_unlock(raw_spinlock_t *lock) { __raw_spin_unlock(lock); } static inline void __raw_spin_unlock(raw_spinlock_t *lock) { spin_release(\u0026amp;lock-\u0026gt;dep_map, 1, _RET_IP_); do_raw_spin_unlock(lock); preempt_enable(); } static inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock) { arch_spin_unlock(\u0026amp;lock-\u0026gt;raw_lock); __release(lock); } static inline void arch_spin_unlock(arch_spinlock_t *lock) { smp_mb(); lock-\u0026gt;tickets.owner++; dsb_sev(); } 函数名称：spin_unlock\n文件位置：include/linux/spinlock.h\n主要作用：该宏释放自旋锁lock，它与spin_trylock或spin_lock配对使用。\n函数调用流程：\n// spin_unlock spin_unlock(include/linux/spinlock.h) |--\u0026gt; raw_spin_unlock |--\u0026gt; _raw_spin_unlock(kernel/locking/spinlock.c) |--\u0026gt; __raw_spin_unlock(include/linux/spinlock_api_smp.h) |--\u0026gt; spin_release |--\u0026gt; do_raw_spin_unlock |--\u0026gt; arch_spin_unlock(arch/arm/include/asm/spinlock.h) |--\u0026gt; preempt_enable 下面为arch_spin_unlock代码分析：\n  smp_mb() 函数执行一条内存屏障，确保所有关键数据的顺序性已经刷新到内存中。\n  lock-\u0026gt;tickets.owner++：递增自旋锁的锁拥有者(Ower)计数器，这个计数器的作用是向其他任务表明自旋锁已经被释放。\n 通过上述两个接口可知：\n spin_lock：将next值加1，表示下一个等待获取自旋锁的线程 spin_unlock：将owner的值加1，指向下一个获取自旋锁的线程。     dsb_sev() 则用于内存同步，强制发射缓存条 (FLUSH操作)，确保数据在多个核之间进行同步，以避免潜在的错误和问题。\n  3.2.5 spin_trylock #  static __always_inline int spin_trylock(spinlock_t *lock) { return raw_spin_trylock(\u0026amp;lock-\u0026gt;rlock); } #define raw_spin_trylock(lock)\t__cond_lock(lock, _raw_spin_trylock(lock))  static inline int __raw_spin_trylock(raw_spinlock_t *lock) { preempt_disable(); if (do_raw_spin_trylock(lock)) { spin_acquire(\u0026amp;lock-\u0026gt;dep_map, 0, 1, _RET_IP_); return 1; } preempt_enable(); return 0; } static inline int do_raw_spin_trylock(raw_spinlock_t *lock) { return arch_spin_trylock(\u0026amp;(lock)-\u0026gt;raw_lock); } static inline int arch_spin_trylock(arch_spinlock_t *lock) { unsigned long contended, res; u32 slock; prefetchw(\u0026amp;lock-\u0026gt;slock); do { __asm__ __volatile__( \u0026#34;\tldrex\t%0, [%3]\\n\u0026#34; \u0026#34;\tmov\t%2, #0\\n\u0026#34; \u0026#34;\tsubs\t%1, %0, %0, ror #16\\n\u0026#34; \u0026#34;\taddeq\t%0, %0, %4\\n\u0026#34; \u0026#34;\tstrexeq\t%2, %0, [%3]\u0026#34; : \u0026#34;=\u0026amp;r\u0026#34; (slock), \u0026#34;=\u0026amp;r\u0026#34; (contended), \u0026#34;=\u0026amp;r\u0026#34; (res) : \u0026#34;r\u0026#34; (\u0026amp;lock-\u0026gt;slock), \u0026#34;I\u0026#34; (1 \u0026lt;\u0026lt; TICKET_SHIFT) : \u0026#34;cc\u0026#34;); } while (res); if (!contended) { smp_mb(); return 1; } else { return 0; } } 函数名称：spin_trylock\n文件位置：include/linux/spinlock.h\n主要作用：该宏尝试获得自旋锁lock，如果能立即获得锁，它获得锁并返回true，否则立即返回false，实际上不再“在原地打转”，也就是去掉了while循环的操作！\n函数调用流程：\n// spin_trylock spin_trylock(include/linux/spinlock.h) |--\u0026gt; raw_spin_trylock |--\u0026gt; __raw_spin_trylock(include\\linux\\spinlock_api_smp.h) |--\u0026gt; preempt_disable // 禁止调度  |--\u0026gt; do_raw_spin_trylock |--\u0026gt; arch_spin_trylock(arch/arm/include/asm/spinlock.h) |--\u0026gt; preempt_enable // 打开调度 下面为arch_spin_trylock汇编代码分析：\n  prefetchw(\u0026amp;lock-\u0026gt;slock) 函数用于提前加载锁的地址到处理器缓存中，从而提高锁的获取效率。\n  ldrex %0, [%3] 用于以原子方式读取锁的值到寄存器 %0 中，%3 为锁的地址。\n  mov %2, #0：将变量res置为0\n  subs %1, %0, %0, ror #16：在这行代码中，%0 表示 slock 寄存器，%1 表示 contended 寄存器。该语句的作用是：将锁变量的值存储在slock中，然后将slock右旋16位，获得低16位的值，并将结果存储到 contended 中。接着将 slock 和 slock 旋转后的值相减，并将结果存储到 contended 中。\n 同上，这里解释一下为什么执行sub操作：\n这里的核心思想是：一个32位的值，其高16位为next，低16位为owner，将其右旋16位后，得到的值高16位变成了owner，低16位变成了next，这时候执行相减操作，如果结果为0，则表示owner与next两个字段相等！\n   addeq %0, %0, %4：将%0寄存器的值加上%4的值，结果存放于%0寄存器中\n  strexeq %2, %0, [%3]：用于以原子方式将更新的值 %0 写入锁的地址所指定的内存位置，%2 为写入结果。\n  然后，代码会检查contended的值。如果contended的值为0，表示进程已经成功获取了锁，函数返回1；否则，表示锁已经被其他进程持有，函数返回0。\n  smp_mb() 函数执行一条内存屏障，确保所有关键数据的顺序性已经刷新到内存中。\n  4、总结 #  自旋锁在我们编写内核代码时会较多涉及，其底层实现主要依赖于前面所提到的：内存屏障、中断屏蔽、原子操作！\n 最后，自旋锁使用的方法如下：\n /* 定义一个自旋锁*/ spinlock_t lock; spin_lock_init(\u0026amp;lock); spin_lock (\u0026amp;lock) ; /* 获取自旋锁，保护临界区 */ . . . /* 临界区*/ spin_unlock (\u0026amp;lock) ; /* 解锁 */ 相信通过上面详细的拆解，到这里我们就能够对自旋锁有一个较深的认识了吧！\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":23,"href":"/docs/linux/linux_nvmem_subsystem/nvmem%E5%AD%90%E7%B3%BB%E7%BB%9F%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E5%9B%9Befuse%E9%A9%B1%E5%8A%A8%E5%AE%9E%E7%8E%B0%E6%B5%81%E7%A8%8B/","title":"【NVMEM子系统深入剖析】四、efuse驱动实现流程","section":"Linux NVMEM 子系统","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":24,"href":"/docs/uboot/%E5%9B%9Buboot%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%A8%A1%E5%BC%8F%E5%88%86%E6%9E%90/","title":"四、Uboot命令行模式分析","section":"Uboot开发","content":"四、Uboot命令行模式分析 #   前几篇文章，我们也了解了Uboot的启动流程，那么这节就主要讲讲Uboot的命令行模式。\n另外，文章末尾还提供eMMC5.1官方标准协议.pdf和eMMC4.51官方标准协议-中文.pdf下载渠道，方便深入了解底层协议。\n正文如下：\n  4.1 如何进入命令行模式 #  我们正常启动流程，默认是直接跳过Uboot命令行模式的，因为Uboot主要的作用是引导Kernel，一般我们不进行uboot开发时，都默认跳过进入命令行模式。\n 那么，我们要想进入Uboot命令行模式，需要进行哪些配置呢？\n 打开我们准备好一份Uboot源码，进入menuconfig配置菜单，主要设置下列几个配置信息！\n  CONFIG_CMDLINE：命令行模式开关 CONFIG_SYS_PROMPT：命令行模式提示符 CONFIG_HUSH_PARSER：使用hush shell 来对命令进行解析 BOOTDELAY：设置启动延时   Tip：meneconfig中查找苦难？实时/符号，输入1或2或3，直接查找指定标识。\n   打开之后，重新编译，并将Uboot镜像烧录到开发板中，再次启动，我们就能够看到倒计时。\n[2022-03-02:13:33:47]U-Boot 2020.10-rc1-00043-ge62a6d17c6-dirty (Feb 08 2022 - 10:14:14 +0800) [2022-03-02:13:33:47] [2022-03-02:13:33:47]Model: xxxxxx [2022-03-02:13:33:47]MMC: mmc1@xxxxxx: 1 [2022-03-02:13:33:47]In: serial [2022-03-02:13:33:47]Out: serial [2022-03-02:13:33:47]Err: serial [2022-03-02:13:33:47]Model: xxxxxx [2022-03-02:13:33:49]Hit any key to stop autoboot: 2 Hit any key to stop autoboot：我们在倒计时结束前，任意键入一个按键，即可进入！\n 4.2 Uboot基本命令解析 #  进入Uboot命令行模式后，键入help或者?，可以查看所有支持的Uboot命令。\n注意：Uboot支持的命令大都远远超过显示的，还有好多没有打开，可以在menuconfig中，打开相应的功能，如mmc相关的，md内存相关的。\n 常用命令如下：\nversion\t#查看uboot版本 reset #重启Uboot printenv\t#打印uboot环境变量 setenv name value\t#设置环境变量 md addr\t#查看内存指令 nm addr\t#修改内存值 mm addr\t#自增修改内存值 mmc dev id\t#选择mmc卡 mmc rescan\t#扫描卡 echo $name\t#打印环境变量  更多指令使用，可以见文末整理的文档\n  4.3 命令行模式代码执行流程分析 #   结合下面的程序执行流程图，代码，一起分析。\n 上图为Uboot命令行模式的代码具体执行流程，结合 专栏系列（二）uboot启动流程分析，文章内已经详细分析函数内部实现。\n static int abortboot(int bootdelay) { int abort = 0; if (bootdelay \u0026gt;= 0) { if (IS_ENABLED(CONFIG_AUTOBOOT_KEYED)) abort = abortboot_key_sequence(bootdelay); else abort = abortboot_single_key(bootdelay);\t//按键检测 \t} if (IS_ENABLED(CONFIG_SILENT_CONSOLE) \u0026amp;\u0026amp; abort) gd-\u0026gt;flags \u0026amp;= ~GD_FLG_SILENT; return abort; } static int abortboot_single_key(int bootdelay) { int abort = 0; unsigned long ts; printf(\u0026#34;Hit any key to stop autoboot: %2d \u0026#34;, bootdelay);\t//打印倒计时  /* * Check if key already pressed */ if (tstc()) {\t/* we got a key press\t*/\t//获取按键 \t(void) getc(); /* consume input\t*/ puts(\u0026#34;\\b\\b\\b0\u0026#34;); abort = 1;\t/* don\u0026#39;t auto boot\t*/ } while ((bootdelay \u0026gt; 0) \u0026amp;\u0026amp; (!abort)) { --bootdelay; /* delay 1000 ms */ ts = get_timer(0); do { if (tstc()) {\t/* we got a key press\t*/\t//获取按键 \tint key; abort = 1;\t/* don\u0026#39;t auto boot\t*/ bootdelay = 0;\t/* no more delay\t*/ key = getc(); /* consume input\t*/ if (IS_ENABLED(CONFIG_USE_AUTOBOOT_MENUKEY)) menukey = key; break; } udelay(10000); } while (!abort \u0026amp;\u0026amp; get_timer(ts) \u0026lt; 1000);\t//延时1S  printf(\u0026#34;\\b\\b\\b%2d \u0026#34;, bootdelay); } putc(\u0026#39;\\n\u0026#39;); return abort; } abortboot_single_key：该函数主要用于while循环检测按键，如果有按键按下，将abort标志位置1，最后运行cli_loop命令行模式的函数。\n如果按键不按下，标志位abort不起作用，直接运行run_command_list(s, -1, 0);，s = env_get(\u0026quot;bootcmd\u0026quot;);，直接跳转到我们设置的环境变量bootcmd所设定的指令，而不执行cli_loop函数。\n 对照运行流程图看代码，容易理解！！！\n  4.4 如何添加Uboot命令 #  如何自定义一个Uboot命令呢？\n我们暂且先不考虑实现的原理，就仅仅照葫芦画瓢来实现一个简单的Uboot命令！\n 第一步：照葫芦 #  我们打开Uboot的源码文件，进入cmd目录，没错，所有的命令实现都存放在该目录下。\n有没有看到help.C这个文件呢，我们就拿help这个文件来类比。\nU_BOOT_CMD：用来定义一个命令\nhelp：用于命令行键入的指令\ndo_help：键入指令后，执行的函数\n要想进一步使用该命令，我们不得不去了解每个参数的含义。\nstruct cmd_tbl_s { char\t*name;\t/* Command Name\t*/ int\tmaxargs;\t/* maximum number of arguments\t*/ int\trepeatable;\t/* autorepeat allowed?\t*/ /* Implementation function\t*/ int\t(*cmd)(struct cmd_tbl_s *, int, int, char *[]); char\t*usage;\t/* Usage message\t(short)\t*/ char\t*help;\t/* Help message\t(long)\t*/ /* do auto completion on the arguments */ int\t(*complete)(int argc, char *argv[], char last_char, int maxv, char *cmdv[]); }; typedef struct cmd_tbl_s\tcmd_tbl_t; 每个参数分别对应了：命令名、可接收的最大参数、命令可重复、响应函数、使用示例、帮助信息。\n 第二步：画瓢 #  弄明白这个道理，假如我们想加入一个helpme的指令，该怎么做？\n 定义一个指令  U_BOOT_CMD( helpme,\tCONFIG_SYS_MAXARGS,\t1,\tdo_helpme, \u0026#34;helpme dong\u0026#34;, \u0026#34;\\n\u0026#34; \u0026#34;\t- print brief description of all commands\\n\u0026#34; \u0026#34;helpme command ...\\n\u0026#34; \u0026#34;\t- print detailed usage of \u0026#39;command\u0026#39;\u0026#34; );  定义一个执行函数  static int do_helpme(struct cmd_tbl *cmdtp, int flag, int argc, char *const argv[]) { printf(\u0026#34;Cmd test ok!\\r\\n\u0026#34;); printf(\u0026#34;argc = %d\\r\\n\u0026#34;, argc); printf(\u0026#34;argv = \u0026#34;); for(int i = 0; i \u0026lt; argc; ++i) { printf(\u0026#34;%s\\t\u0026#34;, argv[i]); } printf(\u0026#34;\\r\\n\u0026#34;); } 这样，就可以编译-\u0026gt;烧录-\u0026gt;运行了。\n进入Uboot命令行，键入help查看添加的命令helpme。\n 键入命令测试  =\u0026gt; helpme 123456 123 Cmd test ok! argc = 3 argv = helpme 123456 123  第三步：优雅 #  如果我们只是暂时测试，这样添加无伤大雅；如果我们需要投入正规项目使用，这么做有点激进了。\n更加合理的做法是：\n 在uboot/cmd目录下，建立一个文件XXX.c 将要添加的命令写入XXX.c该文件中 修改Makefile文件，编译该文件：obj-y += XXX.o 重新编译，烧录   说白了，就是创建一个文件，将自定义指令添加进去，尽量不修改源码！\n  4.5 Uboot命令底层实现分析 #  上面写了傻瓜式添加命令的方法，对于进行Uboot开发，当然我们需要去了解一下内部的实现原理。\n4.5.1 U_BOOT_CMD #  查看U_BOOT_CMD宏定义\n#define U_BOOT_CMD(_name, _maxargs, _rep, _cmd, _usage, _help)\t\\ U_BOOT_CMD_COMPLETE(_name, _maxargs, _rep, _cmd, _usage, _help, NULL) #define U_BOOT_CMD_COMPLETE(_name, _maxargs, _rep, _cmd, _usage, _help, _comp) \\ ll_entry_declare(struct cmd_tbl, _name, cmd) =\t\\ U_BOOT_CMD_MKENT_COMPLETE(_name, _maxargs, _rep, _cmd,\t\\ _usage, _help, _comp); #define U_BOOT_CMD_MKENT_COMPLETE(_name, _maxargs, _rep, _cmd,\t\\ _usage, _help, _comp)\t\\ { #_name, _maxargs,\t\\ _rep ? cmd_always_repeatable : cmd_never_repeatable,\t\\ _cmd, _usage, _CMD_HELP(_help) _CMD_COMPLETE(_comp) } #define ll_entry_declare(_type, _name, _list)\t\\ _type _u_boot_list_2_##_list##_2_##_name __aligned(4)\t\\ __attribute__((unused,\t\\ section(\u0026#34;.u_boot_list_2_\u0026#34;#_list\u0026#34;_2_\u0026#34;#_name)))   乍一看，都是宏定义，为什么看起来这么吃力？\n 在这里，不得不提到#和##的区别 #   #：转换为字符串  ... #define TO_STR(x) #x int main() { int value = 123; printf(\u0026#34;TO_STR(value) = %s\\n\u0026#34;, TO_STR(value)); printf(\u0026#34;TO_STR(123) = %s\\n\u0026#34;, TO_STR(123)); } //打印 TO_STR(value) = value; TO_STR(123) = 123;  ##：两个字符拼接  #define CONNECT(x,y) x##y #define VAR(y) data##y int main() { int xy = 123; printf(\u0026#34;xy = %d\\n\u0026#34;, CONNECT(x, y)); CONNECT(x, y) = 123456; printf(\u0026#34;xy = %d\\n\u0026#34;, CONNECT(x, y)); int VAR(1) = 100; printf(\u0026#34;VAR(1) = data1 = %d\\n\u0026#34;, data1); } //打印 xy = 123 xy = 123456 VAR(1) = data1 = 100  回到正文\n 上面的宏定义，简单来看，转换流程就是：\nU_BOOT_CMD -\u0026gt; U_BOOT_CMD_COMPLETE -\u0026gt; ll_entry_declare = U_BOOT_CMD_MKENT_COMPLETE -\u0026gt; _type xxx = {aaa, bbb, ccc , ...}\n其本质就是： struct my_struct test = {1, 2, 3};结构体赋值语句。\n 以help命令为例：\nU_BOOT_CMD( help,\tCONFIG_SYS_MAXARGS,\t1,\tdo_help, \u0026#34;print command description/usage\u0026#34;, \u0026#34;\\n\u0026#34; \u0026#34;\t- print brief description of all commands\\n\u0026#34; \u0026#34;help command ...\\n\u0026#34; \u0026#34;\t- print detailed usage of \u0026#39;command\u0026#39;\u0026#34; ); 直接展开来看：\nstruct cmd_tbl _u_boot_list_2_cmd_2_help __aligned(4) __attribute__((unused, section(\u0026#34;.u_boot_list_2_cmd_2_help\u0026#34;))) = {\u0026#34;help\u0026#34;, CONFIG_SYS_MAXARGS, cmd_always_repeatable, do_help, \u0026#34;xxx\u0026#34;, \u0026#34;xxx\u0026#34;}; 也就相当于，我们定义一个命令，给其赋值。\n定义的命令存放在哪里呢？ #  根据上面展开来看，section(\u0026quot;.u_boot_list_2_cmd_2_help\u0026quot;)，存放在段.u_boot_list_2_cmd_2_help中，打开u-boot.map文件，我们可以查找得到。\n有没有觉得很熟悉，没错，跟前面讲过的驱动模型很像。\n我们定义的命令，被u_boot_list_2_cmd_1和u_boot_list_2_cmd_3两个段所包括，用于遍历，最终查找得到我们想要的命令。\n4.6 Uboot命令响应流程 #  命令响应流程见图：\n 根据4.3 命令行模式代码执行流程分析，我们可以知道，命令行模式最终执行cli_loop函数，实现与用户的交互。\nvoid cli_loop(void) { bootstage_mark(BOOTSTAGE_ID_ENTER_CLI_LOOP); #ifdef CONFIG_HUSH_PARSER \tparse_file_outer(); /* This point is never reached */ for (;;); #elif defined(CONFIG_CMDLINE) \tcli_simple_loop(); #else \tprintf(\u0026#34;## U-Boot command line is disabled. Please enable CONFIG_CMDLINE\\n\u0026#34;); #endif /*CONFIG_HUSH_PARSER*/} 通过分析代码，Uboot的命令行有两种模式：一种是HUSH解析，另一种是通用解析。\n HUSH解析：调用parse_file_outer并不断循环 通用解析：调用cli_simple_loop并不断循环。   无论哪种命令行解析，说白了就是输入输出的处理，必定会读取数据，执行相应命令，打印出对应数据\n HUSH模式\n 输入数据处理：parse_stream 输出数据处理：run_list  通用模式：\n 输入数据处理：cli_readline 输出数据处理：run_command_repeatable   具体实现流程，参照上面的流程图！\n命令行模式的深入解析，准备在下节详细介绍！\n 目前，我们已经对命令行的整体运行流程进行梳理，熟悉整体的运行逻辑，并且能够添加自定义命令喽。\n 4.6 推荐文档 #  [1]：https://www.pianshen.com/article/21471247431/\n[2]：https://blog.csdn.net/weixin_44895651/article/details/108211268\n[3]：https://blog.51cto.com/u_2847568/4917530?b=totalstatistic\n[4]：https://blog.csdn.net/SilverFOX111/article/details/86892231\n[5]：https://blog.csdn.net/andy_wsj/article/details/8614905\n 另外，如果有同学想了解Emmc协议的，可以【戳这里】下载eMMC5.1官方标准协议.pdf和eMMC4.51官方标准协议-中文.pdf\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":25,"href":"/docs/linux/linux_memory_manage/%E5%9B%9B%E7%89%A9%E7%90%86%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%9E%8B/","title":"四、物理地址空间设计模型","section":"Linux 内存管理","content":"Linux内存管理 | 四、物理地址空间设计模型 #  前面几篇文章，主要讲解了虚拟内存空间的布局和管理，下面同步来聊聊物理内存空间的布局和管理。\n 1、物理内存 #   什么是物理内存？\n 我们平时聊的内存，也叫随机访问存储器（random-access memory），也叫RAM。\nRAM分为两类：\n SRAM：静态RAM，其主要用于CPU高速缓存 L1Cache，L2Cache，L3Cache，其特点是访问速度快，访问速度为 1 - 30 个时钟周期，但是容量小，造价高。   DRAM：动态RAM，其主要用于我们常说的主存上，其特点的是访问速度慢（相对高速缓存），访问速度为 50 - 200 个时钟周期，但是容量大，造价便宜些（相对高速缓存）。  DRAM经过组合起来，就作为我们的计算机内存，也是物理内存。\n 2、物理内存访问模型 #  上面介绍了物理内存的基本组成，那么CPU是如何访问物理内存的呢？\n对于CPU访问物理内存，Linux提供了两种架构：UMA(Uniform Memory Access)一致内存访问，NUMA(Non-Uniform Memory Access)非一致内存访问。\n2.1 UMA #  在UMA架构下，多核处理器中的多个CPU，位于总线的一侧，所有的内存条组成的物理内存位于总线的另一侧。\n所有的CPU访问内存都要经过总线，并且距离都是一样的，所以在UMA架构下，所有CPU具有相同的访问特性，即对内存的访问具有相同的速度。\n2.2 NUMA #  这种架构，系统中的各个处理器都有本地内存，处理器与处理器之间也通过总线连接，以便于其他处理器对本地内存的访问。\n与UMA不同的是，处理器访问本地内存的速度要快于对其他处理器本地内存的访问。\n3、物理内存组织模型 #  内存页是物理内存管理中最小单位，有时也成为页帧（Page Frame）。\n内核对物理内存划分为一页一页的连续的内存块，每页大小4KB，并且使用struct page结构体来表示页结构，其中封装了每个页的状态信息，包括：组织结构，使用信息，统计信息等。\n page结构体较为复杂，我们后续再深入了解。\n   更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  3.1 FLATMEM平坦内存模型 #   FLATMEM即：flat memory model。\n 我们把物理内存想象成它是由连续的一页一页的块组成的，我们从0开始对物理页编号，这样每个物理页都会有页号。\n由于物理地址是连续的，页也是连续的，每个页大小也是一样的。因而对于任何一个地址，只要直接除一下每页的大小，很容易直接算出在哪一页。\n如果是这样，整个物理内存的布局就非常简单、易管理，这就是最经典的平坦内存模型（Flat Memory Model）。\n如上图，平坦内存模型中，内核使用一个mem_map的全局数组，来组织所有划分出来的物理内存页，下标由PFN表示。\n在平坦内存模型下 ，page_to_pfn 与 pfn_to_page 的计算逻辑就非常简单，本质就是基于 mem_map 数组进行偏移操作。\n#ifndef ARCH_PFN_OFFSET #define ARCH_PFN_OFFSET\t(0UL) #endif  #if defined(CONFIG_FLATMEM) #define __pfn_to_page(pfn) (mem_map + ((pfn)-ARCH_PFN_OFFSET)) #define __page_to_pfn(page) ((unsigned long)((page)-mem_map) + ARCH_PFN_OFFSET) #endif  ARCH_PFN_OFFSET 是 PFN 的起始偏移量。\n  3.2 DISCONTIGMEM 不连续内存模型 #   DISCONTIGMEM即：discontiguous memory model。\n 我们早期内核使用的是FLATMEM模型，该模型对于较小的，连续的物理空间是方便使用的，但是当物理内存不连续时，使用mem_map管理，就会出现空洞，这会浪费mem_map数组本身占用的内存空间。\n对于NUMA访问内存模型，物理内存分布就是不连续的，为了有效管理，DISCONTIGMEM 不连续内存模型出现了。\n在不连续的物理内存中，DISCONTIGMEM不连续内存模型，将物理内存分成了一个个的node，然后每个node管理一块连续的物理内存，连续的物理内存仍然使用FLATMEM平坦内存模型来管理，从而避免了内存空洞的浪费。\n 我们可以看出 DISCONTIGMEM 非连续内存模型其实就是 FLATMEM 平坦内存模型的一种扩展。\n DISCONTIGMEM是个稍纵即逝的内存模型，在SPARSEMEM出现后即被完全替代。\n 3.3 SPARSEMEM稀疏内存模型 #  随着内存技术的发展，内核可以支持物理内存的热插拔了（像我们的内存条，可以直接插入拔出），这样不连续物理内存已然称为常态。\nSPARSEMEM稀疏内存模型的核心思想就是对粒度更小的连续内存块进行精细的管理，用于管理连续内存块的单元被称作 section 。\n 物理页大小为 4k 的情况下， section 的大小为 128M ，物理页大小为 16k 的情况下， section 的大小为 512M。\n  在内核中，使用struct mem_section结构体表示SPARSEMEM模型中的section\nstruct mem_section { unsigned long section_mem_map; ... }   每个mem_section管理一片小的，物理内存连续的区域，并且支持对该区域的offline/online状态\n  所有的mem_section都保存在一个全局数组中\n   整体的框架如下：\n 在 SPARSEMEM 稀疏内存模型下 page_to_pfn 与 pfn_to_page 的计算逻辑又发生了变化。\n#if defined(CONFIG_SPARSEMEM) /* * Note: section\u0026#39;s mem_map is encoded to reflect its start_pfn. * section[i].section_mem_map == mem_map\u0026#39;s address - start_pfn; */ #define __page_to_pfn(pg)\t\\ ({\tconst struct page *__pg = (pg);\t\\ int __sec = page_to_section(__pg);\t\\ (unsigned long)(__pg - __section_mem_map_addr(__nr_to_section(__sec)));\t\\ })  #define __pfn_to_page(pfn)\t\\ ({\tunsigned long __pfn = (pfn);\t\\ struct mem_section *__sec = __pfn_to_section(__pfn);\t\\ __section_mem_map_addr(__sec) + __pfn;\t\\ }) #endif  在 page_to_pfn 的转换中，首先需要通过 page_to_section 根据 struct page 结构定位到 mem_section 数组中具体的 section 结构。然后在通过 section_mem_map 定位到具体的 PFN。 在 pfn_to_page 的转换中，首先需要通过 __pfn_to_section 根据 PFN 定位到 mem_section 数组中具体的 section 结构。然后在通过 PFN 在 section_mem_map 数组中定位到具体的物理页 Page 。   4、总结 #  以上，我们先对物理内存空间有一个基础的了解，明白物理内存空间的内存访问模型和组织模型，下面我们再详细介绍物理内存空间的布局和管理。\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":26,"href":"/docs/linux/linux_kernel_lock/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3linux%E9%94%81%E6%9C%BA%E5%88%B6%E4%BA%94%E8%A1%8D%E7%94%9F%E8%87%AA%E6%97%8B%E9%94%81/","title":"【深入理解Linux锁机制】五、衍生自旋锁","section":"Linux 内核锁详解","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":27,"href":"/docs/linux/linux_memory_manage/%E4%BA%94%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E7%A9%BA%E9%97%B4%E5%B8%83%E5%B1%80%E5%8F%8A%E7%AE%A1%E7%90%86/","title":"五、物理内存空间布局及管理","section":"Linux 内存管理","content":"Linux内存管理 | 五、物理内存空间布局及管理 #  上章，我们介绍了物理内存的访问内存模型和组织内存模型，我们再来回顾一下：\n物理内存的访问内存模型分为：\n UMA：一致内存访问 NUMA：非一致内存访问  物理内存的组织模型：\n FLATMEM：平坦内存模型 DISCONTIGMEM：不连续内存模型 SMARSEMEM：稀疏内存模型  Linux内核为了用统一的代码获取最大程度的兼容性，对物理内存的定义方面，引入了：内存结点（node）、内存区域（zone），内存页（page）的概念，下面我们来一一探究。\n 更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  1、内存节点node #  内存节点的引入，是Linux为了最大程度的提高兼容性，将UMA和NUMA系统统一起来，对于UMA而言是只有一个节点的系统。\n 下面的代码部分，我们尽可能的只保留暂时用的到的部分，不涉及太多的体系架相关的细节。\n 在Linux内核中，我们使用 typedef struct pglist_data pg_data_t表示一个节点\n/* * On NUMA machines, each NUMA node would have a pg_data_t to describe * it\u0026#39;s memory layout. On UMA machines there is a single pglist_data which * describes the whole memory. * * Memory statistics and page replacement data structures are maintained on a * per-zone basis. */ typedef struct pglist_data { ... int node_id; struct page *node_mem_map; unsigned long node_start_pfn; unsigned long node_present_pages; /* total number of physical pages */ unsigned long node_spanned_pages; /* total size of physical page range, including holes */ ... } pg_data_t;   node_id：每个节点都有自己的ID\n  node_mem_map：当前节点的struct page数组，用来管理这个节点的所有的页\n  node_start_pfn：这个节点的起始页号\n  node_present_pages：这个节点的真正可用的物理内存的页面数\n  node_spanned_pages：这个节点所包含的物理内存的页面数，包括不连续的内存空洞\n   例如，64M 物理内存隔着一个 4M 的空洞，然后是另外的 64M 物理内存。\n这样换算成页面数目就是，16K 个页面隔着 1K 个页面，然后是另外 16K 个页面。\n这种情况下，node_spanned_pages 就是 33K 个页面，node_present_pages 就是 32K 个页面。\n  内核使用了一个大小为 MAX_NUMNODES ，类型为 struct pglist_data 的全局数组 node_data[] 来管理所有的 NUMA 节点。\n2、内存区域zone #  2.1 各区域的布局 #  每一个节点，都被分成了一个个区域zone，我们看一下zone的定义：\nenum zone_type { #ifdef CONFIG_ZONE_DMA  ZONE_DMA, #endif #ifdef CONFIG_ZONE_DMA32  ZONE_DMA32, #endif  ZONE_NORMAL, #ifdef CONFIG_HIGHMEM  ZONE_HIGHMEM, #endif  ZONE_MOVABLE, #ifdef CONFIG_ZONE_DEVICE  ZONE_DEVICE, #endif  __MAX_NR_ZONES }; ZONE_DMA：用作DMA的内存。\n DMA 是这样一种机制：要把外设的数据读入内存或把内存的数据传送到外设，原来都要通过 CPU 控制完成，但是这会占用 CPU，影响 CPU 处理其他事情，所以有了 DMA 模式。\nCPU 只需向 DMA 控制器下达指令，让 DMA 控制器来处理数据的传送，数据传送完毕再把信息反馈给 CPU，这样就可以解放 CPU。\n ZONE_DMA32：对于 64 位系统，有两个 DMA 区域。除了上面说的 ZONE_DMA，还有 ZONE_DMA32。\nZONE_NORMAL：直接映射区，也就i是之前讲的从物理内存到虚拟内存的内核区域，通过加上一个常量直接映射。\nZONE_HIGHMEM：高端内存区\nZONE_MOVABLE：可移动区域，通过将物理内存划分为可移动分配区域和不可移动分配区域来避免内存碎片。\n2.2 各区域的管理 #  上面我们大致了解了，每个zone的布局情况，下面我们来看看内核是如何对其进行管理的。\n 接着上面介绍的pglist_data结构体\n /* * On NUMA machines, each NUMA node would have a pg_data_t to describe * it's memory layout. On UMA machines there is a single pglist_data which * describes the whole memory. * * Memory statistics and page replacement data structures are maintained on a * per-zone basis. */ typedef struct pglist_data { ... int node_id; struct page *node_mem_map; unsigned long node_start_pfn; unsigned long node_present_pages; /* total number of physical pages */ unsigned long node_spanned_pages; /* total size of physical page range, including holes */ ... struct zone node_zones[MAX_NR_ZONES]; struct zonelist node_zonelists[MAX_ZONELISTS]; int nr_zones; ... } pg_data_t;  nr_zones：用于统计 NUMA 节点内包含的物理内存区域个数，不是每个 NUMA 节点都会包含以上介绍的所有物理内存区域，NUMA 节点之间所包含的物理内存区域个数是不一样的。   事实上只有第一个 NUMA 节点可以包含所有的物理内存区域，其它的节点并不能包含所有的区域类型，因为有些内存区域比如：ZONE_DMA，ZONE_DMA32 必须从物理内存的起点开始。这些在物理内存开始的区域可能已经被划分到第一个 NUMA 节点了，后面的物理内存才会被依次划分给接下来的 NUMA 节点。因此后面的 NUMA 节点并不会包含 ZONE_DMA，ZONE_DMA32 区域。\nZONE_NORMAL、ZONE_HIGHMEM 和 ZONE_MOVABLE 是可以出现在所有 NUMA 节点上的。\n  node_zones[MAX_NR_ZONES]：node_zones该数组包括了所有的zone物理内存区域 node_zonelists[MAX_ZONELISTS]：是 struct zonelist 类型的数组，它包含了备用 NUMA 节点和这些备用节点中的物理内存区域。    下面我们看一下struct zone结构体\n struct zone { ...... struct pglist_data\t*zone_pgdat; struct per_cpu_pageset __percpu *pageset; unsigned long\tzone_start_pfn; /* * spanned_pages is the total pages spanned by the zone, including * holes, which is calculated as: * spanned_pages = zone_end_pfn - zone_start_pfn; * * present_pages is physical pages existing within the zone, which * is calculated as: *\tpresent_pages = spanned_pages - absent_pages(pages in holes); * * managed_pages is present pages managed by the buddy system, which * is calculated as (reserved_pages includes pages allocated by the * bootmem allocator): *\tmanaged_pages = present_pages - reserved_pages; * */ unsigned long\tmanaged_pages; unsigned long\tspanned_pages; unsigned long\tpresent_pages; const char\t*name; ...... /* free areas of different sizes */ struct free_area\tfree_area[MAX_ORDER]; /* zone flags, see below */ unsigned long\tflags; /* Primarily protects free_area */ spinlock_t\tlock; ...... } ____cacheline_internodealigned_in_  zone_start_pfn：表示属于这个zone的第一个页 spanned_pages：看注释我们可以知道，spanned_pages = zone_end_pfn - zone_start_pfn，表示该区域的所有物理内存的页面数，包括内存空洞 present_pages：看注释我们可以知道，present_pages = spanned_pages - absent_pages(pages in holes)，表示该区域真实存在的物理内存页面数，不包括空洞 managed_pages：看注释我们可以知道，managed_pages = present_pages - reserved_pages，被伙伴系统管理的所有页面数。 per_cpu_pageset：用于区分冷热页，   什么叫冷热页呢？咱们讲 x86 体系结构的时候讲过，为了让 CPU 快速访问段描述符，在 CPU 里面有段描述符缓存。CPU 访问这个缓存的速度比内存快得多。同样对于页面来讲，也是这样的。如果一个页被加载到 CPU 高速缓存里面，这就是一个热页（Hot Page），CPU 读起来速度会快很多，如果没有就是冷页（Cold Page）。由于每个 CPU 都有自己的高速缓存，因而 per_cpu_pageset 也是每个 CPU 一个。\n   更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  3、内存页page #  内存页是物理内存最小单位，有时也叫页帧（page frame），Linux会为系统的物理内存的每一个页都创建了struct page对象，并用全局变量struct page *mem_map来存放所有物理页page对象的指针，页的大小取决于MMU（Memory Management Unit），后者主要用来将虚拟地址空间转换为物理地址空间。\n 看一下page的结构体\n struct page { unsigned long flags;\t/* Atomic flags, some possibly * updated asynchronously */ union { struct {\t/* Page cache and anonymous pages */ /** * @lru: Pageout list, eg. active_list protected by * zone_lru_lock. Sometimes used as a generic list * by the page owner. */ struct list_head lru; /* See page-flags.h for PAGE_MAPPING_FLAGS */ struct address_space *mapping; pgoff_t index;\t/* Our offset within mapping. */ /** * @private: Mapping-private opaque data. * Usually used for buffer_heads if PagePrivate. * Used for swp_entry_t if PageSwapCache. * Indicates order in the buddy system if PageBuddy. */ unsigned long private; }; struct {\t/* slab, slob and slub */ union { struct list_head slab_list;\t/* uses lru */ struct {\t/* Partial pages */ struct page *next; #ifdef CONFIG_64BIT  int pages;\t/* Nr of pages left */ int pobjects;\t/* Approximate count */ #else  short int pages; short int pobjects; #endif  }; }; struct kmem_cache *slab_cache; /* not slob */ /* Double-word boundary */ void *freelist;\t/* first free object */ union { void *s_mem;\t/* slab: first object */ unsigned long counters;\t/* SLUB */ struct {\t/* SLUB */ unsigned inuse:16; unsigned objects:15; unsigned frozen:1; }; }; }; ..... } 我们能够看到struct page有很多union组成，union 结构是在 C 语言中被用于同一块内存根据情况保存不同类型数据的一种方式。这里之所以用了 union，是因为一个物理页面使用模式有多种。\n 第一种模式：直接用一整页，这一整页的物理内存直接与虚拟地址空间建立映射关系，我们把这种称为匿名页（Anonymous Page）。或者用于关联一个文件，然后再和虚拟地址空间建立映射关系，这样的文件，我们称为内存映射文件（Memory-mapped File），这种分配页级别的，Linux采用一种被称为伙伴系统（Buddy System）的技术。 第二种模式：仅需要分配小的内存块。有时候，我们不需要一下子分配这么多的内存，例如分配一个 task_struct 结构，只需要分配小块的内存，去存储这个进程描述结构的对象。为了满足对这种小内存块的需要，Linux 系统采用了一种被称为slab allocator的技术   上面说的两种，都是页的分配方式，也就是物理内存的分配方式，下一章，我们继续深入分析物理内存的这两种分配方式。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":28,"href":"/docs/linux/linux_kernel_lock/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3linux%E9%94%81%E6%9C%BA%E5%88%B6%E5%85%AD%E4%BF%A1%E5%8F%B7%E9%87%8F/","title":"【深入理解Linux锁机制】六、信号量","section":"Linux 内核锁详解","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":29,"href":"/docs/linux/linux_memory_manage/%E5%85%AD%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E4%BC%99%E4%BC%B4%E7%B3%BB%E7%BB%9F/","title":"六、物理内存分配——伙伴系统","section":"Linux 内存管理","content":"Linux内存管理 | 六、物理内存分配——伙伴系统 #  上一章，我们了解了物理内存的布局以及Linux内核对其的管理方式，页（page）也是物理内存的最小单元，Linux内核对物理内存的分配主要分为两种：一种是整页的分配，采用的是伙伴系统，另一种是小内存块的分配，采用的是slab技术。\n下面我们先来看看什么是伙伴系统！\n 1、伙伴系统（Buddy System） #  Linux系统中，对物理内存进行分配的核心是建立在页面级的伙伴系统之上。Linux内存管理的页大小为4KB，把所有的空闲页分组为11个页块链表，每个链表分别包含很多个大小的页块，有 1、2、4、8、16、32、64、128、256、512 和 1024 个连续页的页块，最大可以申请 1024 个连续页，对应 4MB 大小的连续内存。每个页块的第一个页的物理地址是该页块大小的整数倍。\n如下图所示：\n 第 i 个页块链表中，页块中页的数目为 2^i。——仔细理解这个页块的含义。\n  在struct zone结构体中，有下面定义\nstruct free_area\tfree_area[MAX_ORDER]; #define MAX_ORDER 11 free_area：存放不同大小的页块\nMAX_ORDER：就是指数\n 当向内核请求分配 (2^(i-1)，2^i] 数目的页块时，按照 2^i 页块请求处理。如果对应的页块链表中没有空闲页块，那我们就在更大的页块链表中去找。当分配的页块中有多余的页时，伙伴系统会根据多余的页块大小插入到对应的空闲页块链表中。\n举个例子：\n例如，要请求一个 128 个页的页块时，先检查 128 个页的页块链表是否有空闲块。如果没有，则查 256 个页的页块链表；如果有空闲块的话，则将 256 个页的页块分成两份，一份使用，一份插入 128 个页的页块链表中。如果还是没有，就查 512 个页的页块链表；如果有的话，就分裂为 128、128、256 三个页块，一个 128 的使用，剩余两个插入对应页块链表。\n 上面的这套机制就是伙伴系统所做的事情，它主要负责对物理内存页面进行跟踪，记录哪些是被内核使用的页面，哪些是空闲页面。\n 2、页面分配器（Page Allocator） #  由上一章我们知道，物理内存被分为了几个区域：ZONE_DMA、ZONE_NORMAL、ZONE_HIGHMEM，其中前两个区域的物理页面与虚拟地址空间是线性映射的。\n页面分配器主要的工作原理如下：\n 如果页面分配器分配的物理页面在ZONE_DMA、ZONE_NORMAL区域，那么对应的虚拟地址到物理地址映射的页目录已经建立，因为是线性映射，两者之间有一个差值PAGE_OFFSET。 如果页面分配器分配的物理页面在ZONE_HIGHMEM区域，那么内核此时还没有对该页面进行映射，因此页面分配器的调用者，首先在虚拟地址空间的动态映射区或者固定映射区分配一个虚拟地址，然后映射到该物理页面上。   以上就是页面分配器的原理，对于我们只需要调用相关接口函数就可以了。\n页面分配函数主要有两个：alloc_pages和__get_free_pages，而这两个函数最终也会调用到alloc_pages_node，其实现原理完全一样。\n 下面我们从代码层面来看页面分配器的工作原理\n   更多干货可见：高级工程师聚集地，助力大家更上一层楼！\n  3、gfp_mask #  我们先来了解一下gfp_mask，它并不是页面分配器函数，而只是这些页面分配函数中一个重要的参数，是个用于控制分配行为的掩码，并可以告诉内核应该到哪个zone中分配物理内存页面。\n/* Plain integer GFP bitmasks. Do not use this directly. */ #define ___GFP_DMA\t0x01u #define ___GFP_HIGHMEM\t0x02u #define ___GFP_DMA32\t0x04u #define ___GFP_MOVABLE\t0x08u #define ___GFP_RECLAIMABLE\t0x10u #define ___GFP_HIGH\t0x20u #define ___GFP_IO\t0x40u #define ___GFP_FS\t0x80u #define ___GFP_WRITE\t0x100u #define ___GFP_NOWARN\t0x200u #define ___GFP_RETRY_MAYFAIL\t0x400u #define ___GFP_NOFAIL\t0x800u #define ___GFP_NORETRY\t0x1000u #define ___GFP_MEMALLOC\t0x2000u #define ___GFP_COMP\t0x4000u #define ___GFP_ZERO\t0x8000u #define ___GFP_NOMEMALLOC\t0x10000u #define ___GFP_HARDWALL\t0x20000u #define ___GFP_THISNODE\t0x40000u #define ___GFP_ATOMIC\t0x80000u #define ___GFP_ACCOUNT\t0x100000u #define ___GFP_DIRECT_RECLAIM\t0x200000u #define ___GFP_KSWAPD_RECLAIM\t0x400000u #ifdef CONFIG_LOCKDEP #define ___GFP_NOLOCKDEP\t0x800000u #else #define ___GFP_NOLOCKDEP\t0 #endif   ___GFP_DMA：在ZONE_DMA标识的内存区域中查找空闲页。\n  ___GFP_HIGHMEM：在ZONE_HIGHMEM标识的内存区域中查找空闲页。\n  ___GFP_MOVABLE：内核将分配的物理页标记为可移动的。\n  ___GFP_HIGH：内核允许使用紧急分配链表中的保留内存页。该请求必须以原子方式完成，意味着请求过程不允许被中断。\n  ___GFP_IO：内核在查找空闲页的过程中可以进行I/O操作，如此内核可以将换出的页写到硬盘。\n  ___GFP_FS：查找空闲页的过程中允许执行文件系统相关操作。\n  ___GFP_ZERO：用0填充成功分配出来的物理页。\n   通常意义上，这些以“__”打头的GFP掩码只限于在内存管理组件内部的代码使用，对于提供给外部的接口，比如驱动程序中所使用的页面分配函数，gfp_mask掩码以“GFP_”的形式出现，而这些掩码基本上就是上面提到的掩码的组合。\n例如内核为外部模块提供的最常使用的几个掩码如下：\n#define GFP_ATOMIC\t(__GFP_HIGH|__GFP_ATOMIC|__GFP_KSWAPD_RECLAIM) #define GFP_KERNEL\t(__GFP_RECLAIM | __GFP_IO | __GFP_FS) #define GFP_KERNEL_ACCOUNT (GFP_KERNEL | __GFP_ACCOUNT) #define GFP_NOWAIT\t(__GFP_KSWAPD_RECLAIM) #define GFP_NOIO\t(__GFP_RECLAIM) #define GFP_NOFS\t(__GFP_RECLAIM | __GFP_IO) #define GFP_USER\t(__GFP_RECLAIM | __GFP_IO | __GFP_FS | __GFP_HARDWALL) #define GFP_DMA\t__GFP_DMA #define GFP_DMA32\t__GFP_DMA32 #define GFP_HIGHUSER\t(GFP_USER | __GFP_HIGHMEM) #define GFP_HIGHUSER_MOVABLE\t(GFP_HIGHUSER | __GFP_MOVABLE) #define GFP_TRANSHUGE_LIGHT\t((GFP_HIGHUSER_MOVABLE | __GFP_COMP | \\ __GFP_NOMEMALLOC | __GFP_NOWARN) \u0026amp; ~__GFP_RECLAIM) #define GFP_TRANSHUGE\t(GFP_TRANSHUGE_LIGHT | __GFP_DIRECT_RECLAIM)  GFP_ATOMIC：内核模块中最常使用的掩码之一，用于原子分配。此掩码告诉页面分配器，在分配内存页时，绝对不能中断当前进程或者把当前进程移出调度器。 GFP_KERNEL：内核模块中最常使用的掩码之一，带有该掩码的内存分配可能导致当前进程进入睡眠状态。 GFP_USER：用于为用户空间分配内存页，可能引起进程的休眠。 GFP_NOIO：在分配过程中禁止I/O操作 GFP_NOFS：禁止文件系统相关的函数调用 GFP_DMA：限制页面分配器只能在ZONE_DMA域中分配空闲物理页面，用于分配适用于DMA缓冲区的内存。   通过gfp_mask掩码，更加方便我们控制页面分配器到哪个区域去分配物理内存，分配内存的优先级如下：\n 指定__GFP_HIGHMEM：先在ZONE_HIGHMEM域中查找空闲页，如果无法满足当前分配，页分配器将回退到ZONE_NORMAL域中继续查找，如果依然无法满足当前分配，分配器将回退到ZONE_DMA域，或者成功或者失败。 指定__GFP_DMA：只能在ZONE_DMA中分配物理页面，如果无法满足，则分配失败。 没有__GFP_NORMAL这样的掩码，但是前面已经提到，如果gfp_mask中没有明确指定__GFP_HIGHMEM或者是__GFP_DMA，默认就相当于__GFP_NORMAL，优先在ZONE_NORMAL域中分配，其次是ZONE_DMA域。   4、alloc_pages #  alloc_pages函数负责分配2^order个连续的物理页面并返回起始页的struct page实例。\nalloc_pages的实现源码如下：\nstatic inline struct page * alloc_pages(gfp_t gfp_mask, unsigned int order) { return alloc_pages_current(gfp_mask, order); } /** * alloc_pages_current - Allocate pages. * *\t@gfp: *\t%GFP_USER user allocation, * %GFP_KERNEL kernel allocation, * %GFP_HIGHMEM highmem allocation, * %GFP_FS don\u0026#39;t call back into a file system. * %GFP_ATOMIC don\u0026#39;t sleep. *\t@order: Power of two of allocation size in pages. 0 is a single page. * *\tAllocate a page from the kernel page pool. When not in *\tinterrupt context and apply the current process NUMA policy. *\tReturns NULL when no page can be allocated. */ struct page *alloc_pages_current(gfp_t gfp, unsigned order) { struct mempolicy *pol = \u0026amp;default_policy; struct page *page; if (!in_interrupt() \u0026amp;\u0026amp; !(gfp \u0026amp; __GFP_THISNODE)) pol = get_task_policy(current); /* * No reference counting needed for current-\u0026gt;mempolicy * nor system default_policy */ if (pol-\u0026gt;mode == MPOL_INTERLEAVE) page = alloc_page_interleave(gfp, order, interleave_nodes(pol)); else page = __alloc_pages_nodemask(gfp, order, policy_node(gfp, pol, numa_node_id()), policy_nodemask(gfp, pol)); return page; } EXPORT_SYMBOL(alloc_pages_current); alloc_pages调用alloc_pages_current，其中\n gfp参数：即上文的gfp_mask，表明我们想要在哪个物理内存区域进行内存分配 order参数：表示分配 2 的 order 次方个页。  __alloc_pages_nodemask为伙伴系统的核心实现，它会调用 get_page_from_freelist。\nstatic struct page * get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags, const struct alloc_context *ac) { ...... for_next_zone_zonelist_nodemask(zone, z, ac-\u0026gt;zonelist, ac-\u0026gt;high_zoneidx, ac-\u0026gt;nodemask) { struct page *page; ...... page = rmqueue(ac-\u0026gt;preferred_zoneref-\u0026gt;zone, zone, order, gfp_mask, alloc_flags, ac-\u0026gt;migratetype); ...... } 这里面的逻辑也很容易理解，就是在一个循环中先看当前节点的 zone。如果找不到空闲页，则再看备用节点的 zone。\n 每一个 zone，都有伙伴系统维护的各种大小的队列，就像上面伙伴系统原理里讲的那样。\n 这里调用 rmqueue 就很好理解了，就是找到合适大小的那个队列，把页面取下来。\n 伙伴系统的实现代码，感兴趣的可以深入探究。\n 在调用这个函数的时候，有几种情况：\n 如果gfp_mask中没有指定__GFP_HIGHMEM，那么分配的物理页面必然来自ZONE_NORMAL或者ZONE_DMA，由于这两个区域内核在初始化的时候就已经建立了映射关系，所以内核很容易就能找到对应的虚拟地址KVA（Kernel Virtual Address） 如果gfp_mask中指定了__GFP_HIGHMEM，那么页分配器将优先在ZONE_HIGHMEM域中分配物理页，但也不排除因为ZONE_HIGHMEM没有足够的空闲页导致页面来自ZONE_NORMAL与ZONE_DMA域的可能性。对于新分配出的高端物理页面，由于内核尚未在页表中为之建立映射关系，所以此时需要：  在内核的动态映射区分配一个KVA 通过操作页表，将第一步中的KVA映射到该物理页面上，通过kmap实现     5、__get_free_pages #  __get_free_pages该函数负责分配2^ordev个连续的物理页面，返回起始页面所在内核线性地址。\n函数的实现如下：\n/* * Common helper functions. Never use with __GFP_HIGHMEM because the returned * address cannot represent highmem pages. Use alloc_pages and then kmap if * you need to access high mem. */ unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order) { struct page *page; page = alloc_pages(gfp_mask \u0026amp; ~__GFP_HIGHMEM, order); if (!page) return 0; return (unsigned long) page_address(page); } EXPORT_SYMBOL(__get_free_pages); 我们可以看到，函数内部调用了alloc_pages函数，并且不能从__GFP_HIGHMEM高端内存分配物理页，最后通过page_address来返回页面的起始页面的内核线性地址。\n 6、get_zeroed_page #  get_zeroed_page用于分配一个物理页同时将页面对应的内容填充为0，函数返回页面所在的内核线性地址。\n可以看下内核代码：\nunsigned long get_zeroed_page(gfp_t gfp_mask) { return __get_free_pages(gfp_mask | __GFP_ZERO, 0); } EXPORT_SYMBOL(get_zeroed_page); 仅仅是在__get_free_pages基础上，使用了 __GFP_ZERO标志，来初始化分配页面的初始内容。\n 7、总结 #  以上，就是建立在伙伴系统之上的页面级分配器，常用的函数有：alloc_pages、__get_free_pages、get_zeroed_page、__get_dma_pages等，其底层实现都是一样的，只是gfp_mask不同。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":30,"href":"/docs/linux/linux_kernel_lock/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3linux%E9%94%81%E6%9C%BA%E5%88%B6%E4%B8%83%E4%BA%92%E6%96%A5%E4%BD%93/","title":"【深入理解Linux锁机制】七、互斥体","section":"Linux 内核锁详解","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":31,"href":"/docs/linux/linux_kernel_lock/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3linux%E9%94%81%E6%9C%BA%E5%88%B6%E5%85%AB%E5%AE%8C%E6%88%90%E9%87%8F/","title":"【深入理解Linux锁机制】八、完成量","section":"Linux 内核锁详解","content":" 我的圈子：高级工程师聚集地  创作理念：专注分享高质量嵌入式文章，让大家读有所得！  \u0026nbsp; 亲爱的读者，你好：  感谢你对我的专栏的关注和支持，我很高兴能和你分享我的知识和经验。如果你喜欢我的内容，想要阅读更多的精彩技术文章，可以扫码加入我的社群。\n  欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":32,"href":"/docs/embeded_tech/self_improve/10w+%E9%98%85%E8%AF%BB%E8%80%97%E6%97%B6%E4%B8%80%E5%91%A8%E6%80%BB%E7%BB%93%E7%9A%84%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF%E8%B6%85%E8%AF%A6%E7%BB%86/","title":"【10W+阅读】耗时一周总结的嵌入式学习路线，超详细","section":"嵌入式工程师养成记","content":"【10W+阅读】耗时一周总结的嵌入式学习路线，超详细 #  人们常说：“人生就是一场场游戏，我们要做的，就是打怪，升级，通关”，学习嵌入式的过程也是如此。\n1、前言 #  最近看到知乎上，给我推送了很多类似的回答，借此机会，也想着重新审视一下自己的学习历程，以及结合自身和大牛，分享一些学习经验，希望对大家有所启发和帮助。\n本文主要目的是为了：\n 提供一张嵌入式学习路线地图 提供不同阶段的学习建议 推荐不同阶段的学习资料  3000余字，耗时1周，建议收藏；码字不易，如有帮助，慷慨三连\n 本文将嵌入式学习路线分为几个方面：\n 嵌入式基础必备知识 51单片机 STM32单片机 小而美的RTOS ARM+LINUX   在这个快节奏的时代，能静下心，耐住性子看看文章，实属不易。\n  2、嵌入式基础必备知识 #  老子曰：“合抱之木，生于毫末：九层之台，起于垒土；千里之行，始于足下”，根基的重要性不言而喻。\n那么对于嵌入式这条路线而言，如何建立一个稳固的根基？\n 2.1、学习内容 #   C语言基础  该部分，主要包括几个核心知识点：三大语法结构、常用的数据类型、函数、结构体、指针、文件操作等。\n 硬件基础知识  该部分，核心知识点在于：电路基础知识、数电模电基础知识、常用的电子元器件等。\n 数据结构  核心知识点：数组、队列、链表、堆栈、树、图、散列表等。\n 操作系统  核心知识点：进程管理、内存管理、文件管理、输入输出管理等。\n 计算机原理  核心知识点：数据表示和运算、存储系统、指令系统、总线系统、中央处理器、输入输出系统等。\n 2.2、学习建议 #   对于C语言基础学习，一定要重点熟练掌握，根基的牢固直接决定了我们的代码质量。 对于硬件基础学习，要适当了解，要能够看懂一些简单的电路结构，认识常用的电子元器件。 对于数据结构学习，前五个是必备学习的，可能在刚开始学习的时候，可能会感觉不到作用在哪里，但是随着接触到嵌入式底层设计以及算法设计的时候，才会恍然大悟。 对于操作系统学习，重点学习其思想，对相关知识点有一个大概的了解，后续接触到继续重点学习，这些无论是RTOS，还是Linux，都有涉及到的。 对于计算机原理学习，可以将其看作是嵌入式系统的各个模块的详解，会让你对嵌入式有一个整体的了解，每一个部分都值得深究。   2.3、学习资料 #   C语言基础：推荐经典书籍**《C语言程序设计》（第2版）谭浩强版本**。 硬件基础：大学里面的《数电模电》书籍所涉及的知识即可。 数据结构：推荐经典书籍**《数据结构》——严蔚敏版**。 操作系统、计算机原理：我用的是**《王道》的系列丛书**，个人感觉不错。  计算机组成、数据结构、操作系统、数据库是嵌入式或者说计算机的入门必读书籍，并且也被列入高校教材内，是真正的基础知识。\n 以上，不一定是全部看完才能体验编程的乐趣，这个基础是一个循序渐进的过程，也不是一朝一夕就能完成的，可以先有一个大概，后续做项目时，哪里不懂补哪里！\n这里涉及到一个重要的学习方法：项目导向的学习法。\n 3、嵌入式入门篇——51单片机 #  在上面的基础知识进行熟悉之后（C语言基础、计算机组成、硬件基础必备），我们准备叩开嵌入式世界的大门。\n入门篇，依旧推荐51单片机，当然有人会说，直接上STM32岂不更好？\n我的看法：建议新手还是以51单片机来入门，因为STM32体系架构比51大很多，对于新手刚开始可能会不太容易适应。\n 3.1、学习内容 #  该部分，主要在最小嵌入式系统中，实现各种有趣的实验。通过51单片机的学习，我们要做到：\n 软件类：  主要知识点有：认识单片机、熟悉逻辑运算、点亮一颗LED灯、按键检测、串口通信、定时器、中断等。\n 硬件类：  主要知识点有：电阻元器件了解，基本模块电路了解，时钟电路，尝试绘制51单片机原理图和PCB\n 3.2、学习建议 #   对于软件类，我们主要做到：认识单片机，熟悉单片机的GPIO的输入、输出操作，串口通信协议掌握等，这些部分都是任何一款嵌入式设备的必备技能。 对于硬件类：我们主要做到：能看懂电路图，熟悉一些简单模块的设计电路，了解Altium Designer的使用方法。   3.3、学习资料 #  51单片机：郭天祥的51单片机教程，经典著作，经久不衰，强烈推荐。\n 庄子说：“水之积也不厚，则其负大舟也无力。“\n该部分，是嵌入式领域的基石，只有将基础打牢，才能负得起Linux泰坦号。\n 4、STM32进阶篇 #  STM32是C51的进阶版，拥有C51的基础知识，开发STM32会得心应手。\nSTM32的系统架构以及硬件设计相比于C51来说，都是上升了一个维度的，这也是为什么我推荐入门学习C51的原因。\n 以STM32F407平台为基础，去学习目前嵌入式主流的一些技术，探寻底层的原理，做到不同平台，都能够得心应手。\n 4.1、学习内容 #   基础练习  该部分，主要练习：点亮LED灯、GPIO的输入输出操作、中断操作、UART通信、IIC通信等\n 进阶练习  该部分，主要练习：DMA通信、SPI通信、CAN通信、LCD显示屏，ADC等\n 高阶练习  该部分，主要学习：STM32时钟架构、总线架构、电源管理、代码框架、SDIO通信、USB通信等。\n 4.2、学习建议 #   对于基础练习，主要目的是为了方便让我们从C51到STM32环境的过渡。 对于进阶练习，主要练习一些通信类相关的协议，可以结合一些传感器进行开发。 对于高阶练习，主要目的是为了熟悉单片机的设计架构，编程的框架，以及一些更复杂的通信技术。  另外，STM32会有寄存器和库函数两个版本，建议交叉学习，理解会更加深刻。\n 4.3、学习资料 #  STM32单片机：推荐正点原子、野火的STM32F103或者STM32F407系列。\n两家的学习资料都非常丰富，既有详细的文档说明，也有完整的学习视频教程，非常适合新手入门学习。\n 俗话说：“有道无术，术尚可求，有术无道，止于术”。要明白道和术的区别，不要本末倒置。\n 5、小而美的RTOS #  RTOS，实时操作系统，可以理解为STM32与Linux之间的桥梁，由于其实现思想大都取之于Linux，所以也称之为精简版的Linux。\n我们常用的有实时操作系统有：UCOS，VxWork，FreeRtos，近些年RT-Thread也异军突起。\n学习这些简单的嵌入式系统，一来能够帮助我们为学习Linux操作系统打下基础，二来也能够扩宽我们的职业道路。\n前面也说过了，无论是UCOS、FreeRtos、Rt-thread，其内部的设计思想大同小异，下面主要以Ucos为例。\n5.1、学习内容 #   实时系统学习  该部分，主要学习：移植Ucos系统、多任务管理、调度算法、消息队列、信号量互斥量、事件、内存管理等。\n 5.2、学习建议 #   对于实时系统学习，除了上述的那些核心知识点外，还要结合2.1 基础必备知识的操作系统书籍加深理解。   5.3、学习资料 #  RTOS的学习：依旧推荐正点原子，野火，因为这些实时操作系统开发，可以基于STM32开发板，同时也有非常详细的文档和视频教学。\n 6、ARM+Linux篇 #  学习完RTOS后，基本嵌入式所涉及的技术已经掌握一半了，你也可以独立完成一些小的项目，也可以找到一个不错的工作，但是一定不要自我满足，有机会一定要接触Linux。\n还是那句话：ARM+Linux，也是最为复杂的东西，如果你不去接触Linux，你永远不知道嵌入式的魅力。\n Linux开发又分为驱动开发，内核开发，应用开发，每一个方向都需要几年甚至几十年的积累。\n 作为初学者，我们要做的就是宏观了解，扩大我们的知识面，然后去选择自己感兴趣的方面。\n 6.1、学习内容 #   Linux基础篇  该部分主要学习：Linux常用命令、VIM学习、Linux的Shell编程、Gcc编译、Makefile等。\n 驱动篇  该部分主要学习：内核模块编译原理、字符设备驱动框架、平台设备驱动、设备树、Pinctrl子系统、I2C子系统、中断子系统、块设备驱动框架、Bootloader等\n 内核篇  该部分主要学习：系统调用、存储管理、进程管理、内存管理、文件管理等。\n 应用篇  该部分主要学习：QT编程、TCP/IP协议、HTTP协议等。\n 6.2、学习建议 #   对于基础学习，刚接触到Linux，一般比较难上手，与之前的单片机完全不同，需要一个熟悉环境的过程。 对于驱动学习，重要在于明白“如何在Linux环境下编写驱动程序”，驱动的底层原理还是那样，加了一层层的框架，需要我们去熟悉。 对于内核学习，上述也是系统的几大核心特色，重点在于\u0026quot;如何使Linux性能最优\u0026quot; 对于应用学习，上述的几个方面也是基础，重点还在于开发什么应用，去学习哪方面的知识，没有定论。  对于Linux，有句老话“学习Linux，3年才算入门，5年才勉强算Linux工程师，对于不太熟悉的领域，博主也不敢妄加断言。”\n 6.3、学习资料 #   对于基础学习，推荐**《鸟哥的Linux私房菜》，《Unix环境高级编程》**等入门书籍。 对于驱动开发，推荐**《Linux设备驱动开发详解》**，Linux内核源码详解等。 对于内核学习，推荐**《Linux Shell脚本攻略》、《深入理解Linux内核》**等。 对于应用开发，推荐**《嵌入式Linux应用开发完全手册》、《Unix网络编程》**等。 另外，推荐正点原子，野火，韦东山三个Linux开发教程，韦老师的课程好评居多，但还是看哪个更适合自己。   7、总结 #  全文整体的学习路线：嵌入式基础学习 -\u0026gt; 51单片机 -\u0026gt; STM32单片机 -\u0026gt; RTOS篇 -\u0026gt; ARM+Linux\n每一个部分，也都从学习内容，学习建议，学习资料三个方面来展开，层层深入，步步指引。\n文章既是我的学习历程，又结合了一些大佬的学习分享，不断调整总结出来的，如有异同，可以讨论。\n全文3000余字，耗时1周，如有帮助，望不吝点赞关注。\n最后，文章所涉及的学习资料以及整理的思维导图，全部会在我的星球【嵌入式艺术】分享！\n 欢迎关注【嵌入式艺术】，董哥原创！  "},{"id":33,"href":"/about/index_zh/","title":"About","section":"Abouts","content":"1、个人介绍 #  🙍🏻‍♂️ 大家好，我是董哥，一名工作多年的嵌入式Linux开发工程师。以下是我的基本信息介绍：\n 参加全国机器人大赛（Robocon），两次获得全国一等奖 毕业后斩获科沃斯，石头，格力等多家头部机器人公司的offer，后入职世界五百强格力电器担任嵌入式开发工程师 现今就职于独角兽芯片企业，担任嵌入式Linux驱动开发工程师 熟练使用C/C++语言开发，熟悉各类MCU开发，如STM32，ARM，SOC等，熟悉Ucos，RT-thread实时操作系统等 目前主要负责Linux驱动，系统开发，WiFi\u0026amp;BT开发等相关工作，同时跟进并参与多款百万级量产项目的研发。 荣获优质嵌入式领域创作者称号，拿下2022年度博客之星嵌入式领域TOP 5，全网收获超百万读者。  2、技术与分享 #  记录Blog是一项值得挑战的事情，一方面是对自我技术的沉淀，另一方面也是四万万嵌入式开发者前行路上的加速剂；并且网上大多数文章七零八落，每个人对技术的理解程度不同，因此好的文章，永不过时！\n我的一些自媒体平台：\n CSDN：卍一十二画卍 知乎：嵌入式艺术 公众号：嵌入式艺术 知识星球：嵌入式艺术  3、我的星球 #  🚩 【嵌入式艺术】星球，目前是处于起步阶段，我们的目标是：携手共创高质量的嵌入式基地，兼收并蓄，群英荟萃，实现升职加薪创业梦！\n🛎️ 我们提供的服务有：\n 提供一个高级嵌入式工程师聚集地，聚焦嵌入式工程师成长与发展。 高质量嵌入式项目、技术的拆解与分析 高效率的嵌入式开发工具分享 AIGC + 嵌入式 应用，跟上时代的脚步 嵌入式的行业趋势与热点分析  🛎️ 我们后续要做的事情：\n 引入更多嵌入式领域大咖加入我们的星球，为大家提供更好的服务！ 引入更多优质公司的内推岗位，以便大家走内部推荐通道，加入头部企业！ 拆解更多嵌入式项目，为大家提供实战经验，以目标为导向，实现更好的学习效果！ 星球不定期举办激励活动，有实物激励以及现金激励两种，希望大家踊跃参加！  "}]